'use strict';(function(){const indexCfg={encode:false,tokenize:function(str){return str.replace(/[\x00-\x7F]/g,'').split('');}};indexCfg.doc={id:'id',field:['title','content'],store:['title','href','section'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/','title':"Docs",'section':"Home",'content':"  Documents     "});index.add({'id':1,'href':'/docs/documents/projects/','title':"Projects",'section':"Documents",'content':"Projects #     hU-Go-Girl   HUGO (feat Github)ë¥¼ ì´ìš©í•œ ì •ì  ì›¹í˜ì´ì§€ ìƒì„±   Rust_SDL2   Rudst SDL2 Game   Defenders   C\u0026#43;\u0026#43; Defenders   Wordfighter   C\u0026#43;\u0026#43; Word Fighter   Qmodeler   C# FLO Quick Modeler   Cube   C# 3D Cube   SCM Template   Visual Basicìœ¼ë¡œ ì‘ì„±í•œ SCM Operation Template   "});index.add({'id':2,'href':'/docs/documents/projects/hugogirl/','title':"hU-Go-Girl",'section':"Projects",'content':"hU-Go-Girl #   ê´€ë ¨ ë‚´ìš© ë° ì°¸ê³  ë§í¬\n Static Web page Hugo Jekyll Github Markdown Buy me a coffee   -- Static vs Dynamic #  ì •ì ì›¹ì€ ì‚¬ìš©ìì˜ ìš”ì²­ì— ëŒ€í•´ ì¤€ë¹„ëœ ì‘ë‹µë§Œì„ ì œê³µ.\n mermaid.initialize({ \"flowchart\": { \"useMaxWidth\":true }, \"theme\": \"default\" } ) graph LR A[ ì‚¬ìš©ì ]--|ìš”ì²­|B[ì›¹ì„œë²„] B--|ì‘ë‹µ|A style A fill:#ffffff,stroke:#000000,stroke-width:1px style B fill:#ffffff,stroke:#000000,stroke-width:1px ë™ì ì›¹ì€ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ìœ„í•´ ì¶”ê°€ì ì¸ ê°€ê³µì„ ê±°ì³ ì²˜ë¦¬.\ngraph LR D[ ì‚¬ìš©ì ]--|ìš”ì²­|F[ì›¹ì„œë²„]--|í”„ë¡œì„¸ì‹±|G[(DBì„œë²„)] G--|ì‘ë‹µ|F--|ì‘ë‹µ|D style D fill:#ffffff,stroke:#000000,stroke-width:1px style F fill:#ffffff,stroke:#000000,stroke-width:1px style G fill:#ffffff,stroke:#000000,stroke-width:1px ê°œì¸ ê¸°ë¡ ìš©ë„ë¡œëŠ” ë™ì  ì„œë¹„ìŠ¤ê°€ ë¶ˆí•„ìš”. í•´ë‹¹ ì‚¬ì´íŠ¸ëŠ” ì •ì ì›¹ìœ¼ë¡œ ì„ íƒ.\nGithub #  Gitì€ ë¦¬ëˆ…ìŠ¤ ì»¤ë„ì˜ ì†ŒìŠ¤ì½”ë“œ ê´€ë¦¬ë¥¼ ìœ„í•´ ë¦¬ëˆ„ìŠ¤ í† ë°œì¦ˆ1ê°€ ê°œë°œ. Gitì„ ìœ„í•œ ë¬´ë£Œ ì €ì¥ì†Œ Github. í•´ë‹¹ ì„œë¹„ìŠ¤ì—ëŠ” ìë£Œ ê³µìœ ë¥¼ ìœ„í•œ Markdown ì§€ì›ì˜ ì›¹ í˜¸ìŠ¤íŒ… ì„œë¹„ìŠ¤ë„ í¬í•¨.\n Github ê°€ì… Repository ìƒì„± Github Page ì„¤ì • Add a README file í¬í•¨ Setting \u0026gt; Github Pages \u0026gt; Source ì„¤ì •  Jekyll vs Hugo #  Jekyllì€ Githubì˜ ì°½ë¦½ì2ê°€ Ruby3ë¼ëŠ” ì–¸ì–´ë¡œ ë§Œë“  ì •ì  ì›¹ ìƒì„±ê¸°. ë³„ë„ ë¹Œë“œ ì—†ì´ Githubì™€ ì—°ë™ ê°€ëŠ¥. HugoëŠ” GOë¡œ ì‘ì„±ëœ ì •ì  ì›¹ ìƒì„±ê¸°. ìœ ì‚¬í•œ íˆ´ë¡œ ì¤‘êµ­ì—ì„œ ì œì‘ëœ Hexoë„ ìˆìŒ.4 ë‘˜ë‹¤ ë¹ ë¥¸ ë¹Œë“œë¥¼ íŠ¹ì§•ìœ¼ë¡œ í•˜ê³  ìˆìŒ.\nJekyllì€ í…Œë§ˆê°€ ë§ê³  Githubì— ë¹Œë“œ ì—†ì´ ì—°ë™í•  ìˆ˜ ìˆìŒ. ë‹¤ë§Œ ì»¨í…ì¸ ì— ë”°ë¼ ë¹Œë“œ ì‹œê°„ì´ ë¹„ë¡€í•˜ì—¬ ì¦ê°€5.\nTheme #  ë¬¸ì„œì™€ ê¸°ë¡ì´ ì£¼ ìš©ë„ë¡œ í…ìŠ¤íŠ¸ ê¸°ë°˜ì˜ í…Œë§ˆë¥¼ ê²€í† . Jekyllì—ì„œëŠ” Just-the-doc. Hugoì—ì„œëŠ” Book í…Œë§ˆ. í•„ìš”ì— ë”°ë¼ ëª‡ê°€ì§€ ë””ìì¸ì„ ìˆ˜ì •.\nMarkdown #  Markdownì€ HTMLê³¼ ê°™ì€ ë§ˆí¬ì—… ì–¸ì–´ì„. ë§ˆí¬ì—… ì–¸ì–´ëŠ” ë³„ë„ í‘œí˜„ì„ ìœ„í•œ ê¸°ìˆ ì–¸ì–´ë¥¼ ì˜ë¯¸.\nMarkdown ì •ë¦¬\nBuy Me a Coffee #  ê°œì¸ ì»¨í…ì¸  ì œì‘ìë¥¼ ìœ„í•œ í›„ì› ì„œë¹„ìŠ¤ ì‚¬ì´íŠ¸. Buy me a coffee ê°€ì… ì¤‘ Paypalì— ë“±ë¡í•˜ì—¬ ë°©ë¬¸ê°ì´ 1$~5$ ì†Œì•¡ ì†¡ê¸ˆì´ ê°€ëŠ¥.6 ì»¤í”¼ ì•„ì´ì½˜ì„ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•˜ì—¬ ì‚¬ìš©.\nConclusion #   VSCì—ì„œ Markdownì„ ì´ìš© í¬ìŠ¤íŠ¸ë¥¼ ì‘ì„± Hugo Build / Severë¥¼ í†µí•´ ë¡œì»¬ì—ì„œ í™•ì¸ Hugo Publicë¥¼ í†µí•´ Public ë¹Œë“œ Gitì„ í†µí•´ Githubì— Pushí•˜ê³  Webìœ¼ë¡œ Publish  # 1) Hugo Build ê²½ë¡œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤. rm -rf ./public/ # 2) Hugo Build í•©ë‹ˆë‹¤. hugo -t book # 3) ì „ ì œ githubë¥¼ Hugo build ê²½ë¡œ ì•ˆì— ìœ„ì¹˜ ì‹œì¼°ìŠµë‹ˆë‹¤. í•´ë‹¹ë‚´ìš©ì„ .gitë§Œ ì œì™¸í•˜ê³  ì‚­ì œí•˜ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤. find ./youraccount.github.io/ -maxdepth 1 ! -name \u0026#34;.git\u0026#34; ! -name \u0026#34;youraccount.github.io\u0026#34; -exec rm -rf {} \\; # 4) publicì„ pullí•œ ê²½ë¡œë¡œ ì˜®ê¹ë‹ˆë‹¤. cp -r ./public/* ./youraccount.github.io/ # 5) pullí•œ ê²½ë¡œë¡œ ì´ë™í•©ë‹ˆë‹¤. cd ./youraccount.github.io # 6) add / commit / push í•©ë‹ˆë‹¤. git add . git commit -m \u0026#34;$(date +\u0026#34;%y%m%d_%H:%M\u0026#34;)\u0026#34; git push origin master \nReference #   ìƒí™œì½”ë”© : ë²„ì „ê´€ë¦¬ ì‹œìŠ¤í…œ ìƒí™œì½”ë”© : Github  Footnote #    ì €ëª…í•˜ì‹  í‚¤ë³´ë“œ ì›Œë¦¬ì–´ \u0026#x21a9;\u0026#xfe0e;\n í†° í”„ë ˆìŠ¤í†¤ ì›Œë„ˆ \u0026#x21a9;\u0026#xfe0e;\n ì¼ë³¸ì‚° \u0026#x21a9;\u0026#xfe0e;\n ë¹„êµìë£Œ \u0026#x21a9;\u0026#xfe0e;\n [Jekyllì„±ëŠ¥ì´ìŠˆ]](https://forestry.io/blog/how-i-reduced-my-jekyll-build-time-by-61/) \u0026#x21a9;\u0026#xfe0e;\n í•œêµ­ PayPal FAQs \u0026#x21a9;\u0026#xfe0e;\n   "});index.add({'id':3,'href':'/docs/documents/projects/rust_sdl2/','title':"Rust_SDL2",'section':"Projects",'content':"Rust SDL2 #   Rustë¥¼ ì´ìš©í•œ SDL2 Test\nrust_sdl2 github\n"});index.add({'id':4,'href':'/docs/documents/projects/defenders/','title':"Defenders",'section':"Projects",'content':"Defenders #   C++ì„ ì´ìš©í•œ Isometrix Defenders Game\ndefenders github\n "});index.add({'id':5,'href':'/docs/documents/projects/wordfighter/','title':"Wordfighter",'section':"Projects",'content':"Wordfighter #   C++ì„ ì´ìš©í•œ Word Fighter\nwordfighter github\n "});index.add({'id':6,'href':'/docs/documents/projects/qmodeler/','title':"Qmodeler",'section':"Projects",'content':"QModeler #   C#ì„ ì´ìš©í•œ FLO Quick Modeler\nqmodeler github\n "});index.add({'id':7,'href':'/docs/documents/projects/cube/','title':"Cube",'section':"Projects",'content':"Cube #   C#ì„ ì´ìš©í•œ 3D Cube\ncube github\n "});index.add({'id':8,'href':'/docs/documents/projects/scmtemplate/','title':"SCM Template",'section':"Projects",'content':"SCM Template #   Visual Basicìœ¼ë¡œ ì‘ì„±í•œ SCM Operation Template\n"});index.add({'id':9,'href':'/docs/documents/frontend/','title':"Front End",'section':"Documents",'content':"Front End #     HTML   HTML Study   Markdown   Markdown ë¬¸ë²• ì„¤ëª…   "});index.add({'id':10,'href':'/docs/documents/frontend/html/','title':"HTML",'section':"Front End",'content':"HTML #   Hyper Text Markup Language\nw3schools\nHeader #  HTML \u0026lt;h1\u0026gt;Text\u0026lt;/h1\u0026gt; Result Text  "});index.add({'id':11,'href':'/docs/documents/frontend/markdown/','title':"Markdown",'section':"Front End",'content':"Markdown #   Markdownì€ HTML ë³€í™˜ì´ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²½ëŸ‰ ë§ˆí¬ì—… ì–¸ì–´. íŠ¹ìˆ˜ ê¸°í˜¸ì™€ ë¬¸ìë¥¼ ì´ìš©í•´ ì‰½ê²Œ ì“°ê³  ì½ì„ ìˆ˜ ìˆìœ¼ë©° ë¬¸ë²•ì´ ë¹„êµì  ì‰½ê³  ê°„ë‹¨í•œ ê²ƒì´ íŠ¹ì§•.\nHeader #  Markdown # H1 í¬ê¸° ì œëª©ì…ë‹ˆë‹¤. ## H2 í¬ê¸° ì œëª©ì…ë‹ˆë‹¤. ### H3 í¬ê¸° ì œëª©ì…ë‹ˆë‹¤. #### H4 í¬ê¸° ì œëª©ì…ë‹ˆë‹¤. ##### H5 í¬ê¸° ì œëª©ì…ë‹ˆë‹¤. ###### H6 í¬ê¸° ì œëª©ì…ë‹ˆë‹¤. Result H1 í¬ê¸° ì œëª©ì…ë‹ˆë‹¤. H2 í¬ê¸° ì œëª©ì…ë‹ˆë‹¤. H3 í¬ê¸° ì œëª©ì…ë‹ˆë‹¤. H4 í¬ê¸° ì œëª©ì…ë‹ˆë‹¤. H5 í¬ê¸° ì œëª©ì…ë‹ˆë‹¤. H6 í¬ê¸° ì œëª©ì…ë‹ˆë‹¤.  Emphasis #  Markdown *ê°•ì¡° í˜•íƒœ 1* _ê°•ì¡° í˜•íƒœ 2_ **ê°•ì¡° í˜•íƒœ 3** __ê°•ì¡° í˜•íƒœ 4__ ~~ê°•ì¡° í˜•íƒœ 5~~ Result ê°•ì¡° í˜•íƒœ 1\nê°•ì¡° í˜•íƒœ 2\nê°•ì¡° í˜•íƒœ 3\nê°•ì¡° í˜•íƒœ 4\nê°•ì¡° í˜•íƒœ 5 List #  Markdown 1. ìˆœì„œ ë¦¬ìŠ¤íŠ¸ 1 2. ìˆœì„œ ë¦¬ìŠ¤íŠ¸ 2 3. ìˆœì„œ ë¦¬ìŠ¤íŠ¸ 3 - ë¦¬ìŠ¤íŠ¸ 1 - ë¦¬ìŠ¤íŠ¸ 2 - ë¦¬ìŠ¤íŠ¸ 3 - ë¦¬ìŠ¤íŠ¸ 1 - ë¦¬ìŠ¤íŠ¸ 1-1 - ë¦¬ìŠ¤íŠ¸ 1-1-1 - ë¦¬ìŠ¤íŠ¸ 2 - ë¦¬ìŠ¤íŠ¸ 2-1 - ë¦¬ìŠ¤íŠ¸ 2-1-1 - ë¦¬ìŠ¤íŠ¸ 3 - ë¦¬ìŠ¤íŠ¸ 3-1 - ë¦¬ìŠ¤íŠ¸ 3-1-1 Result  ìˆœì„œ ë¦¬ìŠ¤íŠ¸ 1 ìˆœì„œ ë¦¬ìŠ¤íŠ¸ 2 ìˆœì„œ ë¦¬ìŠ¤íŠ¸ 3   ë¦¬ìŠ¤íŠ¸ 1 ë¦¬ìŠ¤íŠ¸ 2 ë¦¬ìŠ¤íŠ¸ 3 ë¦¬ìŠ¤íŠ¸ 1  ë¦¬ìŠ¤íŠ¸ 1-1  ë¦¬ìŠ¤íŠ¸ 1-1-1     ë¦¬ìŠ¤íŠ¸ 2  ë¦¬ìŠ¤íŠ¸ 2-1  ë¦¬ìŠ¤íŠ¸ 2-1-1     ë¦¬ìŠ¤íŠ¸ 3  ë¦¬ìŠ¤íŠ¸ 3-1  ë¦¬ìŠ¤íŠ¸ 3-1-1       Blockquotes #  Markdown \u0026gt; ì¸ìš© 1 \u0026gt; ì¸ìš© 2 \u0026gt; ì¸ìš© 3 \u0026gt; ì¸ìš© 1 \u0026gt;\u0026gt; ì¸ìš© 2 \u0026gt;\u0026gt;\u0026gt; ì¸ìš© 3 íƒ­) ì¸ìš© 1 íƒ­) ì¸ìš© 2 íƒ­) ì¸ìš© 3 Result  ì¸ìš© 1\nì¸ìš© 2\nì¸ìš© 3 ì¸ìš© 1\n ì¸ìš© 2\u0026gt;\n ì¸ìš© 3 íƒ­) ì¸ìš© 1 íƒ­) ì¸ìš© 2 íƒ­) ì¸ìš© 3\n    Inline code #  Markdown ```Rust fn main () { println!(\u0026#34;Hello World!\u0026#34;); } ``` \u0026lt;pre\u0026gt;\u0026lt;code\u0026gt; { fn main () { println!(\u0026#34;Hello World!\u0026#34;); } } \u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt; Result fn main () { println!(\u0026#34;Hello World!\u0026#34;); }  { fn main () { println!(\"Hello World!\"); } }   Tables #  Markdown | Header1 | Header2 | Header3 | Header4 | Header5 | | :------ | ------: | :-----: | ------- | ------- | | a | b | c | **d** | ~~e~~ | | f | g | h | i | j | | k | l | m | n | o | Result    Header1 Header2 Header3 Header4 Header5     a b c d e   f g h i j   k l m n o     Checkbox #  Markdown 1. [ ] To-Do 1 2. [x] To-Do 2 - [ ] To-Do 3 - [x] To-Do 4 Result  To-Do 1 To-Do 2   To-Do 3 To-Do 4   Hyperlinks #  Markdown [Prokoptasis](https://prokoptasis.github.io/docs/documents/frontend/markdown/) Result Prokoptasis Images #  Markdown ![coffee](../../../../../coffee.jpg) [![coffee](../../../../../coffee.jpg)](https://prokoptasis.github.io/docs/documents/frontend/markdown/) Result  Iframe #  Markdown \u0026lt;iframe width=\u0026#34;100%\u0026#34; height=\u0026#34;400px\u0026#34; src=\u0026#34;https://www.youtube.com/embed/4Vs25c7dzTQ\u0026#34; frameborder=\u0026#34;0\u0026#34; allow=\u0026#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\u0026#34; allowfullscreen\u0026gt;\u0026lt;/iframe\u0026gt; Result   Etc #  Markdown ë¼ì¸1 *** ë¼ì¸2 --- ë¼ì¸3 ___ ë¼ì¸4 \u0026lt;hr\u0026gt; ë¬¸ì¥ 1 \u0026lt;br\u0026gt; ë¬¸ì¥ 2 (ê³µë°±3ì¹¸) ë¬¸ì¥ 3 (ê³µë°±ì—†ìŒ) ë¬¸ì¥ 4 \u0026lt;div align=\u0026#34;center\u0026#34;\u0026gt;ê°€ìš´ë° ì •ë ¬\u0026lt;/div\u0026gt; \u0026lt;span style=\u0026#34;color:red\u0026#34;\u0026gt;ë¹¨\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026#34;color:#0055FF\u0026#34;\u0026gt;íŒŒ\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026#34;color:rgb(21, 250, 16)\u0026#34;\u0026gt;ë…¹\u0026lt;/span\u0026gt; Result ë¼ì¸1\n ë¼ì¸2\n ë¼ì¸3\n ë¼ì¸4\n ë¬¸ì¥ 1 ë¬¸ì¥ 2 (ê³µë°±3ì¹¸) ë¬¸ì¥ 3 (ê³µë°±ì—†ìŒ) ë¬¸ì¥ 4 ê°€ìš´ë° ì •ë ¬ ë¹¨ íŒŒ ë…¹  Footnotes #  ê°ì£¼[^1] ê°ì£¼1\n  ê°ì£¼ì…ë‹ˆë‹¤. \u0026#x21a9;\u0026#xfe0e;\n   "});index.add({'id':12,'href':'/docs/documents/backend/','title':"Back End",'section':"Documents",'content':"Back End #     Rust   Rust Language Study   Go   Go Language Study   SQL   SQL ë¬¸ë²• ì„¤ëª…   "});index.add({'id':13,'href':'/docs/documents/backend/rust/','title':"Rust",'section':"Back End",'content':"RUST #     Rust Advanced   Rust Advanced   Rust Basic   The Rust Programming Language   "});index.add({'id':14,'href':'/docs/documents/backend/go/','title':"Go",'section':"Back End",'content':"GO #     GO Basic   ì˜ˆì œë¡œ ë°°ìš°ëŠ” GO í”„ë¡œê·¸ë¦¬ë° : GO ê¸°ì´ˆ   GO Advanced   Nomadcoders - ì‰½ê³  ë¹ ë¥¸ Go ì‹œì‘í•˜ê¸°   GO DB ì—°ê²°   GO Oracle DB ì—°ê²°   "});index.add({'id':15,'href':'/docs/documents/backend/sql/','title':"SQL",'section':"Back End",'content':"SQL #     SQL Basic   SQL Basic   SQL Advanced   SQL Advanced Query   "});index.add({'id':16,'href':'/docs/documents/backend/go/go01/','title':"GO Basic",'section':"Go",'content':"GO Basic #   Go ê¸°ë³¸ì ì¸ ë¬¸ë²• ì •ë¦¬. í•˜ê¸° ì‚¬ì´íŠ¸ ì°¸ì¡°.\nì˜ˆì œë¡œ ë°°ìš°ëŠ” Go í”„ë¡œê·¸ë˜ë°\nHello World #  package main func main() { println(\u0026#34;Hello World!\u0026#34;) } Declare #  ì„ ì–¸ ë° í• ë‹¹ ë°©ì‹\n declare ...  package main func main() { // ì„ ì–¸ ë° í• ë‹¹ \tvar a int var b float32 = 11. println(a, b) // ì„ ì–¸ ë° í• ë‹¹ \ta = 10 b = 12.0 println(a, b) // ìƒìˆ˜ \tconst c int = 10 const d string = \u0026#34;Hello\u0026#34; println(c, d) // ìƒìˆ˜ ë‚˜ì—´ \tconst ( Visa = \u0026#34;Visa\u0026#34; Master = \u0026#34;MasterCard\u0026#34; Amex = \u0026#34;American Express\u0026#34; ) println(Visa, Master, Amex) // ìƒìˆ˜ ë‚˜ì—´ \tconst ( Apple = iota Grape Orage ) println(Apple, Grape, Orage) }     Reserved Keywords #  GO 25ê°œ ì˜ˆì•½ì–´\nbreak, case, chan, const, continue, const, default, const, defer, else,fallthrough, for, func, go, goto, if, import, interface, map, package, range, return, select, struct, switch, type, var  Declaration of Varialbe and Operators #  ë³€ìˆ˜ ì„ ì–¸ ë° ì—°ì‚°ì\n declaration of varialbe ...  func main() { textLine1 := `ì²«ë²ˆì§¸ ì¤„ì…ë‹ˆë‹¤.\\n ë‘ë²ˆì§¸ ì¤„ì…ë‹ˆë‹¤.\\n ì„¸ë²ˆì§¸ ì¤„ì…ë‹ˆë‹¤` fmt.Println(textLine1) textLine2 := \u0026#34;ì²«ë²ˆì§¸ ì¤„ì…ë‹ˆë‹¤.\\në‘ë²ˆì§¸ ì¤„ì…ë‹ˆë‹¤.\\nì„¸ë²ˆì§¸ ì¤„ì…ë‹ˆë‹¤\u0026#34; fmt.Println(textLine2) textLine3 := \u0026#34;ì²«ë²ˆì§¸ ì¤„ì…ë‹ˆë‹¤.\\n\u0026#34; + \u0026#34;ë‘ë²ˆì§¸ ì¤„ì…ë‹ˆë‹¤.\\n\u0026#34; + \u0026#34;ì„¸ë²ˆì§¸ ì¤„ì…ë‹ˆë‹¤\u0026#34; fmt.Println(textLine3) var a int = 100 var b uint = uint(a) var c float32 = float32(a) println(a, b, c) str := \u0026#34;ABCDEFG\u0026#34; byt := []byte(str) stg := string(byt) println(str, byt, byt[0], byt[1], stg) }      operators ...  package main func main() { var a int = 7 var b int = 3 //ì‚°ìˆ ì—°ì‚°ì \tvar c = (a + b) / 5 println(a, b, c) c++ println(c) // ê´€ê³„ì—°ì‚°ì \tif b == c { println(\u0026#34;b==c\u0026#34;) } else { println(\u0026#34;b!=c\u0026#34;) } if a != b { println(\u0026#34;a!=b\u0026#34;) } else { println(\u0026#34;a==b\u0026#34;) } if a \u0026gt;= b { println(\u0026#34;a\u0026gt;=b\u0026#34;) } else { println(\u0026#34;a\u0026lt; b\u0026#34;) } // ë…¼ë¦¬ ì—°ì‚°ì \tif a \u0026gt;= b \u0026amp;\u0026amp; b == c { println(\u0026#34;a\u0026gt;=b \u0026amp;\u0026amp; b==c\u0026#34;) } else { println(\u0026#34;not(a\u0026gt;=b \u0026amp;\u0026amp; b==c)\u0026#34;) } if a \u0026gt;= b || b \u0026gt; c { println(\u0026#34;a\u0026gt;=b || b\u0026lt;\u0026gt;c\u0026#34;) } else { println(\u0026#34;not(a\u0026gt;=b || b\u0026lt;\u0026gt;c)\u0026#34;) } // Bitwise ì—°ì‚°ì \tvar d = (a \u0026amp; b) var e = (a \u0026amp; b) \u0026lt;\u0026lt; 1 println(d, e) // í• ë‹¹ì—°ì‚°ì \ta = 10 println(a) a *= 10 println(a) a \u0026gt;\u0026gt;= 1 println(a) a |= 1 println(a) // í¬ì¸í„° ì—°ì‚°ì \tvar x int = 10 var p = \u0026amp;x //x\u0026#39;s address  // x = value \t// \u0026amp;x = x\u0026#39;s address \t// p = \u0026amp;x = x\u0026#39;s address \t// *p = x\u0026#39;s address\u0026#39;s value \t// \u0026amp;p = p\u0026#39;s address \tprintln(x, \u0026amp;x, p, *p, \u0026amp;p) x++ println(x, \u0026amp;x, p, *p, \u0026amp;p) }     Conditional Statement #   conditional statement ...  package main func main() { var a = 1 if a == 1 { println(\u0026#34;One\u0026#34;) } else if a == 2 { println(\u0026#34;Two\u0026#34;) } else { println(\u0026#34;Other\u0026#34;) } // Optional Statement \tif b := 1; b \u0026lt; 10 { println(b) } var name string var category = 1 switch category { case 1: name = \u0026#34;Paper Book\u0026#34; case 2: name = \u0026#34;eBook\u0026#34; case 3, 4: name = \u0026#34;Blog\u0026#34; default: name = \u0026#34;Other\u0026#34; } println(name) // Expressionì„ ì‚¬ìš©í•œ ê²½ìš° \tswitch x := category \u0026lt;\u0026lt; 2; x - 1 { //... \t} var score = 80 switch { case score \u0026gt;= 90: println(\u0026#34;A\u0026#34;) case score \u0026gt;= 80: println(\u0026#34;B\u0026#34;) case score \u0026gt;= 70: println(\u0026#34;C\u0026#34;) case score \u0026gt;= 60: println(\u0026#34;D\u0026#34;) default: println(\u0026#34;No Hope\u0026#34;) } }     Iteration #   iteration ...  package main func main() { sum := 0 for i := 1; i \u0026lt;= 100; i++ { sum += i } println(sum) n := 2 for n \u0026lt; 100 { n = n * n } println(n) names := []string{\u0026#34;abc\u0026#34;, \u0026#34;def\u0026#34;, \u0026#34;geh\u0026#34;} for index, name := range names { println(index, name) } b := 0 L1: for { if b == 0 { break L1 } } println(\u0026#34;L1\u0026#34;, b) var a = 1 for a \u0026lt; 15 { if a == 5 { a += 4 continue } a++ println(\u0026#34;In\u0026#34;, a) if a \u0026gt; 9 { println(\u0026#34;Here\u0026#34;, a) break } } if a == 11 { goto END } println(a) END: println(\u0026#34;End\u0026#34;, a) }     \u0026lsquo;goto END\u0026rsquo;ëŠ” END ë ˆì´ë¸”ë¡œ ì´ë™ \u0026lsquo;break L1\u0026rsquo;ì€ break ë ˆì´ë¸”ë¡œ ìœ„ì¹˜ë¥¼ ì´ë™.\nMethod #   method ...  package main func main() { msg := \u0026#34;Hello\u0026#34; // Function \tsay(msg) println(\u0026#34;2\u0026#34;, msg) // Pass By Reference \tsayC(\u0026amp;msg) println(\u0026#34;4\u0026#34;, msg) // Variadic Function \tsayV(\u0026#34;this\u0026#34;, \u0026#34;is\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;pen\u0026#34;) sayV(\u0026#34;and\u0026#34;, \u0026#34;pineapple\u0026#34;) // Return Value \ttotal := sum(1, 7, 3, 5, 9) println(total) // Return Multiple Value \tcount, total := sumC(1, 7, 3, 5, 9) println(count, total) // Named Return Parameter \tcount, total = sumN(1, 7, 3, 5, 9) println(\u0026#34;Named Return Param\u0026#34;, count, total) // Anonymous Function \tsumA := func(n ...int) int { //ìµëª…í•¨ìˆ˜ ì •ì˜ \ts := 0 for _, i := range n { s += i } return s } result := sumA(1, 2, 3, 4, 5) println(\u0026#34;result :\u0026#34;, result) // add anonymouse function : í•¨ìˆ˜ëª…ì„ ê°–ì§€ ì•ŠëŠ” í•¨ìˆ˜ \tadd := func(i int, j int) int { return i + j } // send anonumous function to other function \tr1 := calc(add, 10, 20) println(\u0026#34;r1 :\u0026#34;, r1) // First-Calss function : ë‹¤ë¥¸í•¨ìˆ˜ì— íŒŒë¼ë¯¸í„°ë‚˜ ë¦¬í„´ê°’ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” í•¨ìˆ˜ \tr2 := calc(func(x int, y int) int { return x - y }, 10, 20) println(\u0026#34;r2 :\u0026#34;, r2) // prototype function \tr3 := calcP(add, 10, 25) println(\u0026#34;r3 :\u0026#34;, r3) // closure : í•¨ìˆ˜ ë°”ê¹¥ì— ìˆëŠ” ë³€ìˆ˜ë¥¼ ì°¸ì¡°í•˜ëŠ” Function Value \tnext1 := nextValue() println(\u0026#34;next1\u0026#34;, next1()) println(\u0026#34;next1\u0026#34;, next1()) println(\u0026#34;next1\u0026#34;, next1()) next2 := nextValue() println(\u0026#34;next2\u0026#34;, next2()) println(\u0026#34;next2\u0026#34;, next2()) println(\u0026#34;next1\u0026#34;, next1()) } func say(msg string) { println(\u0026#34;1\u0026#34;, msg) } func sayC(msg *string) { println(\u0026#34;3\u0026#34;, *msg) *msg = \u0026#34;World\u0026#34; } func sayV(msg ...string) { for _, s := range msg { println(s) } } func sum(nums ...int) int { s := 0 for _, n := range nums { s += n } return s } func sumC(nums ...int) (int, int) { s := 0 count := 0 for _, n := range nums { s += n count++ } return count, s } func sumN(nums ...int) (count int, total int) { for _, n := range nums { total += n } count = len(nums) return } func calc(f func(int, int) int, a int, b int) int { result := f(a, b) return result } // prototype type calculator func(int, int) int // use prototype function func calcP(f calculator, a int, b int) int { result := f(a, b) return result } // closure func nextValue() func() int { i := 0 return func() int { i++ return i } }     Array / Slice / Map #  ë°°ì—´ì€ ì—°ì†ì ì¸ ë©”ëª¨ë¦¬ ê³µê°„ì— ë™ì¼í•œ íƒ€ì…ì˜ ë°ì´í„°ë¥¼ ìˆœì„œì ìœ¼ë¡œ ì €ì¥í•˜ëŠ” ìë£Œêµ¬ì¡°. ìŠ¬ë¼ì´ìŠ¤ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ë°°ì—´ì— ê¸°ì´ˆí•˜ì—¬ ë§Œë“¤ì–´ì¡Œìœ¼ë‚˜ í¬ê¸°ë¥¼ ë™ì ìœ¼ë¡œ ë³€í•  ìˆ˜ë„ ìˆê³  ë¶€ë¶„ ë°°ì—´ì„ ë°œì·Œí•  ìˆ˜ ìˆìŒ.\n array and slice ...  package main import \u0026#34;fmt\u0026#34; func main() { // Array \tvar arr1 [3]int arr1[0] = 1 arr1[1] = 2 arr1[2] = 3 println(\u0026#34;arr1[0] : \u0026#34;, arr1[0], \u0026#34;arr1[1] : \u0026#34;, arr1[1], \u0026#34;arr1[2] : \u0026#34;, arr1[2]) var arr2 = [3]int{4, 5, 6} var arr3 = [...]int{7, 8, 9} println(\u0026#34;arr2[0] : \u0026#34;, arr2[0], \u0026#34;arr2[1] : \u0026#34;, arr2[1], \u0026#34;arr2[2] : \u0026#34;, arr2[2]) println(\u0026#34;arr3[0] : \u0026#34;, arr3[0], \u0026#34;arr3[1] : \u0026#34;, arr3[1], \u0026#34;arr3[2] : \u0026#34;, arr3[2]) // multi array 1 \tvar arr4 [3][4][5]int arr4[0][1][2] = 10 println(\u0026#34;arr4[0][1][2] : \u0026#34;, arr4[0][1][2]) // multi array 2 \tvar arr5 = [2][3]int{{1, 2, 3}, {4, 5, 6}} println(\u0026#34;arr5[1][2] : \u0026#34;, arr5[1][2]) // Slice \tvar slc1 []int slc1 = []int{1, 2, 3} slc1[1] = 10 fmt.Println(\u0026#34;slc1 : \u0026#34;, slc1) // Slice with Make \tslc2 := make([]int, 5, 10) println(len(slc2), cap(slc2)) fmt.Println(slc2) // Nil Slice \tvar slc3 []int if slc3 == nil { println(\u0026#34;Nil Slice\u0026#34;) } println(len(slc3), cap(slc3)) fmt.Println(slc3) // Sub Slice \tslc4 := []int{0, 1, 2, 3, 4, 5} fmt.Println(\u0026#34;[0:1] : \u0026#34;, slc4[0:1], \u0026#34;,[2:5] : \u0026#34;, slc4[2:5], \u0026#34;,[1:] : \u0026#34;, slc4[1:], \u0026#34;,[:5] : \u0026#34;, slc4[:5]) // Slice append / copy \tslc5 := []int{0, 1} // append one \tslc5 = append(slc5, 2) // append multiple \tslc5 = append(slc5, 3, 4, 5) fmt.Println(slc5) // Underlying array \tslc6 := make([]int, 0, 3) for i := 1; i \u0026lt; 15; i++ { slc6 = append(slc6, i) fmt.Println(\u0026#34;slc6 len, cap : \u0026#34;, len(slc6), cap(slc6)) } fmt.Println(slc6) // append slice \u0026amp; ellipsis(...) \tslc7 := []int{1, 2, 3} slc8 := []int{4, 5, 6} slc7 = append(slc7, slc8...) fmt.Println(slc7) // slice copy \tslc9 := []int{0, 1, 2} slc10 := make([]int, len(slc9), cap(slc9)*2) copy(slc10, slc9) fmt.Println(slc10) println(len(slc10), cap(slc10)) }     Mapì€ Keyì— ëŒ€ì‘í•˜ëŠ” Valueë¥¼ ì°¾ëŠ” Hash Tableì„ êµ¬í˜„í•œ ìë£Œêµ¬ì¡°.\n map ...  package main import \u0026#34;fmt\u0026#34; func main() { // map 1 \tvar map01 map[int]string map01 = make(map[int]string) map01[901] = \u0026#34;Apple\u0026#34; map01[134] = \u0026#34;Grape\u0026#34; map01[777] = \u0026#34;Tomato\u0026#34; str := map01[134] println(\u0026#34;134 :\u0026#34;, str) str = map01[999] println(\u0026#34;999 :\u0026#34;, str) str = map01[777] println(\u0026#34;777 :\u0026#34;, str) delete(map01, 777) println(\u0026#34;777 :\u0026#34;, str) // map key check \tmap02 := map[string]string{ \u0026#34;GOO\u0026#34;: \u0026#34;Google\u0026#34;, \u0026#34;MSF\u0026#34;: \u0026#34;Microsoft\u0026#34;, \u0026#34;FBK\u0026#34;: \u0026#34;Facebook\u0026#34;, \u0026#34;AMZ\u0026#34;: \u0026#34;Amazon\u0026#34;, } val, exists := map02[\u0026#34;MSF\u0026#34;] if !exists { println(\u0026#34;No MSF MAP02\u0026#34;) } println(\u0026#34;Exists :\u0026#34;, val, exists) // map for loop \tmap03 := map[string]string{ \u0026#34;A\u0026#34;: \u0026#34;Apple\u0026#34;, \u0026#34;B\u0026#34;: \u0026#34;Banana\u0026#34;, \u0026#34;C\u0026#34;: \u0026#34;Charlie\u0026#34;, } for key, val := range map03 { fmt.Println(key, val) } }     Package #   package  ...  /lib/lib01.go\npackage lib01 import \u0026#34;fmt\u0026#34; var pop map[string]string func init() { pop = make(map[string]string) pop[\u0026#34;A\u0026#34;] = \u0026#34;Apple\u0026#34; pop[\u0026#34;B\u0026#34;] = \u0026#34;Banana\u0026#34; pop[\u0026#34;C\u0026#34;] = \u0026#34;Chocolate\u0026#34; } func GetItems(item string) string { return pop[item] } func getKeys() { for _, kv := range pop { fmt.Println(kv) } } main.go\npackage main import lib01 \u0026#34;./lib\u0026#34; func main() { // package \titem := lib01.GetItems(\u0026#34;A\u0026#34;) println(item) }     Struct #   struct  ...  package main import \u0026#34;fmt\u0026#34; // struct type person struct { name string age int } type dict struct { data map[int]string } func newDict() *dict { dic := dict{} dic.data = map[int]string{} return \u0026amp;dic } func main() { // person struct \tper1 := person{} per1.name = \u0026#34;Lee\u0026#34; per1.age = 18 fmt.Println(per1) // person struct 2 \tvar per2 person per2 = person{\u0026#34;Bob\u0026#34;, 20} per3 := person{name: \u0026#34;Sean\u0026#34;, age: 30} fmt.Println(per2) fmt.Println(per3) // person struct 3 \tper4 := new(person) per4.name = \u0026#34;Kim\u0026#34; fmt.Println(per4) // constructor \tdic1 := newDict() dic1.data[1] = \u0026#34;A\u0026#34; fmt.Println(dic1) }     Method / Interface #   method / interface ...  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;math\u0026#34; ) // struct type Rect struct { width, height int } // method func (r Rect) area() int { return r.width * r.height } // method func (r *Rect) area2() int { r.width++ return r.width * r.height } // interface type Shpe interface { area() float64 peri() float64 } type RectF struct { width, height float64 } type CircF struct { radius float64 } // interface func (r RectF) area() float64 { return r.width * r.height } func (r RectF) peri() float64 { return 2 * (r.width + r.height) } func (c CircF) area() float64 { return math.Pi * c.radius * c.radius } func (c CircF) peri() float64 { return 2 * math.Pi * c.radius } func showArea(shpe ...Shpe) { for _, s := range shpe { a := s.area() println(a) } } func main() { rect := Rect{10, 20} area := rect.area() println(area) area = rect.area2() println(area) rectf := RectF{10., 20.} circf := CircF{10} showArea(rectf, circf) // empty interface \tvar type_x interface{} type_x = 1 type_x = \u0026#34;Jake\u0026#34; printIt(type_x) // Type Assertion \tvar type_y interface{} = 1 var_i := type_y var_j := type_y.(int) println(var_i) // pointer address \tprintln(var_j) // 1 } func printIt(type_v interface{}) { fmt.Println(type_v) }     Error / Defer / Panic / Recover #   error / defer / panic / recover ...  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; ) func main() { // // Error ì²˜ë¦¬ \t// f, err := os.Open(\u0026#34;C:\\\\temp\\\\1.txt\u0026#34;) \t// if err != nil { \t// println(\u0026#34;Error!!!\u0026#34;) \t// log.Fatal(err.Error()) \t// } \t// println(f.Name())  // // defer \t// f, err := os.Open(\u0026#34;1.txt\u0026#34;) \t// if err != nil { \t// panic(err) \t// } \t// defer f.Close() \t// bytes := make([]byte, 1024) \t// f.Read(bytes) \t// println(len(bytes))  // panic \topenFile(\u0026#34;Invalid.txt\u0026#34;) println(\u0026#34;Done!!!\u0026#34;) } func openFile(fn string) { // deferì— ì˜í•œ recover ì²˜ë¦¬ \tdefer func() { if r := recover(); r != nil { fmt.Println(\u0026#34;Open Error\u0026#34;, r) } }() f, err := os.Open(fn) if err != nil { println(\u0026#34;Panic Herer!!!\u0026#34;) panic(err) } defer f.Close() }     Goroutine / Channel #   gorutine / channel ...  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;runtime\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) func say(s string) { for i := 0; i \u0026lt; 3; i++ { fmt.Println(s, \u0026#34;***\u0026#34;, i) time.Sleep(time.Second * 1) } } func say2(s string) { for i := 0; i \u0026lt;= 1000000; i++ { fmt.Println(s, \u0026#34;***\u0026#34;, i) } } func main() { // goroutine \tsay(\u0026#34;Sync\u0026#34;) go say(\u0026#34;Async1\u0026#34;) go say(\u0026#34;Async2\u0026#34;) go say(\u0026#34;Async3\u0026#34;) time.Sleep(time.Second * 3) // anonymous goroutine \tvar wait sync.WaitGroup wait.Add(2) go func() { defer wait.Done() fmt.Println(\u0026#34;Hello\u0026#34;) }() go func(msg string) { defer wait.Done() fmt.Println(msg) }(\u0026#34;Hi\u0026#34;) wait.Wait() // Concurrency vs Parallelism \truntime.GOMAXPROCS(3) go say2(\u0026#34;Async1\u0026#34;) go say2(\u0026#34;Async2\u0026#34;) go say2(\u0026#34;Async3\u0026#34;) time.Sleep(time.Second * 12) // channel \tch := make(chan int) go func() { ch \u0026lt;- 123 }() var i int i = \u0026lt;-ch println(i) // channel 2 \tdone := make(chan bool) go func() { for i := 0; i \u0026lt; 1000; i++ { fmt.Println(i) } done \u0026lt;- true }() \u0026lt;-done // channel deadlock \t// c := make(chan int) \t// c \u0026lt;- 1 \t// fmt.Println(\u0026lt;-c)  // buffer channel \tch = make(chan int, 1) ch \u0026lt;- 101 fmt.Println(\u0026lt;-ch) // channel sending/receiving \tchstr := make(chan string, 1) sendChnn(chstr) receiveChnn(chstr) // channel close \tch = make(chan int, 3) ch \u0026lt;- 1 ch \u0026lt;- 2 ch \u0026lt;- 3 close(ch) println(\u0026lt;-ch) println(\u0026lt;-ch) if _, success := \u0026lt;-ch; !success { println(\u0026#34;no data\u0026#34;) } // channel receiving 1 \tch = make(chan int, 5) ch \u0026lt;- 1 ch \u0026lt;- 2 ch \u0026lt;- 3 ch \u0026lt;- 3 ch \u0026lt;- 2 close(ch) for i := range ch { println(i) } // channel receiving 2 \tch = make(chan int, 5) ch \u0026lt;- 1 ch \u0026lt;- 2 ch \u0026lt;- 3 ch \u0026lt;- 1 ch \u0026lt;- 4 close(ch) for { if i, success := \u0026lt;-ch; success { println(i) } else { break } } // channel select \tdone1 := make(chan bool) done2 := make(chan bool) go run1(done1) go run2(done2) EXIT: for { select { case \u0026lt;-done1: println(\u0026#34;run1 finished\u0026#34;) case \u0026lt;-done2: println(\u0026#34;run2 finished\u0026#34;) break EXIT } } } func run1(done chan bool) { time.Sleep(1 * time.Second) done \u0026lt;- true } func run2(done chan bool) { time.Sleep(2 * time.Second) done \u0026lt;- true } func sendChnn(ch chan\u0026lt;- string) { ch \u0026lt;- \u0026#34;Data\u0026#34; } func receiveChnn(ch \u0026lt;-chan string) { data := \u0026lt;-ch fmt.Println(data) }     Source : github-go_basic\n ë‹¤ìŒê¸€  "});index.add({'id':17,'href':'/docs/documents/backend/rust/rust02/','title':"Rust Advanced",'section':"Rust",'content':"Rust Advanced #   ì§ì„±ì¤‘.\n ë‹¤ìŒê¸€  "});index.add({'id':18,'href':'/docs/documents/backend/rust/rust01/','title':"Rust Basic",'section':"Rust",'content':"Rust Basic 1 #   RustëŠ” ì‹œìŠ¤í…œ ë ˆë²¨ì˜ í”„ë¡œê·¸ë˜ë° ì–¸ì–´. C/C++ì˜ ëŒ€ì²´ ì–¸ì–´ì˜ íŠ¹ì„±. íƒ€ì… ì•ˆì „ì„±, ë©”ëª¨ë¦¬ ì•ˆì „ì„±, ë™ì‹œì„±, ê·¸ë¦¬ê³  ì„±ëŠ¥ì—ë„ ì´ˆì ì„ ë§ì¶”ì–´ ëŒ€ê·œëª¨, ê³ ì„±ëŠ¥ ì†Œí”„íŠ¸ì›¨ì–´ì˜ ê°œë°œì„ ì‘ì„±í•˜ë„ë¡ ì„¤ê³„ë¨. ë¬´íš¨í•œ ë©”ëª¨ë¦¬ ì ‘ê·¼ì„ ì˜ˆë°©í•˜ê¸° ìœ„í•´ ë¹Œë¦¼ ê²€ì‚¬ì™€ ê°™ì€ ë…íŠ¹í•œ íŠ¹ì„±ì„ ì§€ë‹˜.\ní•˜ê¸° ë§í¬ ì°¸ì¡°.\n The Rust Programming Language The Rust Programming Language (KOR)  Hello World #  Hello World\ncargo init rust_hello --bin cd rust_hello cargo run fn main() { println!(\u0026#34;Hello, world!\u0026#34;); } Guessing Game #   source ...  extern crate rand; use rand::Rng; use std::cmp::Ordering; use std::io; fn main() { // ì²« ì¤„ ì¶œë ¥ \tprintln!(\u0026#34;Guess the number!\u0026#34;); // 1 ë¶€í„° 100ê¹Œì§€ì˜ ìˆ«ìë¥¼ Rangdomìœ¼ë¡œ ìƒì„± í›„ secret_nmberë¡œ ì„ ì–¸  let secret_number = rand::thread_rng().gen_range(1, 101); loop { // ì§ˆë¬¸ ì¶œë ¥  println!(\u0026#34;Please input your guess.\u0026#34;); // guessë¥¼ mutable í˜•íƒœì˜ stringìœ¼ë¡œ ìƒì„±  let mut guess = String::new(); // guessì˜ ì…ë ¥ì„ ë°›ìŒ  io::stdin() .read_line(\u0026amp;mut guess) .expect(\u0026#34;Failed to read line\u0026#34;); // guessë¥¼ trimí•˜ê³  ìˆ«ìë¡œ ì „í™˜  let guess: u32 = match guess.trim().parse() { Ok(num) =\u0026gt; num, Err(_) =\u0026gt; continue, }; // ì‚¬ìš©ìì˜ guessë¥¼ ì¬ì¶œë ¥  println!(\u0026#34;You guessed: {}\u0026#34;, guess); // secret_numberì˜ ìˆ«ìë¥¼ ë¹„êµí•˜ì—¬ ê²°ê³¼ë¥¼ ì¶œë ¥  match guess.cmp(\u0026amp;secret_number) { Ordering::Less =\u0026gt; println!(\u0026#34;Too small!\u0026#34;), Ordering::Greater =\u0026gt; println!(\u0026#34;Too big!\u0026#34;), Ordering::Equal =\u0026gt; { println!(\u0026#34;You win!\u0026#34;); break; } } } }      source check ...  let foo = 5;\t// immutable let mut bar = 5;\t// mutable let baz = String::new(); ê¸°ë³¸ì ìœ¼ë¡œ letê³¼ let mutì€ ë³€ìˆ˜ì˜ ê°€ë³€ì„±ì— ì°¨ì´ë¥¼ ë‘ê³  ë‹¤ë¥´ê²Œ ì„ ì–¸í•˜ëŠ” ê²ƒì„ ì˜ë¯¸.String::newëŠ” Stringíƒ€ì…ì˜ ì—°ê´€í•¨ìˆ˜ë¡œ OOì—ì„œ í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ ì—†ì´ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” ì •ì  ë©”ì†Œë“œì™€ ë™ì¼.\nio::stdin() .read_line(\u0026amp;mut guess) .expect(\u0026#34;Failed to read line\u0026#34;); í”„ë¡œê·¸ë¨ì˜ ì‹œì‘ì ì— \u0026lsquo;use std::io\u0026rsquo;ë¡œ ëª…ì‹œë˜ì—ˆê¸° ë•Œë¬¸ì— í•¨ìˆ˜ í˜¸ì¶œì‹œì˜ io::stdinì€ í‘œì¤€ ì…ë ¥ í•¸ë“¤ íƒ€ì…ì¸ std::io::Stdinì„ ëŒë ¤ì¤Œ. \u0026amp;mut guessë¥¼ ë„˜ê²¨ë°›ì•„ ì‚¬ìš©ìì˜ ì…ë ¥ì„ ì €ì¥í•˜ê²Œ ë¨. ì´ë•Œ ì‚¬ìš©ìì˜ ì…ë ¥ì€ ì§€ì†ì ìœ¼ë¡œ ì¶”ê°€ë¨ìœ¼ë¡œ ê°€ë³€ ë¬¸ìì—´ì´ì–´ì•¼ í•¨. ì´ë•Œ \u0026lsquo;\u0026amp;mut\u0026rsquo; ì¸ ì´ìœ ëŠ” Rustì—ì„œëŠ” \u0026lsquo;\u0026amp;\u0026rsquo; cì°¸ì¡°ìì˜ ê¸°ë³¸ ì†ì„±ì´ ì•ˆì „ì„±ì„ ìœ„í•´ Immutableì„ìœ¼ë¡œ ëª…ì‹œì ìœ¼ë¡œ ê°€ë³€ì„ì„ ì§€ì •í•  í•„ìš”ê°€ ìˆìŒ. \u0026lsquo;.expect\u0026rsquo; ëŠ” ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë©”ì†Œë“œë¡œ ì¸ìŠ¤í„°ìŠ¤ê°€ Errorë¥¼ ë‚¼ ê²½ìš° í•´ë‹¹ ë©”ì„¸ì§€ë¥¼ ì¶œë ¥í•˜ë©° ë™ì‘ì„ ë©ˆì¶”ê²Œ ë¨. ë§Œì¼ \u0026lsquo;expec\u0026rsquo;ì—†ì´ ì»´íŒŒì¼ í•œë‹¤ë©´ ê²½ê³ ê°€ í‘œì‹œë¨.\n// place holder println!(\u0026#34;You guessed: {}\u0026#34;, guess); // place holder ì§€ì • println!(\u0026#34;You guessed: {g} number {n}\u0026#34;, g= guess,n = number); ìœ„ ë¬¸ì¥ì—ì„œ \u0026lsquo;{}\u0026rsquo; ë¶€ë¶„ì€ ë³€ê²½ì(Place Holder)ë¡œ guessì˜ í‘œì‹œ ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ëƒ„. í•˜ë‚˜ ì´ìƒì˜ Place Holderë¥¼ ì•„ë˜ì²˜ëŸ¼ ì§€ì •í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŒ.\n// Cargo.toml [dependencies] rand = \u0026#34;0.4.0\u0026#34; Random ì²˜ë¦¬ë¥¼ ìœ„í•´ Cargo.tomlì— ì˜ì¡´ì„±ì„ ì¶”ê°€í•˜ì—¬ rand crateë¥¼ ëª…ì‹œí•¨.\nmatch guess.cmp(\u0026amp;secret_number) { Ordering::Less =\u0026gt; println!(\u0026#34;Too small!\u0026#34;), Ordering::Greater =\u0026gt; println!(\u0026#34;Too big!\u0026#34;), Ordering::Equal =\u0026gt; { println!(\u0026#34;You win!\u0026#34;); break; } ì‚¬ìš©ìê°€ ì…ë ¥í•œ guess ìˆ«ìì™€ Randomìœ¼ë¡œ ìƒì„±ëœ secret_numberë¥¼ ë¹„êµí•˜ëŠ” ë©”ì†Œë“œ. std í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œë¶€í„° std::cmp::Ordering Scopeë¥¼ ê°€ì ¸ì™€ì„œ Ordering íƒ€ì…ì„ ì‚¬ìš©í•˜ëŠ” í˜•íƒœ.\nlet guess: u32 = match guess.trim().parse() { Ok(num) =\u0026gt; num, Err(_) =\u0026gt; continue, }; ì…ë ¥ëœ guessì˜ ë¬¸ìì—´ì„ secret_numberì™€ ë¹„êµí•˜ê¸° ìœ„í•´ ìˆ«ìë¡œ ë°”ê¾¸ëŠ” ë¼ì¸. ê¸°ì¡´ guess ë³€ìˆ˜ë¥¼ trimingëœ ìˆ«ìë¡œ Shadowingí•˜ì—¬ ì¬ì‚¬ìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ë¨. \u0026lsquo;guess.trim()\u0026lsquo;ì„ í†µí•´ ì‚¬ìš©ì ì…ë ¥ì‹œ ë“¤ì–´ì˜¨ Enterì˜ ê°œí–‰ë¬¸ìë¥¼ ì œê±°í•˜ê³  \u0026lsquo;parse()\u0026lsquo;ë¥¼ í†µí•´ ìˆ«ìì—´ë¡œ íŒŒì‹±. ì´ë•Œ u32ì™€ ê°™ì´ ì •í™•í•œ íƒ€ì…ì„ ëª…ì‹œí•´ì•¼í•¨. ì‚¬ìš©ìê°€ ì…ë ¥í•œ ìˆ«ìì— íŠ¹ìˆ˜ë¬¸ì ê°™ì€ ê²ƒì´ í¬í•¨ë˜ì–´ u32ë¡œ ì „í™˜í•  ìˆ˜ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ì„œ expectì™€ ê°™ì€ ë³„ë„ ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ë„ ìˆìŒ. ì—¬ê¸°ì„œëŠ” Errorê°€ ë“¤ì–´ì˜¬ê²½ìš° continueë¥¼ í†µí•´ ë‹¤ìŒ Loopë¡œ ë‹¤ì‹œ ì‹œì‘ë˜ë„ë¡ ì²˜ë¦¬ ë¨.\nloop { ... match guess.cmp(\u0026amp;secret_number) { ... Ordering::Equal =\u0026gt; { println!(\u0026#34;You win!\u0026#34;); break; } } main í”„ë¡œê·¸ë¨ì€ Ordering::Equalì´ ë‚˜ì˜¬ë•Œê¹Œì§€ Loopë¡œ ë“¤ì–´ê°€ ë°˜ë³µë˜ë©° ìˆ˜í–‰ë¨.\n   Common Programming Concepts #  Variables / Mutability / Data Types\n variables / mutability / data types ...  fn main() { // immutable  let x = 5; println!(\u0026#34;The value of x is: {}\u0026#34;, x); // x = 7; // connot assign twice to immutable variable  println!(\u0026#34;The value of x is: {}\u0026#34;, x); // shadowing  let x = 10; println!(\u0026#34;The value of x is: {}\u0026#34;, x); // shadowing 2  let spaces = \u0026#34; \u0026#34;; println!(\u0026#34;Spaces: {}\u0026#34;, spaces); let spaces = spaces.len(); println!(\u0026#34;Spaces: {}\u0026#34;, spaces); // mutable  let mut x = 6; println!(\u0026#34;The value of x is: {}\u0026#34;, x); x = 7; // connot assign twice to immutable variable  println!(\u0026#34;The value of x is: {}\u0026#34;, x); // mutable  let mut spaces = \u0026#34; \u0026#34;; println!(\u0026#34;Spaces: {}\u0026#34;, spaces); // spaces = spaces.len(); //expected `\u0026amp;str`, found `usize`  println!(\u0026#34;Spaces: {}\u0026#34;, spaces); // const  const MAX_POINTS: u32 = 100_000; println!(\u0026#34;MAX_POINTS: {}\u0026#34;, MAX_POINTS); // let guess = \u0026#34;42\u0026#34;.parse().expect(\u0026#34;Not a number!\u0026#34;); // consider giving `guess` a type  let guess: u32 = \u0026#34;42\u0026#34;.parse().expect(\u0026#34;Not a number!\u0026#34;); // float  let f1: f64 = 2.0; let f2: f32 = 3.0; println!(\u0026#34;{},{}\u0026#34;, f1, f2); // let addition  let sum = 5 + 10; // let subtraction  let difference = 95.5 - 4.3; // let multiplication  let product = 4 * 30; // let division  let quotient = 56.7 / 32.2; // let remainder  let remainder = 43 % 5; println!( \u0026#34;let calc {},{},{},{},{}\u0026#34;, sum, difference, product, quotient, remainder ); // boolean  let t = true; let f: bool = false; println!(\u0026#34;let bool {},{}\u0026#34;, t, f); // character  let c = \u0026#39;z\u0026#39;; let z = \u0026#39;â„¤\u0026#39;; let cat = \u0026#39;ğŸ˜»\u0026#39;; println!(\u0026#34;let char {},{},{}\u0026#34;, c, z, cat); // tuple  let tup: (i32, f64, u8) = (500, 6.4, 1); let (x, y, z) = tup; println!(\u0026#34;x : {}, y : {}, z : {}\u0026#34;, x, y, z); let x: (i32, f64, u8, u8, u8, u8, u8, u8, u8, u8, u8, u8, u8) = (500, 6.4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11); let five_hundred = x.0; println!(\u0026#34;five_hundred : {}, x11 : {}\u0026#34;, five_hundred, x.11); // Array  let ary = [1, 2, 3, 4, 5]; let mon = [ \u0026#34;Jan\u0026#34;, \u0026#34;Feb\u0026#34;, \u0026#34;Mar\u0026#34;, \u0026#34;Apr\u0026#34;, \u0026#34;May\u0026#34;, \u0026#34;Jun\u0026#34;, \u0026#34;Jul\u0026#34;, \u0026#34;Aug\u0026#34;, \u0026#34;Sep\u0026#34;, \u0026#34;Oct\u0026#34;, \u0026#34;Nov\u0026#34;, \u0026#34;Dec\u0026#34;, ]; let index = 4; // let index = 12; // panicked at \u0026#39;index out of bounds: the len is 12 but the index is 12\u0026#39;  let frst = ary[0]; let scnd = mon[1]; let thrd = mon[index]; println!(\u0026#34;frst : {} , scnd : {} , thrd : {}\u0026#34;, frst, scnd, thrd); }     Functions\n functions ...  fn main() { println!(\u0026#34;Hello, world!\u0026#34;); // function  function_a(); // function parameter  function_b(3); // function parameters  function_c(3, 5); // statement  let stmt1 = 6; println!(\u0026#34;stmt1 : {}\u0026#34;, stmt1); // let stmt1 = (let stmt2 = 5); // variable declaration using `let` is a statement  let stmt1 = 5; // expression  let stmt2 = { let stmt1 = 3; stmt1 + 1 }; println!(\u0026#34;stmt1 : {}, stmt2 : {}\u0026#34;, stmt1, stmt2); // function with return values  let stmt3 = function_d(); println!(\u0026#34;stmt3 : {}\u0026#34;, stmt3); // function with return values  let stmt4 = function_e(5); println!(\u0026#34;stmt4 : {}\u0026#34;, stmt4); } fn function_a() { println!(\u0026#34;Hello, again!\u0026#34;) } fn function_b(x: i32) { for i in 0..x { println!(\u0026#34;{} : Hello, again!\u0026#34;, i) } } fn function_c(x: i32, y: i32) { for i in x..y { println!(\u0026#34;{}: Hello, again! {},{}\u0026#34;, i, x, y) } } fn function_d() -\u0026gt; i32 { 5 } fn function_e(x: i32) -\u0026gt; i32 { // without semicolon  x + 1 }     Contrl Flow\n control flow ...  fn main() { // if else 1  let number = 3; if number \u0026lt; 5 { println!(\u0026#34;{} condition was true\u0026#34;, number); } else { println!(\u0026#34;{} condition was false\u0026#34;, number); }; // if else 2  let number = 7; if number \u0026lt; 5 { println!(\u0026#34;{} condition was true\u0026#34;, number); } else { println!(\u0026#34;{} condition was false\u0026#34;, number); }; // if else 3  if number != 0 { println!(\u0026#34;{} number was soething other than zero\u0026#34;, number); }; // if else 4  if number % 4 == 0 { println!(\u0026#34;{} number is divisible by 4\u0026#34;, number) } else if number % 3 == 0 { println!(\u0026#34;{} number is divisible by 3\u0026#34;, number) } else if number % 2 == 0 { println!(\u0026#34;{} number is divisible by 2\u0026#34;, number) } else { println!(\u0026#34;{} number is not divisible by 4,3,2\u0026#34;, number) }; // if with let  let cond = true; let numb = if cond { 5 } else { 6 }; println!(\u0026#34;This value of numb is : {}\u0026#34;, numb); // while  let mut numc = 3; while numc != 0 { println!(\u0026#34;{}\u0026#34;, numc); numc = numc - 1; } println!(\u0026#34;numc : {}\u0026#34;, numc); // for collection  let arry = [10, 20, 30, 40, 50]; for elem in arry.iter() { println!(\u0026#34;the value is: {}\u0026#34;, elem); } // for range rev  for numd in (1..4).rev() { println!(\u0026#34;{}!\u0026#34;, numd); } }     Understanding Ownership #  Stack vs Heap ìŠ¤íƒì€ ê°’ì„ ë°›ì•„ë“¤ì¸ ìˆœì„œëŒ€ë¡œ ì €ì¥í•˜ê³  ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ê°’ì„ ì§€ìš°ëŠ” ë°©ì‹. (Last In, First Out) í™ì€ íŠ¹ì • ê³µê°„ì„ í• ë‹¹í•˜ê³  ìœ„ì¹˜ ê²€ìƒ‰ì„ í†µí•´ ê°’ì„ ë¶ˆëŸ¬ ì˜¤ëŠ” ë°©ì‹.\n   êµ¬ë¶„ Stack Heap     ê´€ë¦¬ CPU Programmer   ì†ë„ ë¹ ë¦„ ëŠë¦¼   ë²”ìœ„ ì§€ì—­ ì „ì—­   í¬ê¸° ì œí•œ ì œí•œì—†ìŒ    Shallow Copy vs Deep Copy RustëŠ” Shallow Copyë¥¼ Moveë¼ í‘œí˜„. ëª…ì‹œì ìœ¼ë¡œ í‘œì‹œí•˜ì§€ ì•ŠëŠ” ì´ìƒ Deep CopyëŠ” ë°œìƒë˜ì§€ ì•ŠìŒ.\n ownership ...  fn main() { // move  let str1 = String::from(\u0026#34;hello\u0026#34;); let str2 = str1; // shallow copy  //println!(\u0026#34;{}\u0026#34;, str1); // move occurs because `str1` has type `std::string::String`  println!(\u0026#34;{}\u0026#34;, str2); // function takes ownership  let str3 = String::from(\u0026#34;world\u0026#34;); take_owns(str3); //println!(\u0026#34;{}\u0026#34;, str3); //value borrowed here after move  // integer copy to function  let intg = 5; make_copy(5); println!(\u0026#34;{}\u0026#34;, intg); //  let str4 = String::from(\u0026#34;again\u0026#34;); let str5 = take_back(str4); //println!(\u0026#34;{}\u0026#34;, str4); //move occurs because `str4` has type `std::string::String`  println!(\u0026#34;{}\u0026#34;, str5); // Reference  let str6 = String::from(\u0026#34;This is a pen\u0026#34;); let leng = calc_leng(\u0026amp;str6); println!(\u0026#34;The length of \u0026#39;{}\u0026#39; is {}\u0026#34;, str6, leng); // Reference  let mut str7 = String::from(\u0026#34;This is a book\u0026#34;); chng_strg(\u0026amp;mut str7); println!(\u0026#34;{}\u0026#34;, str7); // Reference mutable  let mut str8 = String::from(\u0026#34;text1\u0026#34;); let str9 = \u0026amp;mut str8; //first mutable borrow occurs here  // let str10 = \u0026amp;mut str8; //second mutable borrow occurs here  println!(\u0026#34;{}\u0026#34;, str9); // println!(\u0026#34;{}\u0026#34;, str10)  // Reference mutable 1  let mut str11 = String::from(\u0026#34;text2\u0026#34;); { let str12 = \u0026amp;mut str11; println!(\u0026#34;{}\u0026#34;, str12); } let str13 = \u0026amp;mut str11; println!(\u0026#34;{}\u0026#34;, str13); // Reference mutable 2  let mut str14 = String::from(\u0026#34;text3\u0026#34;); let str15 = \u0026amp;str14; let str16 = \u0026amp;str14; // let str17 = \u0026amp;mut str14; //mutable borrow occurs here  println!(\u0026#34;{}\u0026#34;, str15); println!(\u0026#34;{}\u0026#34;, str16); // println!(\u0026#34;{}\u0026#34;, str17);  // dangling references  let noth = dang_refr(); println!(); // slice  let mut str17 = String::from(\u0026#34;fffirst words\u0026#34;); let str18 = frst_word(\u0026amp;str17); str17.clear(); println!(\u0026#34;{}\u0026#34;, str18); // string slice  let str19 = String::from(\u0026#34;hello world\u0026#34;); let str20 = \u0026amp;str19[0..5]; let str21 = \u0026amp;str19[6..11]; println!(\u0026#34;{},{},{}\u0026#34;, str19, str20, str21); let str22 = \u0026amp;str19[..5]; let leng = str19.len(); let str23 = \u0026amp;str19[6..leng]; println!(\u0026#34;{},{},{}\u0026#34;, str22, leng, str23); // literal slice  let mut str24 = String::from(\u0026#34;hello world\u0026#34;); let str25 = frst_word_s(\u0026amp;str24); // str24.clear(); // mutable borrow occurs here  println!(\u0026#34;str25 {}\u0026#34;, str25); // literal slice  let str26 = String::from(\u0026#34;hello world\u0026#34;); let str27 = frst_word_s(\u0026amp;str26[..]); let str28 = \u0026#34;HELLO WORLD\u0026#34;; let str27 = frst_word_s(\u0026amp;str28[..]); let str27 = frst_word_s(str28); println!(\u0026#34;str27 {}\u0026#34;, str27); // etc slice  let intg = [1, 2, 3, 4, 5]; let ints = \u0026amp;intg[1..3]; println!(\u0026#34;{}{}\u0026#34;, ints[0], ints[1]) } fn take_owns(strg: String) { println!(\u0026#34;{}\u0026#34;, strg); } fn make_copy(intg: i32) { println!(\u0026#34;{}\u0026#34;, intg); } fn take_back(strg: String) -\u0026gt; String { strg } fn calc_leng(strg: \u0026amp;String) -\u0026gt; usize { strg.len() } fn chng_strg(strg: \u0026amp;mut String) { strg.push_str(\u0026#34;,Hello?\u0026#34;); } // this function\u0026#39;s return type contains a borrowed value // fn dang_refr() -\u0026gt; \u0026amp;String { // let strg = String::from(\u0026#34;Eh?\u0026#34;); // \u0026amp;strg // }  fn dang_refr() -\u0026gt; String { let strg = String::from(\u0026#34;Eh?\u0026#34;); strg } fn frst_word(strg: \u0026amp;String) -\u0026gt; usize { let bytes = strg.as_bytes(); for (i, \u0026amp;item) in bytes.iter().enumerate() { if item == b\u0026#39; \u0026#39; { return i; } } strg.len() } // fn frst_word_s(strg: \u0026amp;String) -\u0026gt; \u0026amp;str { // let bytes = strg.as_bytes();  // for (i, \u0026amp;item) in bytes.iter().enumerate() { // if item == b\u0026#39; \u0026#39; { // return \u0026amp;strg[0..i]; // } // } // \u0026amp;strg[..] // }  fn frst_word_s(strg: \u0026amp;str) -\u0026gt; \u0026amp;str { let bytes = strg.as_bytes(); for (i, \u0026amp;item) in bytes.iter().enumerate() { if item == b\u0026#39; \u0026#39; { return \u0026amp;strg[0..i]; } } \u0026amp;strg[..] }     Using Structs #   struct / function ...  struct User { name: String, mail: String, sign: u64, actv: bool, } fn add_user(mail: String, name: String) -\u0026gt; User { User { name, mail, sign: 1, actv: true, } } struct Color(i32, i32, i32); struct Point(i32, i32, i32); // Struct with reference (Lifetimes) // struct UserLife { // name: \u0026amp;str, // expected named lifetime parameter // mail: \u0026amp;str, // expected named lifetime parameter // sign: u64, // actv: bool, // }  // struct 1 fn area1(leng: u32, wdth: u32) -\u0026gt; u32 { leng * wdth } // struct 2 fn area2(dims: (u32, u32)) -\u0026gt; u32 { dims.0 * dims.1 } #[derive(Debug)] // struct 3 struct Rect { leng: u32, wdth: u32, } fn area3(rect: \u0026amp;Rect) -\u0026gt; u32 { rect.leng * rect.wdth } #[derive(Debug)] // struct 4 struct Rectangle { length: u32, width: u32, } impl Rectangle { fn area(\u0026amp;self) -\u0026gt; u32 { self.length * self.width } fn hold(\u0026amp;self, othr: \u0026amp;Rectangle) -\u0026gt; bool { self.length \u0026gt; othr.length \u0026amp;\u0026amp; self.width \u0026gt; othr.width } fn sqre(size: u32) -\u0026gt; Rectangle { Rectangle { length: size, width: size, } } } fn main() { // struct  let user1 = User { name: String::from(\u0026#34;Jake\u0026#34;), mail: String::from(\u0026#34;admin@abc.com\u0026#34;), sign: 1, actv: true, }; let user2 = User { name: String::from(\u0026#34;Hans\u0026#34;), mail: String::from(\u0026#34;aaa@abc.com\u0026#34;), sign: user1.sign, actv: user1.actv, }; let user3 = User { name: String::from(\u0026#34;Dale\u0026#34;), mail: String::from(\u0026#34;aaa@abc.com\u0026#34;), ..user1 }; // tuple  println!(\u0026#34;{}\u0026#34;, user1.name); println!(\u0026#34;{}\u0026#34;, user2.name); println!(\u0026#34;{}\u0026#34;, user3.name); let blck = Color(0, 0, 0); let orgn = Point(0, 0, 0); // // struct with reference  // let user1 = User {  // name: String::from(\u0026#34;Jake\u0026#34;),  // mail: String::from(\u0026#34;admin@abc.com\u0026#34;),  // sign: 1,  // actv: true,  // };  let leng = 50; let wdth = 30; println!(\u0026#34;{}\u0026#34;, area1(leng, wdth)); let rect1 = (50, 30); println!(\u0026#34;{}\u0026#34;, area2(rect1)); let rect2 = Rect { leng: 50, wdth: 30 }; println!(\u0026#34;{}\u0026#34;, area3(\u0026amp;rect2)); // derive(Debug)  println!(\u0026#34;rect is {:?}\u0026#34;, rect2); // struct implementation  let rect3 = Rectangle { length: 50, width: 30, }; println!(\u0026#34;{:?},{}\u0026#34;, rect3, rect3.area()); let rect4 = Rectangle { length: 50, width: 30, }; let rect5 = Rectangle { length: 40, width: 10, }; let rect6 = Rectangle { length: 45, width: 60, }; println!(\u0026#34;rect4 hold rect5 {}\u0026#34;, rect4.hold(\u0026amp;rect5)); println!(\u0026#34;rect4 hold rect6 {}\u0026#34;, rect4.hold(\u0026amp;rect6)); // associated functions  let sqre = Rectangle::sqre(3); println!(\u0026#34;sqre : {:?}\u0026#34;, sqre) }     Enums and Pattern Matching #   enums ...  #[derive(Debug)] enum IpAddrKind { V4, V6, } #[derive(Debug)] struct IpAddr { kind: IpAddrKind, addr: String, } #[derive(Debug)] enum IpAddrStr { V4(String), V6(String), } #[derive(Debug)] enum IpAddrU8 { V4(u8, u8, u8, u8), V6(String), } #[derive(Debug)] enum IpAddrStrc { V4(Ipv4Addr), V6(Ipv6Addr), } #[derive(Debug)] struct Ipv4Addr { addr: String, } #[derive(Debug)] struct Ipv6Addr { addr: String, } fn route(ip_type: IpAddrKind) { println!(\u0026#34;{:?}\u0026#34;, ip_type) } #[derive(Debug)] enum Mssg { Quit, Move { x: i32, y: i32 }, Write(String), ChangeColor(i32, i32, i32), } impl Mssg { fn call(\u0026amp;self) { println!(\u0026#34;Mssg::call\u0026#34;) } } struct QuitMessage; struct MoveMessage { x: i32, y: i32, } struct WriteMessage(String); struct ChangeColorMessage(i32, i32, i32); enum Optn\u0026lt;T\u0026gt; { Some(T), None, } fn main() { // enum 1  let ip4 = IpAddrKind::V4; let ip6 = IpAddrKind::V6; route(IpAddrKind::V4); route(IpAddrKind::V6); route(ip4); route(ip6); // enum 2  let home = IpAddr { kind: IpAddrKind::V4, addr: String::from(\u0026#34;127.0.0.1\u0026#34;), }; let lbck = IpAddr { kind: IpAddrKind::V6, addr: String::from(\u0026#34;::1\u0026#34;), }; println!(\u0026#34;{:?}\u0026#34;, home); println!(\u0026#34;{:?}\u0026#34;, lbck); // enum 3  let home = IpAddrStr::V4(String::from(\u0026#34;127.0.0.1\u0026#34;)); let lbck = IpAddrStr::V6(String::from(\u0026#34;::1\u0026#34;)); println!(\u0026#34;{:?}\u0026#34;, home); println!(\u0026#34;{:?}\u0026#34;, lbck); // enum 4  let home = IpAddrU8::V4(127, 0, 0, 1); let lbck = IpAddrU8::V6(String::from(\u0026#34;::1\u0026#34;)); println!(\u0026#34;{:?}\u0026#34;, home); println!(\u0026#34;{:?}\u0026#34;, lbck); // enum 5  let home = IpAddrStrc::V4(Ipv4Addr { addr: String::from(\u0026#34;127.0.0.1\u0026#34;), }); let lbck = IpAddrStrc::V6(Ipv6Addr { addr: String::from(\u0026#34;::1\u0026#34;), }); println!(\u0026#34;{:?}\u0026#34;, home); println!(\u0026#34;{:?}\u0026#34;, lbck); // enum 6  let mssg = Mssg::Write(String::from(\u0026#34;Hello\u0026#34;)); mssg.call(); // enum 7  let numb = Some(5); let strg = Some(\u0026#34;A string\u0026#34;); let abst: Option\u0026lt;i32\u0026gt; = None; println!(\u0026#34;{:?}\u0026#34;, numb); println!(\u0026#34;{:?}\u0026#34;, strg); println!(\u0026#34;{:?}\u0026#34;, abst); }      match ...  #[derive(Debug)] enum UsState { Alabama, Alaska, } enum Coin { Penny, Nickel, Dime, Quater(UsState), } fn value_in_cents(coin: Coin) -\u0026gt; u32 { // enum \u0026amp; match  match coin { Coin::Penny =\u0026gt; { println!(\u0026#34;Lucky penny!\u0026#34;); 1 } Coin::Nickel =\u0026gt; 5, Coin::Dime =\u0026gt; 10, Coin::Quater(state) =\u0026gt; { println!(\u0026#34;State quater from {:?}!\u0026#34;, state); 25 } } } fn plus_one(x: Option\u0026lt;i32\u0026gt;) -\u0026gt; Option\u0026lt;i32\u0026gt; { match x { None =\u0026gt; None, Some(i) =\u0026gt; Some(i + 1), } } fn main() { // enum 1  let coin1 = Coin::Penny; println!(\u0026#34;{}\u0026#34;, value_in_cents(coin1)); // enum2  let coin2 = Coin::Quater(UsState::Alaska); println!(\u0026#34;{}\u0026#34;, value_in_cents(coin2)); // Option\u0026lt;T\u0026gt; Matching  let five = Some(5); let six = plus_one(five); let none = plus_one(None); println!(\u0026#34;{:?}\u0026#34;, five); println!(\u0026#34;{:?}\u0026#34;, six); println!(\u0026#34;{:?}\u0026#34;, none); // Placeholder  let some_u8_value = 0u8; match some_u8_value { 1 =\u0026gt; println!(\u0026#34;one\u0026#34;), 3 =\u0026gt; println!(\u0026#34;three\u0026#34;), 5 =\u0026gt; println!(\u0026#34;five\u0026#34;), 6 =\u0026gt; println!(\u0026#34;seven\u0026#34;), 7 =\u0026gt; (), _ =\u0026gt; println!(\u0026#34;???\u0026#34;), } let some_u8_value = Some(3); // before if let  match some_u8_value { Some(3) =\u0026gt; println!(\u0026#34;three\u0026#34;), _ =\u0026gt; (), } // after if let (syntax sugar)  if let Some(3) = some_u8_value { println!(\u0026#34;Three\u0026#34;); } // let mut / if let else  let coin2 = Coin::Quater(UsState::Alaska); let mut count = 0; match coin2 { Coin::Quater(state) =\u0026gt; println!(\u0026#34;State {:?}\u0026#34;, state), _ =\u0026gt; count += 1, } let coin2 = Coin::Quater(UsState::Alabama); if let Coin::Quater(state) = coin2 { println!(\u0026#34;State {:?}\u0026#34;, state); } else { count += 1; } }     Mods #   mods ...  main.rs\nextern crate rust_comm; pub mod a { pub mod sers { pub mod of { pub fn nest_mods() {} } } } use a::sers::of; use a::sers::of::nest_mods; enum TrffLight { Red, Yellow, Green, } // use greenì€ ì œì™¸ // use TrffLight::{Red, Yellow}; // useì—ì„œ ì „ì²´ í¬í•¨ use TrffLight::*; fn main() { rust_comm::client::connect(); // importing names \ta::sers::of::nest_mods(); // use short type \tof::nest_mods(); // use short type \tnest_mods(); let red = Red; let yellow = Yellow; //let green1 = TrffLight::Green; \tlet green2 = Green; } lib.rs\n// rust_comm // â¨½ network // â¨½ client // // network::connect() // mod network { // fn connect() { // } // }  // // client::connect() // mod client { // fn connect() { // } // }  // rust_comm // â¨½ network // â¨½ client // network::connect // network::client::connect // mod network { // fn connect() {} // mod client { // fn connect() {} // } // }  // rust_comm // â¨½ client // â¨½ network // â¨½ server // client // mod client { // fn connect() { // } // } // // network // // network::server // mod network { // fn connect() { // } // mod server { // fn connect() { // } // } // }  // src/client.rs pub mod client; // src/networ.rs pub mod network; // src/network/client2.rs // pub mod client2;  mod outm { pub fn midd_func() {} pub fn midd_scrt_func() {} pub mod insd { pub fn innr_func() {} pub fn scrt_func() {} } } fn try_me() { outm::midd_func(); outm::midd_scrt_func(); outm::insd::innr_func(); outm::insd::scrt_func(); } #[cfg(test)] mod tests { use super::client2; #[test] fn it_works() { assert_eq!(2 + 2, 4); // client2::connect();  client2::connect(); } } client.rs\npub fn connect() {} \\network\\mod.rs\npub fn connect() {} mod server; \\network\\server.rs\nfn connect() {} \\network\\client2.rs\npub fn connect2() {}     Collections #   vectors ...  #[derive(Debug)] enum SpreadsheetCell { Int(i32), Float(f64), Text(String), } fn main() { // generic  let v1: Vec\u0026lt;i32\u0026gt; = Vec::new(); let v1 = vec![1, 2, 3]; println!(\u0026#34;{:?}\u0026#34;, v1); // push  let mut v1 = Vec::new(); v1.push(5); v1.push(6); v1.push(7); v1.push(8); println!(\u0026#34;{:?}\u0026#34;, v1); // vector scope  { let v2 = vec![1, 2, 3, 4]; println!(\u0026#34;{:?}\u0026#34;, v2); } // println!(\u0026#34;{:?}\u0026#34;, v2);  // get  let v3 = vec![1, 2, 3, 4, 5]; let third: \u0026amp;i32 = \u0026amp;v3[2]; let fourth: Option\u0026lt;\u0026amp;i32\u0026gt; = v3.get(3); let sixth: Option\u0026lt;\u0026amp;i32\u0026gt; = v3.get(5); println!(\u0026#34;{:?},{:?},{:?}\u0026#34;, third, fourth, sixth); // Vecto Push (???)  let mut v4 = vec![1, 2, 3, 4, 5]; let second = \u0026amp;v4[1]; v4.push(6); println!(\u0026#34;{:?}\u0026#34;, v4); // Vector + for  let v5 = vec![10, 3, 2, 5, 7]; for i in \u0026amp;v5 { println!(\u0026#34;{}\u0026#34;, i); } println!(\u0026#34;{:?}\u0026#34;, v5); // Vector + for  let mut v5 = vec![10, 3, 2, 5, 7]; for i in \u0026amp;mut v5 { *i += 50; println!(\u0026#34;{}\u0026#34;, i); } println!(\u0026#34;{:?}\u0026#34;, v5); // enum  let row = vec![ SpreadsheetCell::Int(3), SpreadsheetCell::Text(String::from(\u0026#34;Blue\u0026#34;)), SpreadsheetCell::Float(10.12), ]; println!(\u0026#34;{:?}\u0026#34;, row); }      string ...  fn main() { // str1  let mut str1 = String::new(); let str1 = \u0026#34;hello there?\u0026#34;; // str2  let dat1 = \u0026#34;initial contents\u0026#34;; let str2 = dat1.to_string(); let str2 = \u0026#34;initial contents\u0026#34;.to_string(); // str3  let str3 = String::from(\u0026#34;This is Pen\u0026#34;); println!(\u0026#34;{} , {} , {}\u0026#34;, str1, str2, str3); // str utf8  let hello = String::from(\u0026#34;Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…\u0026#34;); let hello = String::from(\u0026#34;DobrÃ½ den\u0026#34;); let hello = String::from(\u0026#34;Hello\u0026#34;); let hello = String::from(\u0026#34;×©Ö¸××œ×•Ö¹×\u0026#34;); let hello = String::from(\u0026#34;à¤¨à¤®à¤¸à¥à¤¤à¥‡\u0026#34;); let hello = String::from(\u0026#34;ã“ã‚“ã«ã¡ã¯\u0026#34;); let hello = String::from(\u0026#34;ì•ˆë…•í•˜ì„¸ìš”\u0026#34;); let hello = String::from(\u0026#34;ä½ å¥½\u0026#34;); let hello = String::from(\u0026#34;OlÃ¡\u0026#34;); let hello = String::from(\u0026#34;Ğ—Ğ´Ñ€Ğ°Ğ²ÑÑ‚Ğ²ÑƒĞ¹Ñ‚Ğµ\u0026#34;); let hello = String::from(\u0026#34;Hola\u0026#34;); println!(\u0026#34;{}\u0026#34;, hello); // push  let mut str4 = String::from(\u0026#34;foo\u0026#34;); str4.push_str(\u0026#34;bar\u0026#34;); str4.push_str(\u0026#34;quz\u0026#34;); println!(\u0026#34;{}\u0026#34;, str4); // push_str : ownership  let mut str5 = String::from(\u0026#34;foo\u0026#34;); let str6 = \u0026#34;bar\u0026#34;; str5.push_str(\u0026amp;str6); println!(\u0026#34;{} , {}\u0026#34;, str5, str6); // push (lol)  let mut str7 = String::from(\u0026#34;lo\u0026#34;); str7.push(\u0026#39;l\u0026#39;); println!(\u0026#34;{}\u0026#34;, str7); // combine  let str8 = String::from(\u0026#34;Hello,\u0026#34;); let str9 = String::from(\u0026#34;World!\u0026#34;); // let str10 = str8 + str9; //expected `\u0026amp;str`, found struct `std::string::String`  // add moethod -\u0026gt; fn add(self, s: \u0026amp;str) -\u0026gt; String {}  // \u0026amp;String -\u0026gt; \u0026amp;str : deref coercion  let str10 = str8 + \u0026amp;str9; //println!(\u0026#34;{}\u0026#34;, str8); //value borrowed here after move  println!(\u0026#34;{}\u0026#34;, str9); println!(\u0026#34;{}\u0026#34;, str10); // +  let str11 = String::from(\u0026#34;tic\u0026#34;); let str12 = String::from(\u0026#34;tac\u0026#34;); let str13 = String::from(\u0026#34;toe\u0026#34;); let str14 = str11 + \u0026#34;-\u0026#34; + \u0026amp;str12 + \u0026#34;-\u0026#34; + \u0026amp;str13; // println!(\u0026#34;{}\u0026#34;, str11);  println!(\u0026#34;{}\u0026#34;, str12); println!(\u0026#34;{}\u0026#34;, str13); println!(\u0026#34;{}\u0026#34;, str14); // format!  let str11 = String::from(\u0026#34;tic\u0026#34;); let str15 = format!(\u0026#34;{}-{}-{}\u0026#34;, str11, str12, str13); println!(\u0026#34;{}\u0026#34;, str15); let str16 = String::from(\u0026#34;hello wrold\u0026#34;); //let str17 = str16[3]; //std::string::String` cannot be indexed by  // \u0026#34;Hello\u0026#34; \u0026#34;Ğ—Ğ´Ñ€Ğ°Ğ²ÑÑ‚Ğ²ÑƒĞ¹Ñ‚Ğµ\u0026#34; \u0026#34;ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ\u0026#34; \u0026amp;str16[0] = ???  let str17 = \u0026amp;str16[0..4]; println!(\u0026#34;{}\u0026#34;, str17); // Panic  // let str16 = String::from(\u0026#34;ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ\u0026#34;);  // let str17 = \u0026amp;str16[0..1];  // println!(\u0026#34;{}\u0026#34;, str17);  // string for  for chr in \u0026#34;ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ?\u0026#34;.chars() { println!(\u0026#34;{}\u0026#34;, chr); } // string for  for byt in \u0026#34;ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ?\u0026#34;.bytes() { println!(\u0026#34;{}\u0026#34;, byt); } }      hashmap ...  use std::collections::HashMap; fn main() { // Hash map 1  let mut scores = HashMap::new(); scores.insert(String::from(\u0026#34;Blue\u0026#34;), 10); scores.insert(String::from(\u0026#34;Yellow\u0026#34;), 50); println!(\u0026#34;{:?}\u0026#34;, scores); // hash map 2  let teams = vec![String::from(\u0026#34;Blue\u0026#34;), String::from(\u0026#34;Yellow\u0026#34;)]; let init_scores = vec![10, 50]; let score2: HashMap\u0026lt;_, _\u0026gt; = teams.iter().zip(init_scores.iter()).collect(); println!(\u0026#34;{:?}\u0026#34;, score2); let field_nm = String::from(\u0026#34;favorite color\u0026#34;); let field_val = String::from(\u0026#34;Blue\u0026#34;); let mut map = HashMap::new(); map.insert(field_nm, field_val); //println!(\u0026#34;{:?}\u0026#34;, field_nm); // value borrowed here after move  println!(\u0026#34;{:?}\u0026#34;, map); // hash map 3  let team_nm = String::from(\u0026#34;Blue\u0026#34;); let score = scores.get(\u0026amp;team_nm); println!(\u0026#34;{:?}\u0026#34;, score); // hash map for  for (key, value) in \u0026amp;scores { println!(\u0026#34;{}:{}\u0026#34;, key, value); } // hashmap insert  scores.insert(String::from(\u0026#34;Blue\u0026#34;), 25); // hash map for  for (key, value) in \u0026amp;scores { println!(\u0026#34;{}:{}\u0026#34;, key, value); } // hashmap or insert  scores.entry(String::from(\u0026#34;Red\u0026#34;)).or_insert(15); // hash map for  for (key, value) in \u0026amp;scores { println!(\u0026#34;{}:{}\u0026#34;, key, value); } let text = \u0026#34;Hello World Wonderful World Very Wonderful World And Hello Agin\u0026#34;; let mut map = HashMap::new(); for word in text.split_whitespace() { let count = map.entry(word).or_insert(0); // return \u0026amp;mut V  *count += 1; } println!(\u0026#34;{:?}\u0026#34;, map); }     Error Handling #   error handling ...  use std::fs::File; use std::io; // use std::io::ErrorKind; use std::io::Read; // enum Result\u0026lt;T, E\u0026gt; { // Ok(T), // Err(E), // }  fn read_from_file1() -\u0026gt; Result\u0026lt;String, io::Error\u0026gt; { let f = File::open(\u0026#34;not_exist_file.txt\u0026#34;); let mut f = match f { Ok(file) =\u0026gt; file, Err(e) =\u0026gt; return Err(e), }; let mut s = String::new(); match f.read_to_string(\u0026amp;mut s) { Ok(_) =\u0026gt; Ok(s), Err(e) =\u0026gt; Err(e), } } fn read_from_file2() -\u0026gt; Result\u0026lt;String, io::Error\u0026gt; { let mut f = File::open(\u0026#34;not_exist_file.txt\u0026#34;)?; let mut s = String::new(); f.read_to_string(\u0026amp;mut s)?; Ok(s) } fn read_from_file3() -\u0026gt; Result\u0026lt;String, io::Error\u0026gt; { let mut s = String::new(); File::open(\u0026#34;not_exist_file.txt\u0026#34;)?.read_to_string(\u0026amp;mut s)?; Ok(s) } fn main() { // panic direct call  // panic!(\u0026#34;crash and burn\u0026#34;);  // println!(\u0026#34;Hello, world!\u0026#34;);  // panic library call (buffer overread)  // let v = vec![1, 2, 3];  // v[99];  // RUST_BACKTRACE=full cargo run  // Error  // let f: u32 = File::open(\u0026#34;not_exists_file.txt\u0026#34;);  // Error Match  // let f = File::open(\u0026#34;not_exists_file.txt\u0026#34;);  // let f = match f {  // Ok(file) =\u0026gt; file,  // Err(ref error) if error.kind() == ErrorKind::NotFound =\u0026gt; {  // match File::create(\u0026#34;not_exists_file.txt\u0026#34;) {  // Ok(fc) =\u0026gt; fc,  // Err(e) =\u0026gt; panic!(\u0026#34;Tried Create {:?}\u0026#34;, e),  // }  // }  // Err(error) =\u0026gt; panic!(\u0026#34;Error {:?}\u0026#34;, error),  // };  // Error with unwrap  // let f = File::open(\u0026#34;not_exists_file.txt\u0026#34;).unwrap();  // Error with expect  // let f = File::open(\u0026#34;not_exists_file.txt\u0026#34;).expect(\u0026#34;Failed\u0026#34;);  // Error propagating  read_from_file1(); // Error propagating 2  read_from_file2(); // Error propagating 3  read_from_file3(); // ? without return Error  // let x = File::open(\u0026#34;not_exists_file.txt\u0026#34;)?; }     Generic, Trait, Lifetime #   generic ...  fn find_max_num(nums: \u0026amp;[i32]) -\u0026gt; i32 { let mut num_max = nums[0]; for \u0026amp;num in nums.iter() { if num \u0026gt; num_max { num_max = num; } } num_max } fn find_max_chr(chrs: \u0026amp;[char]) -\u0026gt; char { let mut chr_max = chrs[0]; for \u0026amp;chr in chrs.iter() { if chr \u0026gt; chr_max { chr_max = chr; } } chr_max } fn find_max\u0026lt;T: PartialOrd + Copy\u0026gt;(list: \u0026amp;[T]) -\u0026gt; T { let mut item_max = list[0]; for \u0026amp;item in list.iter() { if item \u0026gt; item_max { item_max = item; } } item_max } fn main() { // Find Max Number 1  let nums = vec![34, 50, 2, 5, 100, 65]; let mut num_max = nums[0]; for num in nums { if num \u0026gt; num_max { num_max = num; } } println!(\u0026#34;Number Max : {:?}\u0026#34;, num_max); // Find Max Number 2  let nums = vec![102, 34, 6000, 89, 54, 2, 43, 8]; let mut num_max = nums[0]; for num in nums { if num \u0026gt; num_max { num_max = num; } } println!(\u0026#34;Number Max : {:?}\u0026#34;, num_max); // Find Max Number 3  let nums = vec![10, 60, 150, 59, 34, 22, 13, 80]; let num_max = find_max_num(\u0026amp;nums); println!(\u0026#34;Number Max : {:?}\u0026#34;, num_max); // Find Max Chars 4  let chrs = vec![\u0026#39;a\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;z\u0026#39;]; let chr_max = find_max_chr(\u0026amp;chrs); println!(\u0026#34;Chars Max : {:?}\u0026#34;, chr_max); // Find Max 5  let nums = vec![10, 60, 150, 59, 34, 22, 13, 80]; let chrs = vec![\u0026#39;a\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;z\u0026#39;]; let num_max = find_max(\u0026amp;nums); let chr_max = find_max(\u0026amp;chrs); println!(\u0026#34;Number Max : {:?} , Chars Max : {:?}\u0026#34;, num_max, chr_max); // generic data type 1  let int1 = Point1 { x: 5, y: 10 }; let flt1 = Point1 { x: 1.0, y: 3.0 }; println!(\u0026#34;{:?},{:?}\u0026#34;, int1, flt1); // let not_work = Point { x: 5, y: 3.0 };  // generic data type 2  let int1 = Point2 { x: 5, y: 10 }; let flt1 = Point2 { x: 1.0, y: 3.0 }; let mix1 = Point2 { x: 5, y: 3.0 }; println!(\u0026#34;{:?},{:?},{:?}\u0026#34;, int1, flt1, mix1); // generic type with method  let int1 = Point1 { x: 3, y: 8 }; println!(\u0026#34;int1.x = {}, int1.y = {}\u0026#34;, int1.x(), int1.y()); // generic type with method 2  let pnt1 = Point2 { x: 3, y: 10.5 }; let pnt2 = Point2 { x: \u0026#34;Text\u0026#34;, y: \u0026#39;H\u0026#39; }; let pnt3 = pnt1.mixup(pnt2); println!(\u0026#34;{:?}\u0026#34;, pnt3); // zero cost generic \u0026lt;-- monomorphization } #[derive(Debug)] struct Point1\u0026lt;T\u0026gt; { x: T, y: T, } impl\u0026lt;T\u0026gt; Point1\u0026lt;T\u0026gt; { fn x(\u0026amp;self) -\u0026gt; \u0026amp;T { \u0026amp;self.x } fn y(\u0026amp;self) -\u0026gt; \u0026amp;T { \u0026amp;self.y } } #[derive(Debug)] struct Point2\u0026lt;A, B\u0026gt; { x: A, y: B, } impl\u0026lt;A, B\u0026gt; Point2\u0026lt;A, B\u0026gt; { fn mixup\u0026lt;C, D\u0026gt;(self, other: Point2\u0026lt;C, D\u0026gt;) -\u0026gt; Point2\u0026lt;A, D\u0026gt; { Point2 { x: self.x, y: other.y, } } }      trait ...  lib.rs\npub trait Summarizable { fn summary(\u0026amp;self) -\u0026gt; String; } pub struct NewsArticle { pub headline: String, pub location: String, pub author: String, pub content: String, } impl Summarizable for NewsArticle { fn summary(\u0026amp;self) -\u0026gt; String { format!(\u0026#34;{}, by {} ({})\u0026#34;, self.headline, self.author, self.location) } } pub struct Tweet { pub username: String, pub content: String, pub reply: bool, pub retweet: bool, } impl Summarizable for Tweet { fn summary(\u0026amp;self) -\u0026gt; String { format!(\u0026#34;{}: {}\u0026#34;, self.username, self.content) } } pub fn notify\u0026lt;T: Summarizable\u0026gt; (item: T) { println!(\u0026#34;Breaking news! {}\u0026#34;, item.summary());} main.rs\nmod lib; use lib::Summarizable; use lib::Tweet; fn main() { let tweet = Tweet { username: String::from(\u0026#34;horse_ebooks\u0026#34;), content: String::from(\u0026#34;of course, as you probably already know, people\u0026#34;), reply: false, retweet: false, }; println!(\u0026#34;1 new tweet: {}\u0026#34;, tweet.summary()); }      lifetime ...  use std::fmt::Display; fn main() { // dangligh reference 1  { let r; { let x = 5; r = \u0026amp;x; } // println!(\u0026#34;r: {}\u0026#34;, r); // borrow later used here  } // lifetime 1  let str1 = String::from(\u0026#34;abcd\u0026#34;); let str2 = \u0026#34;xyz\u0026#34;; let rst1 = str_long(str1.as_str(), str2); println!(\u0026#34;the longest string is {}\u0026#34;, rst1); // lifetime 2  let str1 = String::from(\u0026#34;long string is long\u0026#34;); { let str2 = String::from(\u0026#34;xyz\u0026#34;); let rst1 = str_long(str1.as_str(), str2.as_str()); println!(\u0026#34;the longest string is {}\u0026#34;, rst1); } // lifetime 3  let str1 = String::from(\u0026#34;long string is long\u0026#34;); let rst2; { let str2 = String::from(\u0026#34;xyz\u0026#34;); rst2 = str_long(str1.as_str(), str2.as_str()); } // println!(\u0026#34;the longest string is {}\u0026#34;, rst2);  // struct lifetime  let novel = String::from(\u0026#34;Call me Ishmael. Some years ago...\u0026#34;); let first_sentence = novel.split(\u0026#39;.\u0026#39;).next().expect(\u0026#34;Could not find a \u0026#39;.\u0026#39;\u0026#34;); let i = ImportantExcerpt { part: first_sentence, }; println!(\u0026#34;{:?}\u0026#34;, i); // static lifetime  let s: \u0026amp;\u0026#39;static str = \u0026#34;I have a static lifetime.\u0026#34;; let str1 = String::from(\u0026#34;abcd\u0026#34;); let str2 = String::from(\u0026#34;efg\u0026#34;); // long with announce  let rst3 = long_with_an_annc(str1.as_str(), str2.as_str(), \u0026#34;what?\u0026#34;); println!(\u0026#34;{}\u0026#34;, rst3); } // lifetime ëª…ì‹œ fn str_long\u0026lt;\u0026#39;a\u0026gt;(x: \u0026amp;\u0026#39;a str, y: \u0026amp;\u0026#39;a str) -\u0026gt; \u0026amp;\u0026#39;a str { if x.len() \u0026gt; y.len() { x } else { y } } // lifetime ëª…ì‹œ 1 fn str_lon1\u0026lt;\u0026#39;a\u0026gt;(x: \u0026amp;\u0026#39;a str, y: \u0026amp;str) -\u0026gt; \u0026amp;\u0026#39;a str { x } // // lifetime ëª…ì‹œ 2 // error // fn str_lon2\u0026lt;\u0026#39;a\u0026gt;(x: \u0026amp;str, y: \u0026amp;str) -\u0026gt; \u0026amp;\u0026#39;a str { // let result = String::from(\u0026#34;long string\u0026#34;); // result.as_str() // }  #[derive(Debug)] struct ImportantExcerpt\u0026lt;\u0026#39;a\u0026gt; { part: \u0026amp;\u0026#39;a str, } // fn first_word\u0026lt;\u0026#39;a\u0026gt;(s: \u0026amp;\u0026#39;a str) -\u0026gt; \u0026amp;\u0026#39;a str {...} fn first_word(s: \u0026amp;str) -\u0026gt; \u0026amp;str { let bytes = s.as_bytes(); for (i, \u0026amp;item) in bytes.iter().enumerate() { if item == b\u0026#39; \u0026#39; { return \u0026amp;s[0..i]; } } \u0026amp;s[..] } fn long_with_an_annc\u0026lt;\u0026#39;a, T\u0026gt;(x: \u0026amp;\u0026#39;a str, y: \u0026amp;\u0026#39;a str, ann: T) -\u0026gt; \u0026amp;\u0026#39;a str where T: Display, { println!(\u0026#34;Announcement! {}\u0026#34;, ann); if x.len() \u0026gt; y.len() { x } else { y } }     Testing #   test ...  src/lib.rs\n#[derive(Debug)] pub struct Rectangle { length: u32, width: u32, } impl Rectangle { pub fn can_hold(\u0026amp;self, other: \u0026amp;Rectangle) -\u0026gt; bool { self.length \u0026gt; other.length \u0026amp;\u0026amp; self.width \u0026gt; other.width } } pub fn add_two(a: i32) -\u0026gt; i32 { a + 2 } pub fn greeting(name: \u0026amp;str) -\u0026gt; String { format!(\u0026#34;Hello {}!\u0026#34;, name) } pub fn just_greeting(name: \u0026amp;str) -\u0026gt; String { format!(\u0026#34;Hello!\u0026#34;) } pub struct Guess { value: u32, } impl Guess { pub fn new(value: u32) -\u0026gt; Guess { if value \u0026lt; 1 || value \u0026gt; 100 { panic!(\u0026#34;Guess value must be between 1 and 100, got {}.\u0026#34;, value); } Guess { value } } pub fn new_new(value: u32) -\u0026gt; Guess { if value \u0026lt; 1 { panic!(\u0026#34;Guess value must be between 1 and 100, got {}.\u0026#34;, value); } Guess { value } } pub fn new_new_new(value: u32) -\u0026gt; Guess { if value \u0026lt; 1 { panic!(\u0026#34;Value \u0026lt; 1, got {}.\u0026#34;, value); } else if value \u0026gt; 100 { panic!(\u0026#34;Value \u0026gt; 100, got {}.\u0026#34;, value); } Guess { value } } } fn prints_and_returns_10(a: i32) -\u0026gt; i32 { println!(\u0026#34;I got the value {}\u0026#34;, a); 10 } pub fn add_two_num(a: i32) -\u0026gt; i32 { internal_adder(a, 2) } fn internal_adder(a: i32, b: i32) -\u0026gt; i32 { a + b } #[cfg(test)] mod tests { #[test] fn it_works_1() { assert_eq!(2 + 2, 4); } #[test] fn it_works_2() { assert_eq!(3 + 3, 6); } #[test] fn it_works_3() { assert_eq!(3 + 3, 4); } #[test] fn it_works_4() { panic!(\u0026#34;Make this fail\u0026#34;); } use super::*; #[test] fn larger_can_hold_smaller() { let larger = Rectangle { length: 8, width: 7, }; let smaller = Rectangle { length: 5, width: 1, }; assert!(larger.can_hold(\u0026amp;smaller)); } #[test] fn smaller_cannot_hold_larger() { let larger = Rectangle { length: 8, width: 7, }; let smaller = Rectangle { length: 5, width: 1, }; assert!(!smaller.can_hold(\u0026amp;larger)); } #[test] fn it_adds_two() { assert_eq!(4, add_two(2)); } #[test] fn it_adds_two_ne() { assert_ne!(5, add_two(2)); } #[test] fn greeting_contains_name() { let result = greeting(\u0026#34;Carol\u0026#34;); assert!(result.contains(\u0026#34;Carol\u0026#34;)); } #[test] fn greeting_without_name() { let result = just_greeting(\u0026#34;Carol\u0026#34;); assert!( result.contains(\u0026#34;Carol\u0026#34;), \u0026#34;Greeting did not contain name, value was `{}`\u0026#34;, result ); } #[test] #[should_panic] fn greater_than_100() { Guess::new(200); } #[test] #[should_panic] fn greater_than_100_2() { Guess::new_new(200); } #[test] #[should_panic(expected = \u0026#34;Value \u0026gt; 100\u0026#34;)] fn greater_than_100_3() { Guess::new_new_new(200); } #[test] #[should_panic(expected = \u0026#34;Value \u0026lt; 1\u0026#34;)] fn greater_than_100_4() { Guess::new_new_new(200); } #[test] #[should_panic(expected = \u0026#34;Value\u0026#34;)] fn greater_than_100_5() { Guess::new_new_new(200); } // cargo test -- --help  // cargo test -- --test-threads=1  // cargo test -- --nocapture  // cargo test this_test_will_pass  // cargo test this_test_will_pass -- --nocapture  // cargo test this_test_will  #[test] fn this_test_will_pass() { let value = prints_and_returns_10(4); assert_eq!(10, value); } #[test] fn this_test_will_fail() { let value = prints_and_returns_10(8); assert_eq!(5, value); } // cargo test this_test_will -- --ignored  #[test] #[ignore] fn this_test_will_expensive_test() { let value = prints_and_returns_10(8); assert_eq!(5, value); } // unit test  use super::*; #[test] fn internal() { assert_eq!(4, internal_adder(2, 2)); } } tests\\intg_test.rs\nextern crate rust_test; mod common; // cargo test : integrated test // cargo test --test intg_test #[test] fn it_adds_twx() { common::setup(); assert_eq!(4, rust_test::add_two_num(2)); } test\\common\\mod.rs\npub fn setup() { println!(\u0026#34;TEST WOW\u0026#34;) }     Command Line Program #   grep ...  src/lib.rs\nuse std::env; use std::error::Error; use std::fs::File; use std::io::prelude::*; pub struct Config { pub pgnm: String, pub quer: String, pub file: String, pub case: bool, } impl Config { pub fn new(args: \u0026amp;[String]) -\u0026gt; Result\u0026lt;Config, \u0026amp;\u0026#39;static str\u0026gt; { if args.len() \u0026lt; 3 { panic!(\u0026#34;not enough arguments\u0026#34;); } let pgnm = args[0].clone(); let quer = args[1].clone(); let file = args[2].clone(); let case = env::var(\u0026#34;CASE\u0026#34;).is_err(); Ok(Config { pgnm, quer, file, case, }) } } pub fn run(conf: Config) -\u0026gt; Result\u0026lt;(), Box\u0026lt;Error\u0026gt;\u0026gt; { let mut f = File::open(conf.file)?; let mut cont = String::new(); f.read_to_string(\u0026amp;mut cont)?; let rslt = if conf.case { search(\u0026amp;conf.quer, \u0026amp;cont) } else { search_case_insensitive(\u0026amp;conf.quer, \u0026amp;cont) }; //println!(\u0026#34;With text:\\n {}\u0026#34;, cont); \tfor line in rslt { println!(\u0026#34;{}\u0026#34;, line) } Ok(()) } #[cfg(test)] mod test { use super::*; #[test] fn one_result() { let quer = \u0026#34;duct\u0026#34;; let cont = \u0026#34;\\ Rust:alloc safe, fast, productive. Pick three.\u0026#34;; assert_eq!(vec![\u0026#34;safe, fast, productive.\u0026#34;], search(quer, cont)); } #[test] fn case_insensitive() { let quer = \u0026#34;rUsT\u0026#34;; let cont = \u0026#34;\\ Rust: safe, fast, productive. Pick three. Trust me.\u0026#34;; assert_eq!( vec![\u0026#34;Rust:\u0026#34;, \u0026#34;Trust me.\u0026#34;], search_case_insensitive(quer, cont) ); } } pub fn search\u0026lt;\u0026#39;a\u0026gt;(quer: \u0026amp;str, cont: \u0026amp;\u0026#39;a str) -\u0026gt; Vec\u0026lt;\u0026amp;\u0026#39;a str\u0026gt; { let mut rslt = Vec::new(); for line in cont.lines() { if line.contains(quer) { rslt.push(line); } } rslt } pub fn search_case_insensitive\u0026lt;\u0026#39;a\u0026gt;(quer: \u0026amp;str, cont: \u0026amp;\u0026#39;a str) -\u0026gt; Vec\u0026lt;\u0026amp;\u0026#39;a str\u0026gt; { let quer = quer.to_lowercase(); let mut rslt = Vec::new(); for line in cont.lines() { if line.to_lowercase().contains(\u0026amp;quer) { rslt.push(line); } } rslt } src/main.rs\nextern crate rust_grep; use rust_grep::Config; use std::env; use std::process; fn main() { let args: Vec\u0026lt;String\u0026gt; = env::args().collect(); //let conf = parse_config(\u0026amp;args);  let conf = Config::new(\u0026amp;args).unwrap_or_else(|err| { println!(\u0026#34;Problem parsing arguments: {}\u0026#34;, err); process::exit(1); }); println!(\u0026#34;Program Name : {}\u0026#34;, conf.pgnm); println!(\u0026#34;Searching for : {}\u0026#34;, conf.quer); println!(\u0026#34;In file : {}\u0026#34;, conf.file); // let mut f = File::open(conf.file).expect(\u0026#34;file not found\u0026#34;);  // let mut cont = String::new();  // f.read_to_string(\u0026amp;mut cont)  // .expect(\u0026#34;Something went wrong reading the file\u0026#34;);  // println!(\u0026#34;With text:\\n {}\u0026#34;, cont);  if let Err(e) = rust_grep::run(conf) { eprintln!(\u0026#34;Application Error : {}\u0026#34;, e); process::exit(1); } } // fn parse_config(args: \u0026amp;[String]) -\u0026gt; Config { // let pgnm = args[0].clone(); // let quer = args[1].clone(); // let file = args[2].clone();  // Config { pgnm, quer, file } // }      ë‹¤ìŒê¸€  "});index.add({'id':19,'href':'/docs/documents/backend/sql/sql01/','title':"SQL Basic",'section':"SQL",'content':"SQL BASIC ê¸°ë³¸ êµìœ¡ ìë£Œ #   SQLì€ Sturctured Query Langugageì˜ ì•½ìë¡œ ë°ì´í„°ë² ì´ìŠ¤ì˜ ë°ì´í„°ë¥¼ ê´€ë¦¬í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ì–¸ì–´. ìë£Œì˜ ê²€ìƒ‰ê³¼ ê´€ë¦¬, ìŠ¤í‚¤ë§ˆì˜ ìƒì„±ê³¼ ìˆ˜ì •, ê°ì²´ì˜ ì ‘ê·¼ ì¡°ì • ë° ê´€ë¦¬ì˜ ê¸°ëŠ¥ ì œê³µ. ANSI SQL ê¸°ë³¸ ë¬¸ë²•ì€ ì›ë˜ ì–´ë–¤ ë°ì´í„°ë² ì´ìŠ¤ì™€ë„ í˜¸í™˜ì´ ê°€ëŠ¥í•´ì•¼ í•˜ë‚˜ í†µìƒ Vendorë“¤ì´ í‘œì¤€ì„ ì˜ì§€í‚¤ì§€ì•ŠëŠ” í¸ì´ë¼ ê¸°ë³¸ì ì¸ ìˆ˜ì¤€ì—ì„œë§Œ í˜¸í™˜ì´ ê°€ëŠ¥. NoSQL ê³„ì—´ì˜ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŒ. (ë‹¤ìŒì€ ì˜¤ë¼í´ ê¸°ì¤€ì„)\nLive SQL #   Oracle ê°€ì… Oracle Live SQL ì ‘ì† \u0026ldquo;Start Coding Now\u0026rdquo; Live SQL ì‹¤í–‰  Hello World! #  select \u0026#39;Hello World!\u0026#39; from dual; DDL / DML / DCL #  ì¡°íšŒë¥¼ ìœ„í•œ ì˜ˆì œ í…Œì´ë¸”ì´ ì‘ì„±ì„ ìœ„í•´ DDL, DML, DCL ê°œë… í•„ìš”.\n ddl (data definition language) ...  ë°ì´í„° ì •ì˜ ì–¸ì–´ë¡œ ë°ì´í„°ë² ì´ìŠ¤ì— ì˜¤ë¸Œì íŠ¸ë¥¼ ìƒì„±,ì‚­ì œ,ë³€ê²½ ë° ì œê±°í•˜ëŠ” ëª…ë ¹ë¬¸\ncreate : ë°ì´í„°ë² ì´ìŠ¤ì— ì˜¤ë¸Œì íŠ¸ë¥¼ ìƒì„± drop : ë°ì´í„°ë² ì´ìŠ¤ì— ì˜¤ë¸Œì íŠ¸ë¥¼ ì‚­ì œ alter : ë°ì´í„°ë² ì´ìŠ¤ì— ì˜¤ë¸Œì íŠ¸ë¥¼ ë³€ê²½ truncate : í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ì‚­ì œ\n    dml (data manipulation languge) ...  ë°ë””í„° ì¡°ì‘ ì–¸ì–´ë¡œ ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ì§ˆì˜,ìƒì„±,ìˆ˜ì • ë° ì‚­ì œí•˜ëŠ” ì§ˆì˜ë¬¸\nselect : í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ê²€ìƒ‰ insert : í…Œì´ë¸”ì— ë°ì´í„°ë¥¼ ìƒì„± update : í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ë³€ê²½ delete : ë°ì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ì‚­ì œ\n    dcl (data control language) ...  ë°ì´í„° ì œì–´ ì–¸ì–´ë¡œ ë°ì´í„°ë² ì´ìŠ¤ì˜ ê¶Œí•œì„ ê´€ë¦¬í•˜ëŠ”ë° ì‚¬ìš©í•˜ëŠ” ëª…ë ¹ë¬¸\ngrant : ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©ìì—ê²Œ íŠ¹ì • ê¶Œí•œì„ ë¶€ì—¬ revoke : ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©ìì—ê²Œ íŠ¹ì • ê¶Œí•œì„ ì œê±°\n   Table Creation #   table creation ...  create table my_table_1 ( column_1 varchar2(10), column_2 number, column_3 date ) ; insert into my_table_1 (column_1,column_2,column_3) values (\u0026#39;í™ê¸¸ë™\u0026#39;,23,sysdate); insert into my_table_1 (column_1,column_2,column_3) values (\u0026#39;ê¹€ì² ìˆ˜\u0026#39;,19,sysdate); insert into my_table_1 (column_1,column_2,column_3) values (\u0026#39;ì´ì˜í¬\u0026#39;,32,sysdate); insert into my_table_1 (column_1,column_2,column_3) values (\u0026#39;ë°•ë¯¼í˜¸\u0026#39;,22,sysdate); insert into my_table_1 (column_1,column_2,column_3) values (\u0026#39;ê¹€ì¢…ì² \u0026#39;,45,sysdate); insert into my_table_1 (column_1,column_2,column_3) values (\u0026#39;ê¹€ì² ìˆ˜\u0026#39;,36,sysdate); insert into my_table_1 (column_1,column_2,column_3) values (\u0026#39;ì‹ ì˜ì‹ \u0026#39;,32,sysdate); insert into my_table_1 (column_1,column_2,column_3) values (\u0026#39;ìµœì² í˜¸\u0026#39;,11,sysdate);     ë°ì´í„° ìƒì„± ê²°ê³¼ë¥¼ ì¡°íšŒ\nquery select * from my_table_1 ; result    COLUMN_1 COLUMN_2 COLUMN_3     í™ê¸¸ë™ 23 10-OCT-20   ê¹€ì² ìˆ˜ 19 10-OCT-20   ì´ì˜í¬ 32 10-OCT-20   ë°•ë¯¼í˜¸ 22 10-OCT-20   ê¹€ì¢…ì²  45 10-OCT-20   ê¹€ì² ìˆ˜ 36 10-OCT-20   ì‹ ì˜ì‹  32 10-OCT-20   ìµœì² ë¯¼ 11 10-OCT-20    Download CSV 8 rows selected. Select / From #   select / from ...  -- ì£¼ì„ì…ë‹ˆë‹¤. select * from my_table_1; -- ì»¬ëŸ¼ëª…ì„ ì§€ì •í•©ë‹ˆë‹¤. select column_1,column_3 from my_table_1;     Where / And #   where / and ...  -- where select * from my_table_1 where column_2 = 32; -- where / ë¶€ë“±í˜¸ 1 select * from my_table_1 where column_2 \u0026lt;= 30; -- where / ë¶€ë“±í˜¸ 2 select * from my_table_1 where column_2 \u0026gt;= 30; -- where / ë¶€ë“±í˜¸ 3 select * from my_table_1 where column_2 \u0026gt; 30; -- where / ë¶€ë“±í˜¸ 4 select * from my_table_1 where column_2 \u0026lt; 30; -- where / and select * from my_table_1 where column_2 \u0026gt; 10 and column_2 \u0026lt; 40; -- where / and / ë¬¸ìì—´ 1 select * from my_table_1 where column_1 = \u0026#39;í™ê¸¸ë™\u0026#39; and column 2 = 23; -- where / and / ë¬¸ìì—´ 2 select * from my_table_1 where column_1 != \u0026#39;í™ê¸¸ë™\u0026#39;; -- where / and / ë¬¸ìì—´ 3 select * from my_table_1 where column_1 \u0026lt;\u0026gt; \u0026#39;í™ê¸¸ë™\u0026#39;; -- where / and / ë¬¸ìì—´ / like 1 select * from my_table_1 where column_1 like \u0026#39;ê¹€%\u0026#39;; -- where / and / ë¬¸ìì—´ / like 2 select * from my_table_1 where column_1 like \u0026#39;%ì² %\u0026#39;; -- where / and / ë¬¸ìì—´ / like 3 select * from my_table_1 where column_1 like \u0026#39;%ì² \u0026#39;; -- where / and / ë¬¸ìì—´ / like 4 select * from my_table_1 where column_1 like \u0026#39;ê¹€%ì² \u0026#39;; -- where / and / ë¬¸ìì—´ / like 5 select * from my_table_1 where column_1 not like \u0026#39;%ì² \u0026#39;; -- where / and / between select * from my_table_1 where column_2 between 22 and 32; -- where / and / in select * from my_table_1 where column_1 in (\u0026#39;ì´ì˜í¬\u0026#39;,\u0026#39;ê¹€ì² ìˆ˜\u0026#39;); -- where / and / not in select * from my_table_1 where column_1 not in (\u0026#39;ì´ì˜í¬\u0026#39;,\u0026#39;ê¹€ì² ìˆ˜\u0026#39;); -- where / and / in / or select * from my_table_1 where column_1 in (\u0026#39;ì´ì˜í¬\u0026#39;,\u0026#39;ê¹€ì² ìˆ˜\u0026#39;) or column_2 = 22; -- where / and / or / and 1 select * from my_table_1 where column_1 in (\u0026#39;ì´ì˜í¬\u0026#39;,\u0026#39;ê¹€ì² ìˆ˜\u0026#39;) or column_2 = 22 and column_1 like \u0026#39;%ìˆ˜\u0026#39;; -- where / and / or / and 2 select * from my_table_1 where (column_1 in (\u0026#39;ì´ì˜í¬\u0026#39;,\u0026#39;ê¹€ì² ìˆ˜\u0026#39;) or column_2 = 22) and column_1 like \u0026#39;%ìˆ˜\u0026#39;;     dual / + - * / #   dual / operatios ...  -- dual / + - * / select 2+3, 5-2, 2*3, 8/2 from dual; -- mod, power, sign, cos, tan select mod(7,3),power(2,3),sign(1),cos(1),tan(1) from dual; -- number, text, null, number+null, date select 1, \u0026#39;TEXT\u0026#39;, null, 10+2+null, sysdate from dual; -- number+null = null select 10+2+null from dual; -- number+text(number) = number select 10+2+\u0026#39;3\u0026#39; from dual; -- number+text : error select 10+2+\u0026#39;A\u0026#39; from dual; -- lower/upper select \u0026#39;Case\u0026#39;,lower(\u0026#39;Case\u0026#39;),upper(\u0026#39;Case\u0026#39;) from dual;     count / ditinct / alias / lower / upper #   count / ditinct / alias / lower / upper ...  -- count select count(*) from my_table_1; -- distinct select distinct column_1 from my_table_1; -- distinct / count select count(distinct column_1),count(column_1) from my_table_1; -- distinct / count / alias select count(distinct column_1) col1,count(column_1) col2,count(tab.column_1) col3 from my_table_1 tab;     from / nested from / with as #   from / nested from / with as ...  -- from 1 select column_1,column_2,column_2+10 column_3 from my_table_1 where column_1 in (\u0026#39;í™ê¸¸ë™\u0026#39;,\u0026#39;ê¹€ì² ìˆ˜\u0026#39;,\u0026#39;ì´ì˜í¬\u0026#39;) ; -- nested from select * from ( select column_1,column_2,column_2+10 column_3 from my_table_1 where column_1 in (\u0026#39;í™ê¸¸ë™\u0026#39;,\u0026#39;ê¹€ì² ìˆ˜\u0026#39;,\u0026#39;ì´ì˜í¬\u0026#39;) ) ; -- with 1 with tab_1 as ( select column_1,column_2,column_2+10 column_3 from my_table_1 where column_1 in (\u0026#39;í™ê¸¸ë™\u0026#39;,\u0026#39;ê¹€ì² ìˆ˜\u0026#39;,\u0026#39;ì´ì˜í¬\u0026#39;) ) select * from tab_1 ; -- with 2 with tab_1 as ( select column_1,column_2,column_2+10 column_3 from my_table_1 where column_1 in (\u0026#39;í™ê¸¸ë™\u0026#39;,\u0026#39;ê¹€ì² ìˆ˜\u0026#39;,\u0026#39;ì´ì˜í¬\u0026#39;) ) , tab_2 as ( select column_1,column_2,column_2+10 column_3 from my_table_1 where column_2 \u0026lt;= 30 ) select * from tab_2 ; -- with 3 with tab_1 as ( select column_1,column_2,column_2+10 column_3 from my_table_1 where column_1 in (\u0026#39;í™ê¸¸ë™\u0026#39;,\u0026#39;ê¹€ì² ìˆ˜\u0026#39;,\u0026#39;ì´ì˜í¬\u0026#39;) ) , tab_2 as ( select column_1,column_2,column_2+10 column_3 from my_table_1 where column_2 \u0026lt;= 30 ) select * from ( select * from tab_2 where column_1 like \u0026#39;%ì² %\u0026#39; ) ;     union all / union / intersect / minus #   union all / union / intersect / minus ...  -- union all with tab_1 as (select column_1,column_2,column_2+10 column_3 from my_table_1 where column_1 in (\u0026#39;í™ê¸¸ë™\u0026#39;,\u0026#39;ê¹€ì² ìˆ˜\u0026#39;,\u0026#39;ì´ì˜í¬\u0026#39;)) , tab_2 as (select column_1,column_2,column_2+10 column_3 from my_table_1 where column_2 \u0026lt;= 30) select * from tab_1 union all select * from tab_2 ; -- union with tab_1 as (select column_1,column_2,column_2+10 column_3 from my_table_1 where column_1 in (\u0026#39;í™ê¸¸ë™\u0026#39;,\u0026#39;ê¹€ì² ìˆ˜\u0026#39;,\u0026#39;ì´ì˜í¬\u0026#39;)) , tab_2 as (select column_1,column_2,column_2+10 column_3 from my_table_1 where column_2 \u0026lt;= 30) select * from tab_1 union all select * from tab_2 ; -- intersect with tab_1 as (select column_1,column_2,column_2+10 column_3 from my_table_1 where column_1 in (\u0026#39;í™ê¸¸ë™\u0026#39;,\u0026#39;ê¹€ì² ìˆ˜\u0026#39;,\u0026#39;ì´ì˜í¬\u0026#39;)) , tab_2 as (select column_1,column_2,column_2+10 column_3 from my_table_1 where column_2 \u0026lt;= 30) select * from tab_1 intersect select * from tab_2 ; -- minus with tab_1 as (select column_1,column_2,column_2+10 column_3 from my_table_1 where column_1 in (\u0026#39;í™ê¸¸ë™\u0026#39;,\u0026#39;ê¹€ì² ìˆ˜\u0026#39;,\u0026#39;ì´ì˜í¬\u0026#39;)) , tab_2 as (select column_1,column_2,column_2+10 column_3 from my_table_1 where column_2 \u0026lt;= 30) select * from tab_1 intersect select * from tab_2 ;     join 1 / join 2 / join 3 / join 4 / join 5 #   join ...  -- join 1 with tab_1 as (select column_1,column_2,column_2+10 column_3 from my_table_1 where column_1 in (\u0026#39;í™ê¸¸ë™\u0026#39;,\u0026#39;ê¹€ì² ìˆ˜\u0026#39;,\u0026#39;ì´ì˜í¬\u0026#39;)) , tab_2 as (select column_1,column_2,column_2+10 column_3 from my_table_1 where column_2 \u0026lt;= 30) select * from tab_1 a, tab_2 b where a.column_1 = b.column_1 ; -- join 2 with tab_1 as (select column_1,column_2,column_2+10 column_3 from my_table_1 where column_1 in (\u0026#39;í™ê¸¸ë™\u0026#39;,\u0026#39;ê¹€ì² ìˆ˜\u0026#39;,\u0026#39;ì´ì˜í¬\u0026#39;)) , tab_2 as (select column_1,column_2,column_2+10 column_3 from my_table_1 where column_2 \u0026lt;= 30) select * from tab_1 a, tab_2 b where a.column_1 = b.column_1 and a.column_2 = b.column_2 ; -- join 3 with tab_1 as (select column_1,column_2,column_2+10 column_3 from my_table_1 where column_1 in (\u0026#39;í™ê¸¸ë™\u0026#39;,\u0026#39;ê¹€ì² ìˆ˜\u0026#39;,\u0026#39;ì´ì˜í¬\u0026#39;)) , tab_2 as (select column_1,column_2,column_2+10 column_3 from my_table_1 where column_2 \u0026lt;= 30) select * from tab_1 a, tab_2 b where a.column_1 = b.column_1 (+) and a.column_2 = b.column_2 (+) ; -- join 4 with tab_1 as (select column_1,column_2,column_2+10 column_3 from my_table_1 where column_1 in (\u0026#39;í™ê¸¸ë™\u0026#39;,\u0026#39;ê¹€ì² ìˆ˜\u0026#39;,\u0026#39;ì´ì˜í¬\u0026#39;)) , tab_2 as (select column_1,column_2,column_2+10 column_3 from my_table_1 where column_2 \u0026lt;= 30) select * from tab_1 a, tab_2 b where a.column_1 (+) = b.column_1 and a.column_2 (+) = b.column_2 ; -- join 5 with tab_1 as (select column_1,column_2,column_2+10 column_3 from my_table_1 where column_1 in (\u0026#39;í™ê¸¸ë™\u0026#39;,\u0026#39;ê¹€ì² ìˆ˜\u0026#39;,\u0026#39;ì´ì˜í¬\u0026#39;)) , tab_2 as (select column_1,column_2,column_2+10 column_3 from my_table_1 where column_2 \u0026lt;= 30) select * from tab_1 a full outer join tab_2 b on ( a.column_1 = b.column_1 and a.column_2 = b.column_2 ) ;     substr / length / replace / decode / instr / lpad / rapd / reg_exp #   substr / length / replace / decode / instr / lpad / rapd / reg_exp  ...  select column_1, substr(column_1,1,1) \u0026#34;ì„±2\u0026#34;, substr(column_1,2) \u0026#34;ì´ë¦„1\u0026#34;, substr(column_1,2,2) \u0026#34;ì´ë¦„2\u0026#34;, substr(column_1,-2) \u0026#34;ì´ë¦„3\u0026#34;, length(column_1) \u0026#34;ê¸¸ì´\u0026#34;, replace(column_1,\u0026#39;ê¸¸ë™\u0026#39;,\u0026#39;ê¸¸ìˆ˜\u0026#39;) \u0026#34;êµì²´\u0026#34;, decode(column_1,\u0026#39;ì´ì˜í¬\u0026#39;,\u0026#39;ìš”ì£¼ì˜\u0026#39;,\u0026#39;ê¹€ì² ìˆ˜\u0026#39;,\u0026#39;ì•…í”ŒëŸ¬\u0026#39;,\u0026#39;ê¹€ì¢…ì² \u0026#39;,\u0026#39;ìœ ë¶€ë‚¨\u0026#39;,\u0026#39;-\u0026#39;) \u0026#34;ë””ì½”ë“œ\u0026#34;, instr(column_1,\u0026#39;ì² \u0026#39;) \u0026#34;ì°¾ê¸°1\u0026#34;, instr(column_1,\u0026#39;ì‹ \u0026#39;,1) \u0026#34;ì°¾ê¸°2\u0026#34;, instr(column_1,\u0026#39;ì‹ \u0026#39;,2) \u0026#34;ì°¾ê¸°3\u0026#34;, column_1, lpad(column_2,3,\u0026#39;0\u0026#39;) \u0026#34;ì±„ìš°ê¸°1\u0026#34;, rpad(column_2,3,\u0026#39;0\u0026#39;) \u0026#34;ì±„ìš°ê¸°2\u0026#34;, substr(column_1,1,1)||\u0026#39;ì”¨_\u0026#39;||column_2||\u0026#39;ì„¸\u0026#39; \u0026#34;ì‡ê¸°\u0026#34;, regexp_substr(column_1,\u0026#39;[^ì² ]+\u0026#39;,1,1) \u0026#34;ì •ê·œì‹ì°¾ê¸°1\u0026#34; from my_table_1 ;     nvl / trim #   nvl / trim ...  with data_list as ( select \u0026#39; data1 \u0026#39; col1 from dual union all select \u0026#39;data2 \u0026#39; col1 from dual union all select \u0026#39; data3\u0026#39; col1 from dual union all select \u0026#39; \u0026#39; col1 from dual union all select null col1 from dual ) select col1, length(col1) col1_len, trim(col1) col2, length(trim(col1)) col2_len, nvl(trim(col1),\u0026#39;-\u0026#39;) col3, length(nvl(trim(col1),\u0026#39;-\u0026#39;)) col3_len from data_list ;     order by #   order by asc/desc ...  select * from my_table_1 order by column_2 asc,column_1 desc,column_3     count / sum / avg / min / max / median /stddev / round / trunc / ceil / floor #   count / sum / avg / min / max / median /stddev / round / trunc / ceil / floor  ...  select count(column_1), sum(column_2), avg(column_2), min(column_2), max(column_2), median(column_2), stddev(column_2), round(avg(column_2)), round(stddev(column_2),2), trunc(avg(column_2)), ceil(avg(column_2)), floor(avg(column_2)) from my_table_1 ;     count / sum / avg / min / max / median /stddev / round / trunc / ceil / floor #   count / sum / avg / min / max / median /stddev / round / trunc / ceil / floor  ...  select substr(column_1,1,1),sum(column_2) from my_table_1 group by substr(column_1,1,1) ; select substr(column_1,1,1),sum(column_2) from my_table_1 group by substr(column_1,1,1) having sum(column_2) \u0026gt;= 30 ; select * from my_table_1 where substr(column_1,1,1) = \u0026#39;ê¹€\u0026#39; ; select substr(column_1,1,1),sum(column_2),avg(column_2) from my_table_1 where substr(column_1,1,1) = \u0026#39;ê¹€\u0026#39; group by substr(column_1,1,1) ; select substr(column_1,1,1),sum(column_2),avg(column_2) from my_table_1 where substr(column_1,1,1) = \u0026#39;ê¹€\u0026#39; and column_2 \u0026gt;= 20 group by substr(column_1,1,1) ;     case when / to_char / to_date / add_months / last_day #   case when / to_char / to_date / add_months  ...  select column_1,column_2, case when column_1 = \u0026#39;í™ê¸¸ë™\u0026#39; then \u0026#39;ìš©ì˜ì\u0026#39; when column_1 = \u0026#39;ë°•ë¯¼í˜¸\u0026#39; and column_2 = 22 then \u0026#39;ìš©ì˜ì\u0026#39; when column_1 = \u0026#39;ì‹ ì˜ì‹ \u0026#39; then \u0026#39;ìš©ì˜ì\u0026#39; else \u0026#39;ì¼ë°˜ì‹œë¯¼\u0026#39; end column_x, case when column_1 = \u0026#39;í™ê¸¸ë™\u0026#39; then column_2 when column_1 = \u0026#39;ë°•ë¯¼í˜¸\u0026#39; and column_2 = 22 then column_2 when column_1 = \u0026#39;ì‹ ì˜ì‹ \u0026#39; then column_2 else 0 end column_y, to_char(column_3,\u0026#39;yyyymmdd\u0026#39;) date_to_char_01, to_char(column_3,\u0026#39;yy/mm/dd\u0026#39;) date_to_char_02, to_char(column_3,\u0026#39;mm.dd.yy\u0026#39;) date_to_char_03, to_char(column_3,\u0026#39;hh24:mi:ss\u0026#39;) date_to_char_04, to_char(column_3,\u0026#39;D\u0026#39;) date_to_char_05, to_date(\u0026#39;20200315\u0026#39;,\u0026#39;yyyymmdd\u0026#39;)-to_date(\u0026#39;20200215\u0026#39;,\u0026#39;yyyymmdd\u0026#39;) date_calc_01, to_date(\u0026#39;20200315\u0026#39;,\u0026#39;yyyymmdd\u0026#39;)+17 date_calc_02, trunc(to_date(\u0026#39;20200315\u0026#39;,\u0026#39;yyyymmdd\u0026#39;),\u0026#39;IW\u0026#39;) date_calc_03, add_months(to_date(\u0026#39;20200331\u0026#39;,\u0026#39;yyyymmdd\u0026#39;),-1) date_calc_04, add_months(to_date(\u0026#39;20200331\u0026#39;,\u0026#39;yyyymmdd\u0026#39;),+1) date_calc_05, last_day(to_date(\u0026#39;20200205\u0026#39;,\u0026#39;yyyymmdd\u0026#39;)) date_calc_06 from my_table_1 ;     window function #   window function ...  select column_1,column_2, sum(column_2) over (partition by substr(column_1,1,1)) family_sum, sum(column_2) over (partition by substr(column_1,1,1) order by column_2) family_cum, sum(column_2) over (partition by substr(column_1,1,1) order by column_2 desc) family_dum from my_table_1 ;      ë‹¤ìŒê¸€  "});index.add({'id':20,'href':'/docs/documents/backend/go/go02/','title':"GO Advanced",'section':"Go",'content':"Go Advanced #   Go í™œìš© ì •ë¦¬. í•˜ê¸° ì‚¬ì´íŠ¸ ì°¸ì¡°.\në…¸ë§ˆë“œ ì½”ë” - ì‰½ê³  ë¹ ë¥¸ Go ì‹œì‘í•˜ê¸°\nAccount #   account ...  main.go\npackage main import ( \u0026#34;fmt\u0026#34; accounts \u0026#34;./accounts\u0026#34; ) func main() { // newAccount \tacct := accounts.NewAccount(\u0026#34;jhook\u0026#34;) fmt.Println(\u0026#34;01 : \u0026#34;, acct) // Deposit \tacct.Deposit(1000) fmt.Println(\u0026#34;02 : \u0026#34;, acct) // Withdraw O \terr := acct.Withdraw(300) fmt.Println(\u0026#34;03 : \u0026#34;, acct) if err != nil { fmt.Println(err) } // Withdraw X \terr = acct.Withdraw(1100) fmt.Println(\u0026#34;04 : \u0026#34;, acct) if err != nil { fmt.Println(err) } fmt.Println(\u0026#34;05 : \u0026#34;, acct) } accounts/account.go\npackage accounts import ( \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; ) type Account struct { owner string balance int } func NewAccount(owner string) *Account { account := Account{owner: owner, balance: 0} return \u0026amp;account } func (acct *Account) Deposit(amnt int) { acct.balance += amnt } func (acct *Account) Balance() int { return acct.balance } var errNoMoney = errors.New(\u0026#34;Can\u0026#39;t Withdraw\u0026#34;) func (acct *Account) Withdraw(amnt int) error { if acct.balance \u0026lt; amnt { return errNoMoney } acct.balance -= amnt return nil } func (acct *Account) ChangeOwner(nOwnr string) { acct.owner = nOwnr } func (acct *Account) Owner() string { return acct.owner } func (acct *Account) String() string { return fmt.Sprint(acct.Owner(), \u0026#34;\u0026#39;s account. Has:\u0026#34;, acct.Balance()) }     My Dict #   mydict ...  main.go\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;./mydict\u0026#34; ) func main() { dictionary := mydict.Dictionary{\u0026#34;first\u0026#34;: \u0026#34;First word\u0026#34;} definition1, err1 := dictionary.Search(\u0026#34;second\u0026#34;) if err1 != nil { fmt.Println(err1) } else { fmt.Println(definition1) } definition2, err2 := dictionary.Search(\u0026#34;first\u0026#34;) if err2 != nil { fmt.Println(err2) } else { fmt.Println(definition2) } dictionary.Add(\u0026#34;second\u0026#34;, \u0026#34;Second word\u0026#34;) definition1, err1 = dictionary.Search(\u0026#34;second\u0026#34;) if err1 != nil { fmt.Println(err1) } else { fmt.Println(definition1) } dictionary.Add(\u0026#34;third\u0026#34;, \u0026#34;Third word\u0026#34;) definition1, err1 = dictionary.Search(\u0026#34;third\u0026#34;) if err1 != nil { fmt.Println(err1) } else { fmt.Println(definition1) } } mydict/mydict.go\npackage mydict import \u0026#34;errors\u0026#34; type Dictionary map[string]string var errNotFound = errors.New(\u0026#34;Not Found\u0026#34;) var errWordExists = errors.New(\u0026#34;Word Exists\u0026#34;) func (d Dictionary) Search(word string) (string, error) { value, exists := d[word] if exists { return value, nil } return \u0026#34;\u0026#34;, errNotFound } func (d Dictionary) Add(word, def string) error { _, err := d.Search(word) switch err { case errNotFound: d[word] = def case nil: return errWordExists } return nil }     Scrapper #   scrapper ...  main.go\npackage main import ( \u0026#34;net/http\u0026#34; \u0026#34;./scrp\u0026#34; \u0026#34;github.com/labstack/echo\u0026#34; ) func handleHome(c echo.Context) error { return c.String(http.StatusOK, \u0026#34;Hello, World!\u0026#34;) } func main() { // scrp.Scrp \tscrp.Scrp(\u0026#34;scm\u0026#34;) } scrp/scrp.go\npackage scrp import ( \u0026#34;encoding/csv\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;github.com/PuerkitoBio/goquery\u0026#34; ) // extractedJob êµ¬ì¡°ì²´ ì •ì˜ type extractedJob struct { id string title string location string salary string summary string } // Scrp Function func Scrp(term string) { // base URL ë³€ìˆ˜ ì„ ì–¸ \tvar baseURL string = \u0026#34;https://kr.indeed.com/jobs?q=\u0026#34; + term + \u0026#34;\u0026amp;limit=50\u0026#34; // extractedJob êµ¬ì¡°ì²´ ë°°ì—´ ì„ ì–¸ \tvar jobs []extractedJob // extractedJOb ë°°ì—´í˜• ì±„ë„ ì„ ì–¸ \tc := make(chan []extractedJob) // 1) getPages Functionì„ í†µí•´ baseURLì„ ì „ë‹¬ \t// 2) getPages Functionì„ í†µí•´ totalPagesë¥¼ ë°›ìŒ \ttotalPages := getPages(baseURL) // totalpages ìˆ˜ë§Œí¼ go routine ìœ¼ë¡œ getPageë¥¼ ìˆ˜í–‰ \tfor i := 0; i \u0026lt; totalPages; i++ { // ê³ ë£¨í‹´ìœ¼ë¡œ getPageë¥¼ ìˆ˜í–‰ \t// í˜ì´ì§€ ë²ˆí˜¸, ê¸°ë³¸URL, ì±„ë„ \tgo getPage(i, baseURL, c) // ìµœì¢…ì ìœ¼ë¡œ extractedJob êµ¬ì¡°ì²´ ë°°ì—´í˜• c (mainC)ì— \t// ê²€ìƒ‰ëœ JobCardë“¤ì´ ë‹´ê¹€ \t} // ì±„ë„ë¡œ ì „ë‹¬ë°›ì€ pageë³„ extractedJobsë¥¼ \t// ë¡œì»¬ Jobsë¡œ ìµœì¢… ì·¨í•© \tfor i := 0; i \u0026lt; totalPages; i++ { extratedJobs := \u0026lt;-c jobs = append(jobs, extratedJobs...) } // pageë³„ë¡œ ì „ë‹¬ ë°›ì€ jobsë¥¼ writJobsë¥¼ í†µí•´ íŒŒì¼ë¡œ ê¸°ë¡ \twriteJobs(jobs) // ìµœì¢… jobsì˜ Lengthë¥¼ ì¶œë ¥í•˜ë©° ì¢…ë£Œ \tfmt.Println(\u0026#34;Done : \u0026#34;, len(jobs)) } // urlì„ ì „ë‹¬ ë°›ì•„ pagesë¥¼ ë°˜í™˜í•¨ func getPages(url string) int { // returní•  pagesë¥¼ ì´ˆê¸°í™” \tpages := 0 // net/http íŒ¨í‚¤ì§€ì˜ Getí•¨ìˆ˜ë¥¼ í†µí•´ í•´ë‹¹ urlì˜ Responseë¥¼ ë°˜í™˜ \tres, err := http.Get(url) // net/http Getì„ í†µí•´ ì „ë‹¬ ë°›ì€ errì„ ì²´í¬ (nil) \tcheckErr(err) // net/http Getì„ í†µí•´ ì „ë‹¬ ë°›ì€ resë¥¼ ì²´í¬ (200) \tcheckCode(res) // deferë¥¼ í†µí•´ responseì˜ Bodyë¥¼ Close \tdefer res.Body.Close() // BeautifulSoup : HTML,XMLì—ì„œ dataë¥¼ ì¶”ì¶œí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ \t// goquery : beautifulsoup ìœ ì‚¬ library \t// res.Bodyì—ì„œ documentë¥¼ ì¶”ì¶œ \tdoc, err := goquery.NewDocumentFromReader(res.Body) // goquery.NewDocumentFromReaderë¥¼ í†µí•´ ì „ë‹¬ ë°›ì€ errì„ ì²´í¬ \tcheckErr(err) println(\u0026#34;Base Url : \u0026#34;, url) // goquery.NewDocumentFromReaderë¥¼ í†µí•´ ì „ë‹¬ ë°›ì€ docì—ì„œ \t// 1) pagination ê²€ìƒ‰ \t// 2) a herfì˜ ê¸¸ì´ë¥¼ pagesë¡œ ì „ë‹¬ \tdoc.Find(\u0026#34;.pagination\u0026#34;).Each(func(i int, s *goquery.Selection) { // goquery LengthëŠ” elementì˜ ìˆ˜ë¥¼ returní•¨ \tpages = s.Find(\u0026#34;a\u0026#34;).Length() println(\u0026#34;Total Pages : \u0026#34;, pages) }) // pagesë¥¼ returní•¨ \treturn pages } // net/http Getì„ í†µí•´ ì „ë‹¬ë°›ì€ errì„ ì²´í¬ func checkErr(err error) { // errê°€ nilì´ ì•„ë‹ˆë¼ë©´ fatalln í˜¸ì¶œ \tif err != nil { log.Fatalln(err) } } // net/http Getì„ í†µí•´ ì „ë‹¬ë°›ì€ resë¥¼ ì²´í¬ func checkCode(res *http.Response) { // responseì˜ statusCodeê°€ 200ì´ ì•„ë‹ˆë¼ë©´ fatalln í˜¸ì¶œ \tif res.StatusCode != 200 { log.Fatalln(\u0026#34;Request failed with Status : \u0026#34;, res.StatusCode) } } // Page, BaseUrl, Channelì„ ì „ë‹¬ ë°›ì•„ Jobsë¥¼ return func getPage(page int, url string, mainC chan\u0026lt;- []extractedJob) { // extractedJob êµ¬ì¡°ì²´ ë°°ì—´í˜• jobsë¥¼ ì„ ì–¸ \tvar jobs []extractedJob // extractedJob êµ¬ì¡°ì²´ ì±„ë„ ì„ ì–¸ \tc := make(chan extractedJob) // baseUrlì— pageìˆ˜ * 50ì˜ ì‹œì‘ í˜ì´ì§€ ìƒì„± ë° íŒŒì‹± \tpageURL := url + \u0026#34;\u0026amp;start=\u0026#34; + strconv.Itoa(page*50) // Request pageURL í‘œì‹œ \tfmt.Println(\u0026#34;Request : \u0026#34;, pageURL) // net/http íŒ¨í‚¤ì§€ì˜ Getí•¨ìˆ˜ë¥¼ í†µí•´ í•´ë‹¹ pageURLì˜ Responseë¥¼ ë°˜í™˜ \tres, err := http.Get(pageURL) // net/http Getì„ í†µí•´ ì „ë‹¬ ë°›ì€ errì„ ì²´í¬ (nil) \tcheckErr(err) // net/http Getì„ í†µí•´ ì „ë‹¬ ë°›ì€ resë¥¼ ì²´í¬ (200) \tcheckCode(res) // deferë¥¼ í†µí•´ responseì˜ Bodyë¥¼ Close \tdefer res.Body.Close() // goquery.NewDocumentFromReaderë¥¼ í†µí•´ res.Bodyì—ì„œ docë¥¼ ì¶”ì¶œ \tdoc, err := goquery.NewDocumentFromReader(res.Body) // goquery.NewDocumentFromReaderë¥¼ í†µí•´ ì „ë‹¬ ë°›ì€ errì„ ì²´í¬ \tcheckErr(err) // goquery.NewDocumentFromReaderë¥¼ í†µí•´ ì „ë‹¬ ë°›ì€ docì—ì„œ \t// 1) jobsearch-SerpJobCardë¥¼ ì¶”ì¶œ \tsearchCards := doc.Find(\u0026#34;.jobsearch-SerpJobCard\u0026#34;) searchCards.Each(func(i int, card *goquery.Selection) { // go routineì„ í†µí•´ extractJob ìˆ˜í–‰ \t// jobcard, channelì„ ì¸ìë¡œ ì „ë‹¬ \tgo extractJob(card, c) }) // ê²€ìƒ‰ëœ Card ìˆ˜ë§Œí¼ forë¥¼ íƒ€ë©° \t// extractedJob êµ¬ì¡°ì²´ ë°°ì—´í˜• Jobsì— \t// extractedJob êµ¬ì¡°ì²´í˜• ì±„ë„ cë¡œ ì „ë‹¬ë°›ì€ \t// extractedJob êµ¬ì¡°ì²´í˜• jobì„ append \tfor i := 0; i \u0026lt; searchCards.Length(); i++ { job := \u0026lt;-c jobs = append(jobs, job) } // extractedJob êµ¬ì¡°ì²´ ë°°ì—´í˜• Jobsë¥¼ mainC ì±„ë„ì— ì „ë‹¬ \tmainC \u0026lt;- jobs } // pageURLì—ì„œ ì°¾ì€ jobCard ë° channelì„ ì¸ìë¡œ ì „ë‹¬ ë°›ìŒ // Channelì— ì •ì œëœ Dataë¥¼ ì „ë‹¬í•˜ì—¬ Return func extractJob(card *goquery.Selection, c chan\u0026lt;- extractedJob) { // JobCardì˜ data-jkë¥¼ IDë¡œ ì „ë‹¬ \tid, _ := card.Attr(\u0026#34;data-jk\u0026#34;) // JobCardì˜ \u0026#34;title\u0026gt;a\u0026#34;ì˜ TEXTë¥¼ titleë¡œ ì „ë‹¬ \t// ì´ë•Œ cleanStringì„ í†µí•´ Trim \ttitle := cleanString(card.Find(\u0026#34;.title\u0026gt;a\u0026#34;).Text()) // JobCardì˜ \u0026#34;sjcl\u0026#34;\u0026#34;ì˜ TEXTë¥¼ locationë¡œ ì „ë‹¬ \t// ì´ë•Œ cleanStringì„ í†µí•´ Trim \tlocation := cleanString(card.Find(\u0026#34;.sjcl\u0026#34;).Text()) // JobCardì˜ \u0026#34;salaryText\u0026#34;\u0026#34;ì˜ TEXTë¥¼ salaryë¡œ ì „ë‹¬ \t// ì´ë•Œ cleanStringì„ í†µí•´ Trim \tsalary := cleanString(card.Find(\u0026#34;.salaryText\u0026#34;).Text()) // JobCardì˜ \u0026#34;summary\u0026#34;\u0026#34;ì˜ TEXTë¥¼ summaryë¡œ ì „ë‹¬ \t// ì´ë•Œ cleanStringì„ í†µí•´ Trim \tsummary := cleanString(card.Find(\u0026#34;.summary\u0026#34;).Text()) // extractedJob êµ¬ì¡°ì²´í˜• ì±„ë„ cì— Trimëœ ê° ì¶”ì¶œ í•­ëª©ì„ ì „ë‹¬ \tc \u0026lt;- extractedJob{id: id, title: title, location: location, salary: salary, summary: summary} } // ì „ë‹¬ ë°›ì€ Sringì„ Trimí•˜ê³  ê³µë°±ìœ¼ë¡œ ì¬ì—°ê²° func cleanString(str string) string { return strings.Join(strings.Fields(strings.TrimSpace(str)), \u0026#34; \u0026#34;) } // extractedJob êµ¬ì¡°ì²´ ë°°ì—´í˜• jobsë¥¼ ì¸ìë¡œ ë°›ìŒ func writeJobs(jobs []extractedJob) { // os.Createë¥¼ í†µí•´ jobs.csv íŒŒì¼ì„ create \tfile, err := os.Create(\u0026#34;jobs.csv\u0026#34;) // os.Createì„ í†µí•´ ì „ë‹¬ ë°›ì€ errì„ ì²´í¬ (nil) \tcheckErr(err) // csv.NewWriterë¥¼ í†µí•´ Writerí•­ëª©ì„ wë¡œ ì •ì˜ \tw := csv.NewWriter(file) // deferë¥¼ í†µí•´ ì¢…ë£Œì‹œ wë¥¼ flush \tdefer w.Flush() // Headerë¥¼ ìŠ¤íŠ¸ë§ ë°°ì—´ë¡œ ì„ ì–¸ \theaders := []string{\u0026#34;Link\u0026#34;, \u0026#34;Title\u0026#34;, \u0026#34;Location\u0026#34;, \u0026#34;Salary\u0026#34;, \u0026#34;Summary\u0026#34;} // csv.Writerì˜ Writeë¥¼ í†µí•´ stringë°°ì—´ì¸ headersë¥¼ ê¸°ì…í•˜ê³  errë¥¼ ì „ë‹¬ë°›ìŒ \twErr := w.Write(headers) // csv.Writerì˜ Writeì„ í†µí•´ ì „ë‹¬ ë°›ì€ errì„ ì²´í¬ (nil) \tcheckErr(wErr) // String ë°°ì—´ì˜ ì±„ë„ ìƒì„± \tc := make(chan []string) // ë‚˜ë¦„ ë°±ë¯¸ : channelì„ ëê¹Œì§€ ì“°ë©° ë¶„ì‚°ì‹œí‚´ \t// extractJob êµ¬ì¡°ì²´ ë°°ì—´ Jobsì˜ range ë§Œí¼ \t// extractJob êµ¬ì¡°ì²´ jobì˜ forë¥¼ ëŒë©° \t// jobê³¼ cë¥¼ ì „ë‹¬í•˜ë©° writeJObì„ ê³ ë£¨í‹´ìœ¼ë¡œ ìˆ˜í–‰ \tfor _, job := range jobs { go writeJob(job, c) //jobSlice := []string{\u0026#34;https://kr.indeed.com/viewjob?jk=\u0026#34; + job.id, job.title, job.location, job.salary, job.summary} \t//jwErr := w.Write(jobSlice) \t//checkErr(jwErr) \t} // Jobsì˜ Length ë§Œí¼ Forë¡œ ë” \tfor i := 0; i \u0026lt; len(jobs); i++ { // String ë°°ì—´í˜• ì±„ë„ì—ì„œ jSliceë¡œ ì „ë‹¬ ë°›ìŒ \tjSlice := \u0026lt;-c // jSliceë¥¼ csvì— ê¸°ì… \tjwErr := w.Write(jSlice) // csv.Writerì˜ Writeì„ í†µí•´ ì „ë‹¬ ë°›ì€ errì„ ì²´í¬ (nil) \tcheckErr(jwErr) } } // extractedJob êµ¬ì¡°ì²´ì¸ jobê³¼ // string ë°°ì—´ì¸ cë¥¼ ì¸ìë¡œ ì „ë‹¬ ë°›ìŒ func writeJob(job extractedJob, c chan\u0026lt;- []string) { // string ë°°ì—´ jobSliceë¥¼ ì„ ì–¸ \t// 1) Job ID : ë§í¬í•­ëª© \t// 2) Job Title \t// 3) Job Location\t\t// 4) Job Salary \t// 5) Job Summary \tjobSlice := []string{\u0026#34;https://kr.indeed.com/viewjob?jk=\u0026#34; + job.id, job.title, job.location, job.salary, job.summary} // String ë°°ì—´ JobSliceë¥¼ String ë°°ì—´í˜• Channelì— ì „ë‹¬ \tc \u0026lt;- jobSlice }      ë‹¤ìŒê¸€  "});index.add({'id':21,'href':'/docs/documents/backend/go/go03/','title':"GO DB ì—°ê²°",'section':"Go",'content':"Go Oralce DB ì—°ê²° #   Goì˜ Godrorì„ ì´ìš©í•œ ê°„ë‹¨í•œ Oracle DB ì—°ê²° ë° ì¡°íšŒ\nOracle DB ì—°ê²° #   oracle db ì—°ê²° ...  main.go\npackage main import ( \u0026#34;database/sql\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; _ \u0026#34;github.com/godror/godror\u0026#34; ) type str_item struct { item_cd string item_nm string } type str_item_ls struct { str_item []str_item } func (item_ls *str_item_ls) AddItem(item str_item) []str_item { item_ls.str_item = append(item_ls.str_item, item) return item_ls.str_item } func get_list(sql_str string, item_cnt int, item_list *str_item_ls, c chan bool) { // ë³¸ì¸ ê³„ì •ì— ë§ëŠ” ì ‘ì† ì •ë³´ \tdb, err := sql.Open(\u0026#34;godror\u0026#34;, \u0026#34;db_account/db_password@db_name\u0026#34;) db.SetMaxOpenConns(5) db.SetMaxIdleConns(3) if err != nil { fmt.Println(err) return } defer db.Close() conn, err := db.Query(sql_str, item_cnt) if err != nil { fmt.Println(err) return } defer conn.Close() item := new(str_item) var item_cd string var item_nm string for conn.Next() { err := conn.Scan(\u0026amp;item_cd, \u0026amp;item_nm) if err != nil { log.Fatal(err) } item.item_cd = item_cd item.item_nm = item_nm item_list.AddItem(*item) } c \u0026lt;- true } func main() { c := make(chan bool) item_list := new(str_item_ls) // ë³¸ì¸ DB í…Œì´ë¸” ì¡°íšŒ \tgo get_list(\u0026#34;select tname,tabtype from tab where substr(tname,1,1) in (\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;C\u0026#39;,\u0026#39;D\u0026#39;,\u0026#39;E\u0026#39;,\u0026#39;F\u0026#39;,\u0026#39;G\u0026#39;,\u0026#39;H\u0026#39;,\u0026#39;I\u0026#39;) and rownum \u0026lt;= :val\u0026#34;, 6, item_list, c) go get_list(\u0026#34;select tname,tabtype from tab where substr(tname,1,1) in (\u0026#39;J\u0026#39;,\u0026#39;K\u0026#39;,\u0026#39;L\u0026#39;,\u0026#39;M\u0026#39;,\u0026#39;N\u0026#39;,\u0026#39;O\u0026#39;,\u0026#39;P\u0026#39;,\u0026#39;Q\u0026#39;,\u0026#39;R\u0026#39;) and rownum \u0026lt;= :val\u0026#34;, 6, item_list, c) go get_list(\u0026#34;select tname,tabtype from tab where substr(tname,1,1) in (\u0026#39;S\u0026#39;,\u0026#39;T\u0026#39;,\u0026#39;U\u0026#39;,\u0026#39;V\u0026#39;,\u0026#39;W\u0026#39;,\u0026#39;X\u0026#39;,\u0026#39;Y\u0026#39;,\u0026#39;Z\u0026#39;) and rownum \u0026lt;= :val\u0026#34;, 6, item_list, c) for i := 0; i \u0026lt; 3; i++ { fmt.Println(\u0026lt;-c) } for i, s := range item_list.str_item { fmt.Println(s.item_cd, s.item_nm, i) } }      ë‹¤ìŒê¸€  "});index.add({'id':22,'href':'/docs/documents/backend/sql/sql02/','title':"SQL Advanced",'section':"SQL",'content':"SQL Advanced #   exists / not exists #   exists / not exists ...  select * from my_table_1 a where exists ( select \u0026#39;x\u0026#39; from my_table_1 x where x.column_1 = a.column_1 and x.column_2 = a.column_2 and x.column_2 \u0026lt;= 30 ) ; select * from my_table_1 a where not exists ( select \u0026#39;x\u0026#39; from my_table_1 x where x.column_1 = a.column_1 and x.column_2 = a.column_2 and x.column_2 \u0026lt;= 30 ) ;     pivot / unpivot #   pivot ...  select round(hong,2) hong,round(kim,2) kim,round(lee,2) lee,round(park,2) park,round(shin,2) shin,round(choi,2) choi from ( select substr(column_1,1,1) fnm,avg(column_2) avg_age from my_table_1 a group by substr(column_1,1,1) ) pivot (avg(avg_age) for fnm in (\u0026#39;í™\u0026#39; as hong,\u0026#39;ê¹€\u0026#39; as kim,\u0026#39;ì´\u0026#39; as lee,\u0026#39;ë°•\u0026#39; as park,\u0026#39;ì‹ \u0026#39; as shin,\u0026#39;ìµœ\u0026#39; as choi)) ;      unpivot ...  with fmnm_list as ( select round(hong,2) hong,round(kim,2) kim,round(lee,2) lee,round(park,2) park,round(shin,2) shin,round(choi,2) choi from ( select substr(column_1,1,1) fnm,avg(column_2) avg_age from my_table_1 a group by substr(column_1,1,1) ) pivot (avg(avg_age) for fnm in (\u0026#39;í™\u0026#39; as hong,\u0026#39;ê¹€\u0026#39; as kim,\u0026#39;ì´\u0026#39; as lee,\u0026#39;ë°•\u0026#39; as park,\u0026#39;ì‹ \u0026#39; as shin,\u0026#39;ìµœ\u0026#39; as choi)) ) select * from fmnm_list a unpivot (age for fnm in (hong as \u0026#39;H\u0026#39;,kim as \u0026#39;K\u0026#39;,lee as \u0026#39;L\u0026#39;,park as \u0026#39;P\u0026#39;,shin as \u0026#39;S\u0026#39;,choi as \u0026#39;C\u0026#39;)) ;     lead / lag #   lead / lag ...  select column_1,column_2, lead(column_1,1,\u0026#39;-\u0026#39;) over (partition by 1 order by column_2) lead_col, lag(column_1,1,\u0026#39;-\u0026#39;) over (partition by 1 order by column_2) lag_col from my_table_1 order by column_2 ; select listagg(column_1,\u0026#39;,\u0026#39;) within group (order by column_2) name_list from my_table_1 ;     rows / range / preceding / following / current / unbounded #   rows / range / preceding / following / current / unbounded  ...  select column_1,column_2, sum(column_2) over (partition by 1 order by column_1 rows between 2 preceding and 1 preceding) before_two_rows, sum(column_2) over (partition by 1 order by column_1 rows between 1 following and 2 following) after_two_rows, sum(column_2) over (partition by 1 order by column_1 rows between unbounded preceding and current row) before_all_rows, avg(column_2) over (partition by 1 order by column_1 rows between unbounded preceding and current row) row_sample, avg(column_2) over (partition by 1 order by column_1 range between unbounded preceding and current row) range_sample from my_table_1 order by column_1 ;     connect by #   connect by  ...  with data_set as ( select \u0026#39;A\u0026#39; frid, \u0026#39;D\u0026#39; toid from dual union all select \u0026#39;D\u0026#39; frid, \u0026#39;B\u0026#39; toid from dual union all select \u0026#39;B\u0026#39; frid, \u0026#39;C\u0026#39; toid from dual union all select \u0026#39;C\u0026#39; frid, \u0026#39;G\u0026#39; toid from dual union all select \u0026#39;C\u0026#39; frid, \u0026#39;H\u0026#39; toid from dual union all select \u0026#39;G\u0026#39; frid, \u0026#39;E\u0026#39; toid from dual union all select \u0026#39;H\u0026#39; frid, \u0026#39;I\u0026#39; toid from dual ) select level lvl, connect_by_root(a.frid) root, a.frid,a.toid, sys_connect_by_path(frid,\u0026#39;\u0026gt;\u0026#39;)||\u0026#39;\u0026gt;\u0026#39;||toid conn_str from data_set a start with frid = \u0026#39;A\u0026#39; connect by prior toid = frid order by level ;     ratio_to_report #   ratio_to_report  ...  select column_1,column_2, sum(column_2) over (partition by 1) age_sum, column_2/sum(column_2) over (partition by 1) age_rto_1, ratio_to_report(column_2) over (partition by 1) age_rto_2 from my_table_1 ;     create as #   create as  ...  create table my_table_2 as select * from my_table_1 ; create table my_table_3 as select column_1 as co1,column_2 as co2 from my_table_1 ;     merge #   merge  ...  merge into my_table_1 a using ( select column_1,column_2+10 column_2,column_3 from my_table_1 where substr(column_1,1,1) in (\u0026#39;í™\u0026#39;,\u0026#39;ë°•\u0026#39;,\u0026#39;ì‹ \u0026#39;) union all select \u0026#39;ìœ ì†Œì˜\u0026#39; column_1,27 column_2,sysdate column_3 from dual ) b on ( a.column_1 = b.column_1 ) when matched then update set a.column_2 = column_2 when not matched then insert (a.column_1,a.column_2,a.column_3) values (b.column_1,b.column_2,b.column_3) ;      ë‹¤ìŒê¸€  "});index.add({'id':23,'href':'/docs/documents/devops/','title':"Dev ops",'section':"Documents",'content':"Dev ops #     Oracle   Description   Synology   Description   Ubuntu   Description   Docker   Description   AWS   Description   vi   vi editor ëª…ë ¹ì–´ ì •ë¦¬   "});index.add({'id':24,'href':'/docs/documents/devops/vi/','title':"vi",'section':"Dev ops",'content':"VI Editor #   VIëŠ” UNIX ê³„ì—´ ìš´ì˜ì²´ê³„ì—ì„œ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë¬¸ì„œ í¸ì§‘ê¸°ë¡œ BSD Unix ê°œë°œì— ì¤‘ìš”í•œ ì—­í• ì„ í–ˆë˜ William Nelson Joyë¼ëŠ” ë¶„ì´ ë§Œë“¬. Unixì—ì„œ ë³„ë„ ì—ë””í„° ì—†ì´ ì‚¬ìš©í•´ì•¼ë§Œ í•˜ëŠ” ìƒí™©ì´ ì¢…ì¢… ë°œìƒë˜ëŠ”ë° ë§¤ë²ˆ ì°¾ì•„ë³´ê²Œ ë˜ì—¬ ë³„ë„ë¡œ ì •ë¦¬í•¨.\nEX Command #  vi+Enter : vi ì‹¤í–‰ vi file.txt : file.txt íŒŒì¼ ìƒì„± ë˜ëŠ” í¸ì§‘ vi -35 file.txt : file.txt ì˜¤í”ˆ í›„ 35í–‰ìœ¼ë¡œ ì´ë™ vi -r file.txt : file.txt ì†ìƒ íŒŒì¼ ë³µêµ¬ view file.txt : file.txt ì½ê¸° ì „ìš©ìœ¼ë¡œ ì—´ê¸°\n:q+Enter : vi ì¢…ë£Œ :q!+Enter : ì €ì¥ì—†ì´ ì¢…ë£Œ :wq : ë³€ê²½ì €ì¥ ì¢…ë£Œ\nInput Mode #  i : í˜„ì¬ ì»¤ì„œì—ì„œ ì…ë ¥ëª¨ë“œ ì§„ì… a : í˜„ì¬ ì»¤ì„œì˜ ë‹¤ìŒ ì¹¸ì—ì„œ ì…ë ¥ëª¨ë“œ ì§„ì… o : í˜„ì¬ ì»¤ì„œì˜ ë‹¤ìŒ ì¤„ì—ì„œ ì…ë ¥ëª¨ë“œ ì§„ì… s : í˜„ì¬ ì»¤ì„œ ìœ„ì¹˜ì˜ í•œìë¥¼ ì§€ìš°ë¡œ ì…ë ¥ëª¨ë“œ ì§„ì… I : í˜„ì¬ ì»¤ì„œ ì¤„ ë§¨ ì•ì—ì„œë¶€í„° ì…ë ¥ëª¨ë“œ ì§„ì… A : í˜„ì¬ ì»¤ì„œ ì¤„ ë§¨ ë§ˆì§€ë§‰ì—ì„œë¶€í„° ì…ë ¥ëª¨ë“œ ì§„ì… O : í˜„ì¬ ì»¤ì„œ ì¤„ ì´ì „ ì¤„ì—ì„œ ë¶€í„° ì…ë ¥ëª¨ë“œ ì§„ì… S : í˜„ì¬ ì»¤ì„œ ì¤„ì„ ì§€ìš°ê³  ì…ë ¥ëª¨ë“œ ì§„ì…\nCursor ì´ë™ #  h : ì»¤ì„œ ì™¼ìª½ í•œì¹¸ ì´ë™ j : ì»¤ì„œ ì•„ë˜ìª½ í•œì¹¸ ì´ë™ k : ì»¤ì„œ ìœ„ìª½ í•œì¹¸ ì´ë™ l : ì»¤ì„œ ì˜¤ë¥¸ìª½ í•œì¹¸ ì´ë™ b : ì´ì „ ë‹¨ì–´ ì²«ê¸€ì ì´ë™ B : ì´ì „ ë‹¨ì–´ ì²«ê¸€ì ì´ë™ w : ë‹¤ìŒ ë‹¨ì–´ ì²«ê¸€ì ì´ë™ W : ë‹¤ìŒ ë‹¨ì–´ ì²«ê¸€ì ì´ë™ e : ë‹¤ìŒ ë‹¨ì–´ ëê¸€ì ì´ë™ E : ë‹¤ìŒ ë‹¨ì–´ ëê¸€ì ì´ë™ ^ : í•´ë‹¹ì¤„ì˜ ì²«ê¸€ìë¡œ ì´ë™ $ : í•´ë‹¹ì¤„ì˜ ëê¸€ìë¡œ ì´ë™ :0 : í•´ë‹¹ì¤„ì˜ ì œì¼ ì²˜ìŒìœ¼ë¡œ ì´ë™ :$ : í•´ë‹¹ì¤„ì˜ ì œì¼ ëìœ¼ë¡œ ì´ë™\n   : ë‹¤ìŒì¤„ì˜ ì²«ê¸€ìë¡œ ì´ë™       : ì´ì „ì¤„ì˜ ì²«ê¸€ìë¡œ ì´ë™    ( : ì´ì „ ë¬¸ì¥ì˜ ì²«ê¸€ìë¡œ ì´ë™ ) : ë‹¤ìŒ ë¬¸ì¥ì˜ ì²«ê¸€ìë¡œ ì´ë™ { : ì´ì „ ë¬¸ë‹¨ìœ¼ë¡œ ì´ë™ } : ë‹¤ìŒ ë¬¸ë‹¨ìœ¼ë¡œ ì´ë™\nG : ì œì¼ ë í–‰ìœ¼ë¡œ ì´ë™ gg : ì œì¼ ì²« í–‰ìœ¼ë¡œ ì´ë™ 35G : 35ë²ˆì§¸ í–‰ìœ¼ë¡œ ì´ë™ :35 + Enter : 35ë²ˆì§¸ í–‰ìœ¼ë¡œ ì´ë™ H : í™”ë©´ì˜ ì²«ì¤„ë¡œ ì´ë™ M : í™”ë©´ì˜ ì¤‘ê°„ìœ¼ë¡œ ì´ë™ L : í™”ë©´ì˜ ëì¤„ë¡œ ì´ë™ Ctrl + i : ì´ì „ í™”ë©´ìœ¼ë¡œ ì´ë™ Ctrl + b : ë‹¤ìŒ í™”ë©´ìœ¼ë¡œ ì´ë™ Ctrl + d : ë‹¤ìŒ ë°˜ í™”ë©´ìœ¼ë¡œ ì´ë™ Ctrl + f : ë‹¤ìŒ í™”ë©´ìœ¼ë¡œ ì´ë™ 35% : 35%ì— í•´ë‹¹í•˜ëŠ” ì§€ì ìœ¼ë¡œ ì´ë™\nCopy/Delete/Paste #  x : í˜„ì¬ ì»¤ì„œ ìœ„ì¹˜ ë¬¸ì ì‚­ì œ 5x : í˜„ì¬ ì»¤ì„œ ìœ„ì¹˜ + 4ê¸€ì ì‚­ì œ x,dl : ì»¤ì„œ ìœ„ì¹˜ì˜ ê¸€ì ì‚­ì œ X : í˜„ì¬ ì»¤ì„œ ì´ì „ ê¸€ì ì‚­ì œ X,dh : ì»¤ì„œ ë°”ë¡œ ì•ì˜ ê¸€ì ì‚­ì œ dw : í˜„ì¬ ì»¤ì„œ ë‹¨ì–´ ì‚­ì œ d0 : í˜„ì¬ ì»¤ì„œìœ„ì¹˜ë¶€í„° ì¤„ì˜ ì²˜ìŒê¹Œì§€ ì‚­ì œ dd : í˜„ì¬ ì»¤ì„œ ìœ„ì¹˜ í–‰ ì‚­ì œ dj\t: í˜„ì¬ ì»¤ì„œê°€ ìˆëŠ” ì¤„ê³¼ ê·¸ ë‹¤ìŒ ì¤„ì„ ì‚­ì œ dk : í˜…ì¬ ì»¤ì„œê°€ ìˆëŠ” ì¤„ê³¼ ê·¸ ì´ì „ ì¤„ì„ ì‚­ì œ 3dd : í˜„ì¬ ì»¤ì„œ ìœ„ì¹˜ í–‰ë¶€í„° ë‹¤ìŒ 3í–‰ê¹Œì§€ ì‚­ì œ D : í˜„ì¬ ì»¤ì„œ ìœ„ì¹˜ë¶€í„° í–‰ëê¹Œì§€ ì‚­ì œ D, d$\t: í˜„ì¬ ì»¤ì„œ ìœ„ì¹˜ë¶€í„° ì¤„ì˜ ëê¹Œì§€ ì‚­ì œ yy : í˜„ì¬ ì»¤ì„œê°€ ìˆëŠ” í–‰ì„ ë³µì‚¬ 3 yy : í˜„ì¬ ì»¤ì„œ ë¶€í„° ë‹¤ìŒ 3í–‰ì„ ë³µì‚¬ yw : í˜„ì¬ ì»¤ì„œ ìœ„ì¹˜ ë¶€í„° ë‹¨ì–´ì˜ ëê¹Œì§€ ë³µì‚¬ y0 : í˜„ì¬ ì»¤ì„œ ìœ„ì¹˜ ë¶€í„° í•´ë‹¹ í–‰ì„ ì²˜ìŒê¹Œì§€ ë³µì‚¬ y$ : í˜„ì¬ ì»¤ì„œ ìœ„ì¹˜ ë¶€í„° í•´ë‹¹ í–‰ì˜ ëê¹Œì§€ ë³µì‚¬ yj : í˜„ì¬ ì»¤ì„œ ìœ„ì¹˜ ë¶€í„° ë‹¤ìŒ ì¤„ê¹Œì§€ ë³µì‚¬ yk : í˜„ì¬ ì»¤ì„œ ìœ„ì¹˜ ë¶€í„° ìŠì „ ì¤„ê¹Œì§€ ë³µì‚¬ p : ë³µì‚¬ë‚´ìš©ì„ í˜„ì¬ í–‰ ì´í›„ì— ë¶™ì—¬ ë„£ê¸° P : ë³µì‚¬ë‚´ìš©ì„ í˜€ë‚´ í–‰ ì´ì „ì— ë¶™ì—¬ ë„£ê¸° r : í˜„ì¬ ì»¤ì„œ ìœ„ì¹˜ í•œê¸€ì êµì²´ R : í˜„ì¬ ì»¤ì„œ ìœ„ì¹˜ ESC ëˆ„ë¥´ê¸° ì „ê¹Œì§€ ë‹¤ë¥¸ ê¸€ìë¡œ êµì²´ s, cl : í˜„ì¬ ì»¤ì„œ ìœ„ì¹˜ ê¸€ì í•œê°œë¥¼ ì—¬ëŸ¬ ê¸€ìë¡œ êµì²´ ch : í˜„ì¬ ì»¤ì„œ ìœ„ì¹˜ ì´ì „ ê¸€ì í•œê°œë¥¼ ì—¬ëŸ¬ ê¸€ìë¡œ êµì²´ cw : ì»¤ì„œ ìœ„ì¹˜ì˜ í•œë‹¨ì–´ë¥¼ êµì²´ cO : ì»¤ì„œ ìœ„ì¹˜ë¶€í„° ì¤„ì˜ ì²˜ìŒê¹Œì§€ êµì²´ C, c$ : ì»¤ì„œ ìœ„ì¹˜ë¶€í„° ì¤„ì˜ ëê°€ì§€ êµì²´ cc : ì»¤ì„œê°€ ìˆëŠ” ì¤„ì„ êµì²´ cj : ì»¤ì„œê°€ ìˆëŠ” ì¤„ê³¼ ê·¸ ë‹¤ìŒ ì¤„ì„ êµì²´ ck : ì»¤ì„œê°€ ìˆëŠ” ì¤„ê³¼ ê·¸ ì´ì „ ì¤„ì„ êµì²´\nUndo/Redo #  u : Undo U : í•´ë‹¹ ì¤„ì˜ ì‘ì—… ëª¨ë‘ ì·¨ì†Œ Ctrl+r : Redo . : ì§ì „ ëª…ë ¹ ë°˜ë³µ J : í˜„ì¬ í–‰ê³¼ ë‹¤ìŒ í–‰ì„ ì´ì–´ ë¶™ì„ ~ : ëŒ€ë¬¸ì/ì†Œë¬¸ì ì „í™˜ % : ê´„í˜¸ ë°˜ëŒ€ìª½ ì´ë™ Ctrl+l : í˜„ì¬ í™”ë©´ì„ ì§€ìš°ê³  ë‹¤ì‹œ ë¡œë“œ Ctrl+g : íŒŒì¼ì— ê´€í•œ ì •ë³´ í‘œì‹œ\nSearch #  /text+Enter : \u0026ldquo;text\u0026rdquo; ë¬¸ìì—´ì„ í˜„ì¬ ì»¤ì„œ ì´í›„ë¶€í„° ê²€ìƒ‰ ?text+Enter : \u0026ldquo;text\u0026rdquo; ë¬¹ìì—´ì„ ì—­ë°©í–¥ìœ¼ë¡œ ê²€ìƒ‰ n\t: ì°¾ì€ ë¬¸ì ë‹¤ìŒ ì´ë™ N : ì°¾ì€ ë¬¸ì ì´ì „ ì´ë™\nReplace #  :%s/text/newtext/ig : \u0026ldquo;text\u0026quot;ë¥¼ ì°¾ì•„ \u0026ldquo;newtext\u0026quot;ë¡œ ì¹˜í™˜. i ëŒ€ì†Œë¬¸ì êµ¬ë¶„ì—†ìŒ. g Global ë³€ê²½. :%s/text/newtext/gc : \u0026ldquo;text\u0026quot;ë¥¼ ì°¾ì•„ \u0026ldquo;newtext\u0026quot;ë¡œ ì¹˜í™˜. g Global ë³€ê²½. c ë³€ê²½ì „ Confirm.\nETC #  :set number : í–‰ë²ˆí˜¸ ì¶œë ¥ :set nonumber : í–‰ë²ˆí˜¸ ìˆ¨ê¹€ :cd : í˜„ì¬ ë””ë ‰í† ë¦¬ ì¶œë ¥\n"});index.add({'id':25,'href':'/docs/documents/deeplearning/','title':"Deep Learning",'section':"Documents",'content':"Deep Learning #     Machine Learning   ë¨¸ì‹ ëŸ¬ë‹ í•™ìŠµ ìë£Œ   Tensorflow   Description   "});index.add({'id':26,'href':'/docs/documents/deeplearning/machinelearning/','title':"Machine Learning",'section':"Deep Learning",'content':"Machine Learning #   ë‹¤ìŒ ì‚¬ì´íŠ¸ë“±ì„ ì°¸ì¡°í•˜ì—¬ ë¨¸ì‹  ëŸ¬ë‹ ê´€ë ¨ ìë£Œ ì •ë¦¬.\n ë¨¸ì‹ ëŸ¬ë‹/ë”¥ëŸ¬ë‹ ê°•ì˜ ëª¨ë‘ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹ ì‹œì¦Œ2 ì˜¤í”ˆíŠœí† ë¦¬ì–¼-ìƒí™œí† ë‹¹-ë¨¸ì‹ ëŸ¬ë‹1  Machine Learning ë¶„ë¥˜ #  ë¨¸ì‹  ëŸ¬ë‹ì€ ì§€ë„í•™ìŠµ, ë¹„ì§€ë„í•™ìŠµ, ê°•í™”í•™ìŠµìœ¼ë¡œ ë¶„ë¥˜. ì§€ë„í•™ìŠµì€ ë‹¤ì‹œ íšŒê·€ì™€ ë¶„ë¥˜, ë¹„ì§€ë„ í•™ìŠµì€ êµ°ì§‘í™”,ë³€í™˜,ì—°ê´€ìœ¼ë¡œ ë¶„ë¥˜1.\n Supervised Learning  Regression Classification   Unsupervised Learning  Clustering Transform Association   Reinforcement Learning  Linear Regression2 #  Regressionì´ë€ ì „ì²´ì˜ í‰ê· ìœ¼ë¡œ íšŒê·€í•˜ë ¤ëŠ” ì†ì„±ì„ ë‚˜íƒ€ëƒ„. ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ê°€ì¥ ì˜ ì„¤ëª… í•  ìˆ˜ ìˆëŠ” ì§ì„ ì˜ ë°©ì •ì‹ì„ ì°¾ì•„ë‚´ëŠ” ê²ƒì„ ì˜ë¯¸.\n  --  linear regression ...   H(X) = WX+B ( y = ax + b ) H : Hypothesis (ê°€ì„¤) W : Weight (ê¸°ìš¸ê¸°) B : Bias (ì ˆë³€)     CostëŠ” ë°ì´í„°ì™€ ì§ì„  ì°¨ì´ì˜ í•©. ìŒìˆ˜, ì–‘ìˆ˜ ëª¨ë‘ ì¡´ì¬í•¨ìœ¼ë¡œ ì œê³±ì„ ì‚¬ìš©.\n  --  cost ...   Cost :   \\( H(x) - y \\)  Total Cost :  \\( \\frac{(H(x_1)-y_1)^2 \u0026#43; (H(x_2)-y_2)^2 \u0026#43; ... \u0026#43; (H(x_n)-y_n)^2 }{n} \\)  Total Cost : cost(W) =  \\( \\frac{1}{n}\\, \\textstyle\\sum_{i=1}^n \\,(Wx_i-y)^2 \\)  Cost Function : cost(W,b) =  \\( \\frac{1}{n}\\, \\textstyle\\sum_{i=1}^n \\,(H(x_i)-y_i)^2 \\)  Goal : minimize_w,_b cost(W,b)     Goalì€ ê°€ì„¤ê³¼ ì‹¤ì œ ë°ì´í„° ì°¨ì´ì˜ ì œê³±ì´ ìµœì†Œê°€ ë˜ëŠ” ê¸°ìš¸ê¸°ì™€ ì ˆí¸ì€ ì°¾ëŠ” ë¬¸ì œ.\nGradient Descent #  ì„ì˜ ì ì—ì„œ ì‹œì‘ í•™ìŠµë¥  ë§Œí¼ ê¸°ìš¸ê¸°ê°€ ë‚®ì€ ìª½ìœ¼ë¡œ ì§„í–‰í•˜ë©° ìµœì í•´ë¥¼ íƒìƒ‰í•˜ëŠ” ë°©ì‹ì„ ê²½ì‚¬í•˜ê°•ë²•ì´ë¼ í•¨. Convext ìƒí™©ì—ì„œëŠ” ì˜ ì‘ë™í•˜ë‚˜ Local Optimalì´ ì¡´ì¬í•  ê²½ìš° ë¬¸ì œê°€ ë°œìƒë¨3.\n  --  gradient descent ...    \\( W := W - \\alpha \\frac{\\partial}{\\partial W} \\, \\frac{1}{2m} \\textstyle\\sum_{i=1}^m (W(x_i)-y_i)^2 \\)   \\( W := W - \\alpha \\frac{1}{2m} \\, \\textstyle\\sum_{i=1}^m 2(W(x_i)-y_i)X_i \\)   \\( W := W - \\alpha \\frac{1}{m} \\, \\textstyle\\sum_{i=1}^m (W(x_i)-y_i)X_i \\)    \\( W := W - \\alpha \\frac{\\partial}{\\partial W} \\, cost(W) \\)   \\( \\frac {\\partial}{\\partial W} (WX-Y)^2 \\)  ì²´ì¸ë£° ì ìš©  \\( U = WX\u0026#43;Y \\to \\frac {\\partial U^2}{\\partial U} \\cdotp \\frac {\\partial U}{\\partial W}\\)   \\( 2U \\cdotp \\frac {\\partial U}{\\partial W}\\)   \\( 2(WX\u0026#43;Y) \\cdotp \\frac {\\partial (WX\u0026#43;Y)}{\\partial W}\\)   \\( 2(WX\u0026#43;Y) \\cdotp (X\u0026#43;0)\\)   \\( \\therefore 2(WX\u0026#43;Y)X\\)      Derivative #  í•¨ìˆ˜ f(x)ì— ëŒ€í•œ ë¯¸ë¶„ì€ ì•„ë˜ì™€ ê°™ìŒ. xì˜ ë³€í™”ëŸ‰ì´ 0ìœ¼ë¡œ ìˆ˜ë ´í• ë•Œ yì˜ ë³€í™”ëŸ‰ì„ ì˜ë¯¸.\n derivative ...    \\( f\u0026#39;(x) = \\frac{\\Delta f(x)}{\\Delta x} = \\lim\\limits_{\\Delta x \\rightarrow 0 } \\, \\frac{f(x\u0026#43;\\Delta x)-f(x)}{\\Delta x} \\)      ë¯¸ë¶„ ê¸°ì´ˆ ì •ë¦¬\n derivative basic ...    \\( f(x) = constant -\u0026gt; f\u0026#39;(x) = 0 \\)   \\( f\u0026#39;(x) = e ^ x =\u0026gt; f\u0026#39;(x) = e ^ x \\)   \\( f\u0026#39;(x) = e ^ -x =\u0026gt; f\u0026#39;(x) = -e ^ -x \\)   \\( f\u0026#39;(x) = ax ^ x =\u0026gt; f\u0026#39;(x) = nax ^ {n-1} \\)   \\( f\u0026#39;(x) = ln ^ x =\u0026gt; f\u0026#39;(x) = \\frac{1}{x} \\)   \\( \\frac{1}{x} = x^{-1} \\)      ë¯¸ë¶„ì˜ ì˜ë¯¸\n  meaning of derivative ...    \\( f(x) = x^2 \\)  ì¼ ê²½ìš°  \\( f\u0026#39;(x) = 2 * x^ {2-1} = 2x \\)   \\( f(2) = 2^2 = 4 \\)  ì¼ ê²½ìš°  \\( f\u0026#39;(2) = 2*2 = 4 \\)   \\( f(2) = 1^2 = 1 \\)  ì¼ ê²½ìš°  \\( f\u0026#39;(2) = 2*1 = 2 \\)   \\( f(2) = 0^2 = 0 \\)  ì¼ ê²½ìš°  \\( f\u0026#39;(2) = 2*0 = 0 \\)      ì¦‰ x = 2 ì¼ë•Œ ë¯¸ë¶„ f(x)ëŠ” 4ì˜ ë³€í™”ë¥¼ ê°€ì§€ê²Œ ë¨ ì¦‰ x = 1 ì¼ë•Œ ë¯¸ë¶„ f(x)ëŠ” 2ì˜ ë³€í™”ë¥¼ ê°€ì§€ê²Œ ë¨ ì¦‰ x = 0 ì¼ë•Œ ë¯¸ë¶„ f(x)ëŠ” 0ì˜ ë³€í™”ë¥¼ ê°€ì§€ê²Œ ë¨ ì¦‰ xì˜ í•œì ì—ì„œ yì˜ ë³€í™”ëŸ‰ì„ í†µí•´ ìœ„ì˜ x^2ì´ Costë¼ ê°€ì •í•˜ê³  ë°˜ë³µí•œë‹¤ë©´ ê¸°ìš¸ê¸°ê°€ 0ì´ë˜ëŠ” ì¦‰ Costê°€ ìµœì €ì¸ ì ì„ êµ¬í•  ìˆ˜ ìˆìŒ.\n practice 1 : derivative ...    \\( f(x) = 3x^2 \u0026#43; e^x \u0026#43; 7 \\)  ì˜  \\( f\u0026#39;(x) ? \\)   \\( = 3*2*x^{2-1} \u0026#43; e^x \u0026#43; 0 = 6x \u0026#43; e^x \\)       practice 2 : derivative ...    \\( f(x) = lnx \u0026#43; \\frac{1}{x} \\)  ì˜  \\( f\u0026#39;(x) ? \\)    \\( = \\frac{1}{x} - \\frac{1}{x^2} \\)      ì…ë ¥ë³€ìˆ˜ê°€ í•˜ë‚˜ ì´ìƒì¸ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì—ì„œ ì‚¬ìš©í•˜ëŠ” í¸ë¯¸ë¶„ì„ ì‚¬ìš©. í¸ë¯¸ë¶„ì€ ë¯¸ë¶„í•˜ê³ ì í•˜ëŠ” ë³€ìˆ˜ë¥¼ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ìƒìˆ˜ë¡œ ì·¨ê¸‰.\n practice 1 : partial derivative ...    \\( f(x,y) = 2x \u0026#43; 3xy \u0026#43; y^3 \\)  ë³€ìˆ˜ xì— ëŒ€í•´ í¸ë¯¸ë¶„  \\( = \\frac{\\partial f(x,y)}{\\partial x} = \\frac{\\partial(2x\u0026#43;3xy\u0026#43;y^3)}{\\partial x} \\)   \\( = \\frac{\\partial 2x}{\\partial x} \u0026#43; \\frac{\\partial 3xy}{\\partial x} \u0026#43; \\frac{\\partial y^3}{\\partial x} \\)   \\( = 2 \u0026#43; 3y \u0026#43; 0 \\)   \\( = 2 \u0026#43; 3y \\)   \\( f(x,y) = 2x \u0026#43; 3xy \u0026#43; y^3 \\)  ë³€ìˆ˜ yì— ëŒ€í•´ í¸ë¯¸ë¶„  \\( = \\frac{\\partial f(x,y)}{\\partial y} = \\frac{\\partial(2x\u0026#43;3xy\u0026#43;y^3)}{\\partial y} \\)   \\( = \\frac{\\partial 2x}{\\partial y} \u0026#43; \\frac{\\partial 3xy}{\\partial y} \u0026#43; \\frac{\\partial y^3}{\\partial y} \\)   \\( = 0 \u0026#43; 3x \u0026#43; 3y^2 \\)   \\( = 3x \u0026#43; 3y^2 \\)      ë³µí•© í•¨ìˆ˜ë¥¼ ìœ„í•´ Chain Ruleì„ í†µí•´ ë¯¸ë¶„. íŠ¹ì • í•¨ìˆ˜ë¥¼ ì¹˜í™˜í•˜ì—¬ ì•½ë¶„ ê°œë…ì„ ì ìš©. ë‘ í•¨ìˆ˜ ê³±ì˜ ë¯¸ë¶„ì€ ë‹¨ìˆœ ê³±ì´ ì•„ë‹ˆë©° Product Ruleì´ ì ìš©ë¨.\n  chain rule ...    \\( \\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial t} \\circ \\frac{\\partial t}{\\partial x} \\)       product rule ...    \\( \\frac{\\partial}{\\partial x}[f(x)g(x)]=f(x)g\u0026#39;(x)\u0026#43;f\u0026#39;(x)g(x) \\)       practice 1 : chain rule ...    \\( f(x) = e ^ {3x^2} \\space \\space \\space e^t , \\space t = 3x^2 \\)   \\( \\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial t} \\circ \\frac{\\partial t}{\\partial x} = \\frac{\\partial (e^t)}{\\partial t} \\circ \\frac{\\partial (3x^2)}{\\partial x} = (e^t)(6x) = (e^{3x^2})(6x) = 6xe^{3x2} \\)       practice 2 : chain rule ...    \\( f(x) = e^{-x} \\space \\space \\space e^t , \\space t = -x \\)   \\( \\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial t} \\circ \\frac{\\partial t}{\\partial x} = \\frac{\\partial (e^t)}{\\partial t} \\circ \\frac{\\partial (-x)}{\\partial x} = (e^t)(-1) = (e^{-x})(-1) = -e^{-x} \\)       practice 3 : chain/product rule ...    \\( f(x) = 3xe^x \\)   \\( \\frac{\\partial}{\\partial x}(3xe^x) = 3 \\Big( \\frac{\\partial}{\\partial x}(e^x x) \\Big) \\)  Product Ruleì„ ì ìš©  \\( \\frac{\\partial}{\\partial x}(u v) = v \\frac{\\partial u}{\\partial x} \u0026#43; u \\frac{\\partial v}{\\partial x} \\)  ë‹¤ìŒìœ¼ë¡œ ì¹˜í™˜  \\( u = e^x \\space v = x \\)   \\( 3 \\Big( x \\frac{\\partial}{\\partial x}(e^x) \u0026#43; e^x \\frac {\\partial}{\\partial x}(x) \\Big) \\)  Chain Ruleì„ ì ìš©  \\( \\frac{\\partial}{\\partial x}(e^x) = \\frac {\\partial e^u}{\\partial u} \\frac{\\partial u}{\\partial x} \\)  ë‹¤ìŒìœ¼ë¡œ ì¹˜í™˜  \\( u = x, \\space \\frac{\\partial}{\\partial u}(e^u)=e^u \\)   \\( = 3 \\Big( x e^x \\frac{\\partial}{\\partial x}(x) \u0026#43; e^x \\frac{\\partial}{\\partial x}(x) \\Big) \\)   \\( = 3 \\Big( x e^x 1 \u0026#43; e^x \\frac{\\partial}{\\partial x}(x) \\Big) \\)   \\( = 3 \\Big( x e^x \u0026#43; e^x 1 \\Big) \\)   \\( = 3 ( x e^x \u0026#43; e^x ) \\)   \\( = 3 e^x( x \u0026#43; 1 ) \\)      Loss Function #  ë‹¤ì‹œ ì„ í˜•íšŒê·€ì—ì„œ ì‹¤ì¸¡ë˜ëŠ” ë°ì´í„°ì™€ f(x) = wx+bê°„ errorëŠ” ìµœì†Œê°€ ë˜ì–´ì•¼í•¨. Errorê°€ ìµœì†Œê°€ ë˜ëŠ” W,Bë¥¼ ì—°ì†ì ìœ¼ë¡œ ì¸¡ì •í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ê²ƒì´ ì†ì‹¤í•¨ìˆ˜ì„.\n  loss function ...    \\( Loss Funtion = \\frac{(t_1-y_1)^2 \u0026#43; (t_2-y_2)^2 \u0026#43; ... \u0026#43; (t_n-y_n)^2}{n} \\)   \\( = \\frac{[t_1-(Wx_1\u0026#43;b)]^2 \u0026#43; [t_2-(Wx_2\u0026#43;b)]^2 \u0026#43; ... \u0026#43; [t_n-(Wx_n\u0026#43;b)]^2}{n} \\)   \\( = \\frac{1}{n} \\, \\displaystyle\\sum_{i=1}^n \\, [t_i-(Wx_i\u0026#43;b)]^2 \\)       ì› ê¸°ìš¸ê¸°ì—ì„œ í•™ìŠµë¥ *ë¯¸ë¶„ë§Œí¼ì„ ê°ì†Œì‹œí‚¤ë©° ê¸°ìš¸ê¸°ê°€ 0ì´ ë˜ëŠ” ìµœì í•´ë¡œ ì§„í–‰ë¨.\n gradient descent ...    \\( W := W - \\alpha \\frac{\\partial}{\\partial W} \\, \\frac{1}{2m} \\textstyle\\sum_{i=1}^m (W(x_i)-y_i)^2 \\)   \\( W := W - \\alpha \\frac{1}{2m} \\, \\textstyle\\sum_{i=1}^m 2(W(x_i)-y_i)X_i \\)   \\( W := W - \\alpha \\frac{1}{m} \\, \\textstyle\\sum_{i=1}^m (W(x_i)-y_i)X_i \\)   \\( W := W - \\alpha \\frac{\\partial}{\\partial W} \\, cost(W) \\)   \\( b := b - \\alpha \\frac{\\partial}{\\partial b} \\, cost(b) \\)      ì—¬ê¸°ì„œ í•™ìŠµë¥ ì€ ìµœì í•´ë¥¼ í–¥í•œ ì§„í–‰ í­ì„ ê²°ì •. ì‘ì„ ê²½ìš° ì„±ëŠ¥ì´ ì €í•˜. í´ ê²½ìš° í•™ìŠµì´ ì§„í–‰ë˜ì§€ ì•ŠìŒ.\n  learning rate ...    \\( W = W - \\alpha \\, \\frac{\\partial E(W,b)}{\\partial W} \\)       ì´ë¡ ì ìœ¼ë¡œ Convex Functionì´ë¼ë©´ ëŒ€ì²´ë¡œ ìµœì í•´ë¥¼ ì˜ ì°¾ì„ ìˆ˜ ìˆìœ¼ë‚˜ ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ Parameterì— ë”°ë¼ ê²°ê³¼ê°€ ìƒì´í•  ìˆ˜ ìˆìŒ.\n Dot Product #  ëª¨ë“  ìš”ì†Œë“¤ì˜ í–‰ë ¬ê³±ì„ í†µí•´ ê³„ì‚°.\n  dot product ...    \\( x_1 * W \u0026#43; b_1 = y_1 \\)   \\( x_2 * W \u0026#43; b_2 = y_2 \\)   \\( x_3 * W \u0026#43; b_3 = y_3 \\)   \\( x_4 * W \u0026#43; b_4 = y_4 \\)   \\( x_5 * W \u0026#43; b_5 = y_5 \\)  í–‰ë ¬ì˜ ê³±ìœ¼ë¡œ ì „í™˜  \\( \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{pmatrix} \\circ (W) \u0026#43; b = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ y_5 \\end{pmatrix} \\)   \\( X \\circ W \u0026#43; b = Y \\)       multi variable dot product ...    \\( \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{pmatrix} \\circ \\begin{pmatrix} w_1 \\\\ w_2 \\\\ w_3 \\end{pmatrix} \u0026#43; b = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ y_5 \\end{pmatrix} \\)   \\( X \\circ W \u0026#43; b = Y \\)      Logistic Regression #  Regressionì„ í†µí•´ ì²˜ë¦¬ëœ ê²°ê³¼ë¥¼ Classification ì²˜ë¦¬í•¨. Linear Regressionì˜ ê²°ê³¼ëŠ” ìˆ˜ì¹˜í˜• ê°’ì„ ê°€ì§€ê²Œ ë¨ìœ¼ë¡œ ë¶„ë¥˜ ë¬¸ì œì— ì·¨ì•½. Linearí•œ ê²°ê³¼ë¥¼ Logistic Regressionì„ í†µí•´ ì„ íƒì˜ ê²°ê³¼ë¡œ ëŒ€ì¹˜.\n ë¶„ë¥˜ì˜ ë¬¸ì œë¥¼ ìœ„í•´ ì§€ìˆ˜ í•¨ìˆ˜  \\( e^x \\)  ë¥¼  \\( e^{-x} \\)  ë¡œ ì „í™˜ í›„  \\( \\frac{1}{1 \u0026#43; e^{-x}} \\)  ì— ëŒ€ì…í•˜ì—¬ xì˜ ì¢Œì¸¡ìœ¼ë¡œ ì§„í–‰í• ìˆ˜ë¡œ  \\( \\infty \\)  ì— ê°€ê¹Œì›Œì§€ë©° 0ì— ìˆ˜ë ´í•˜ê²Œë˜ê³  xì˜ ìš°ì¸¡ìœ¼ë¡œ ì§„í–‰í•˜ê²Œ ë ìˆ˜ë¡ 0ì— ìˆ˜ë ´í•˜ë©´ì„œ 1ì— ê°€ê¹Œì›Œì§€ëŠ” ê²°ê³¼ê°€ ë¨.\n   sigmoid ...    \\( Z = Wx \u0026#43; b \\)   \\( y = \\frac {1}{1\u0026#43;e^{-(Wx\u0026#43;b)}} \\)   \\( y = sigmoid(Z) = \\sigma (z) = \\frac{1}{1\u0026#43;e^{-z}} \\)      Logistic Regressionì˜ Cost Functionì€ y = 1 ì¼ë•Œì™€ y = 0 ì¼ë•Œë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ”ë° y = 1 ì¼ë•ŒëŠ”  \\( \\log(h(x)) \\)  ì˜ ì—­  \\( -\\log(h(x)) \\)  ì„ ì·¨í•´ 0ìœ¼ë¡œ ê·¼ì ‘í•  ìˆ˜ë¡ ì˜¤ì°¨ê°€  \\( \\infty \\)  ì— ìˆ˜ë ´í•˜ê²Œ ë¨. ë°˜ëŒ€ë¡œ y = 0 ì¼ë•ŒëŠ”  \\( \\log(1 - h(x)) \\)  ì™€ ê°™ì´ 1ì—ì„œ ì°¨ê°í•˜ì—¬ 1ì— ê·¼ì ‘í•  ìˆ˜ë¡ ì˜¤ì°¨ê°€  \\( \\infty \\)  ì— ìˆ˜ë ´í•˜í•¨. ì´ ë‘ ì‹ì„ í•©í•˜ë©´ Logistic Regressionì˜ Convexí•œ Cost Funtionì´ ë˜ê²Œ ë¨.\n  Cross-Entropy ìœ ë„ì˜ ë˜ë‹¤ë¥¸ ì„¤ëª…ì€ ë‹¤ìŒê³¼ ê°™ìŒ.\ní•˜ë‚˜ì˜ ì…ë ¥ xì— ëŒ€í•´ ì¶œë ¥ì´ 1ì¼ í™•ë¥ ì„ yë¡œ ì •ì˜. yëŠ” 0 ë˜ëŠ” 1ì¼ì„ìœ¼ë¡œ y = sigmoid(Wx+b)ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŒ. ì…ë ¥ xì— ëŒ€í•´ ì¶œë ¥ì´ 0ì¼ í™•ë¥ ì€ 1ì´ ë‚˜íƒ€ë‚  í™•ë¥ ì˜ ë‚˜ë¨¸ì§€ì„ìœ¼ë¡œ 1-y ì„. í™•ë¥  ë³€ìˆ˜ CëŠ” 0 ë˜ëŠ” 1 ì´ì™¸ì—ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠìŒìœ¼ë¡œ ë² ë¥´ëˆ„ì´ ì‹œí–‰ì„ ì „ì œë¡œ í•˜ë©° ë² ë¥´ëˆ„ì´ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜(PMF)ëŠ” ì•„ë˜ì™€ ê°™ì´ ì •ì˜ë¨. ì´ë¥¼ Log ë³€í™˜ì„ í†µí•´ Convexì˜ í˜•íƒœì™€ ê·¹ì ì˜ ìœ„ì¹˜ë¥¼ ìœ ì§€í•˜ë©° ê³±ì„ ì„ í˜•ì˜ ì¡°í•© ê¼´ë¡œ í’€ ìˆ˜ ìˆë„ë¡ ë³€í™˜í•¨.\n bernoulli distribution (PMF) ...    \\( p(C=1|x) = y = sigmoid(Wx\u0026#43;b) \\)   \\( p(C=0|x) = 1 - p(C=1|x) = 1 - y \\)   \\( p(C=p|x) = y^p ( 1 - y )^{1-P} \\)   \\( Likelyhood (W,b) = \\displaystyle\\prod_{i=1}^n p(C=p_i|x_i) = \\displaystyle\\prod_{i=1}^n y_i^{p_i} (1-y_i)^{1-p_i} \\)   \\( E(W,b) = - \\log L(W,b) = -\\displaystyle\\sum_{i=1}^n \\lbrace t_i \\textstyle\\log y_i \u0026#43; (1-p_i) \\log(1-y_i) \\rbrace \\)       logistic regression cross entropy ...    \\( y = \\frac{1}{1\u0026#43;e^{-(Wx\u0026#43;b)}} , \\space p_i = 0 \\space or \\space 1 \\)   \\( E(W,b) = - \\, \\displaystyle\\sum_{i=1}^n \\, \\lbrace p_i \\, \\log y_i \u0026#43; (1-p_i) \\log (1-y_i) \\rbrace \\)      Bayes Theorem #  Logit, Sigmoid, Softmaxë¥¼ ìœ ë„í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ í™•ë¥ ì˜ ê°œë…ì´ ì„ í–‰ë˜ì–´ì•¼í•¨.\n ë² ì´ì§€ì•ˆ ì˜ˆì œ ëª¬í‹°í™€ë¬¸ì œ ë² ì´ì¦ˆì˜ ì •ë¦¬  ë¶ˆí™•ì‹¤ì„± í•˜ì˜ ì˜ì‚¬ ê²°ì •ì˜ ë¬¸ì œë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ë‹¤ë£°ë•Œ ì‚¬ìš©ë˜ëŠ” ë² ì´ì¦ˆì˜ ì •ë¦¬(Bayes Theorem)ì™€ ì „í™•ë¥  ë²•ì¹™(law of total probability)ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ.\n bayes theorem ...    \\( P(Y|X) = \\frac {P(X \\bigcap Y)}{P(X)} \\)   \\( P(X|Y) = \\frac {P(Y \\bigcap X)}{P(Y)} \\)   \\( P(Y \\bigcap X) = P(X \\bigcap Y) = P(X|Y)P(Y) = P(Y|X)P(X) \\)   \\( \\therefore P(Y|X) = \\frac {P(X|Y)P(Y)}{P(X)} \\)  P(Y|X) : ì‚¬í›„í™•ë¥ (Posterio probability)\nP(X|Y) : ê°€ëŠ¥ë„(likelihood)\nP(Y) : í™•ë¥ ë³€ìˆ˜ Yì˜ ì‚¬ì „í™•ë¥ (prior probability)\nP(X) : í™•ë¥ ë³€ìˆ˜ Xì˜ ì‚¬ì „í™•ë¥ (prior probability)\n    í‘œë³¸ ê³µê°„ Së¥¼ nê°œë¡œ ë‚˜ëˆ„ì—ˆì„ë•Œ ì‚¬ê±´ Aì˜ í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚˜ë©° ì´í•©ì€ 1ì´ë¨.\n law of total probability ...    \\( P(A) = P(A \\bigcap B_1) \u0026#43; P(A \\bigcap B_2) \u0026#43; ... \u0026#43; P(A \\bigcap B_n) \\)      ì¼ë°˜ì ìœ¼ë¡œ  \\( A_1,A_2,A_3 \\)  ê°€ ì„œë¡œ Mutually Exclusiveì´ê³  ì´ë“¤ì˜ í•©ì§‘í•©ì´ í‘œë³¸ê³µê°„ê³¼ Sì™€ ê°™ìœ¼ë©´ ì‚¬ê±´  \\( A_1,A_2,A_3 \\)  ëŠ” í‘œë³¸ê³µê°„ Sì˜ ë¶„í• ì´ë¼ê³  ì •ì˜. íŠ¹ì • ì‚¬ê±´ Bê°€ ë‚˜íƒ€ë‚  í™•ë¥ ì€ ì „í™•ë¥  ê³µì‹ì— ì˜ê±° ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŒ.\n b : law of total probability ...    \\( P(B)) = P(A_1)P(B|A_1)\u0026#43; P(A_2)P(B|A_2) \u0026#43; P(A_3)P(B|A_3) = \\displaystyle\\sum_{i=1}^3 P(A_i)P(B|A_i) \\)       \\(P(A_1)\\)  , \\(P(A_2)\\)  , \\(P(A_3)\\)  ì€ ë¯¸ë¦¬ ì•Œê³  ìˆë‹¤ëŠ” ì˜ë¯¸ë¡œ ì‚¬ì „í™•ë¥ (Prior Probability)ë¡œ ë¶ˆë¦¬ê³ ,  \\(P(B|A_1)\\)  , \\(P(B|A_2)\\)  , \\(P(B|A_3)\\)  ëŠ” ìš°ë„(Likelihood Probability)ë¼ ë¶€ë¦„.\n \\( P(A_1|B) \\)  ëŠ” ì‚¬ê±´ Bë¥¼ ê´€ì¸¡í•œ í›„ ì›ì¸ì´ ë˜ëŠ” ì‚¬ê±´ Aì˜ í™•ë¥ ì„ ë”°ì¡Œë‹¤ëŠ” ì˜ë¯¸ì—ì„œ ì‚¬í›„í™•ë¥ (Posterior Probability)ë¡œ ì •ì˜ë˜ë©° ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŒ.\n posterior probability ...    \\( P(A_1|B)) = \\frac {P(A_1)P(B|A_1)}{P(B)} = \\frac {P(A_1)P(B|A_1)}{P(A_1)P(B|A_1)\u0026#43;P(A_2)P(B|A_2)\u0026#43;P(A_3)P(B|A_3)} \\)       practice 1 : ì§„ë‹¨ë¬¸ì œ ...   ì „ì²´ ì¸êµ¬ì˜ 1%ê°€ ì–´ë–¤ ë³‘ì— ê±¸ë ¸ë‹¤ê³  ê°€ì •. ì´ ë³‘ì˜ ì§„ë‹¨ ì •í™•ë„ê°€ 97% ì˜¤ì§„ë¥ ì€ 6%ë¼ê³  ê°€ì •.\n \\( P(D) = 0.01 \\)  : ì‚¬ì „í™•ë¥   \\( P( \\backsim D) = 0.99 \\)  : ì‚¬ì „í™•ë¥   \\( P(\u0026#43;|D) = 0.97 \\)  : ìš°ë„  \\( P(\u0026#43;| \\backsim D) = 0.06 \\)  : ìš°ë„  \\( P(D|\u0026#43;) \\)  = ????? : ì‚¬í›„í™•ë¥  (ì§„ë‹¨í…ŒìŠ¤íŠ¸ ì–‘ì„±ì¼ë•Œ ì‹¤ì œ í™˜ìì¼ í™•ë¥  )  \\( P(\u0026#43;) = P(D \\cap \u0026#43;)\u0026#43;P( \\backsim D \\cap \u0026#43;) \\)   \\( = P(D)P(\u0026#43;|D)\u0026#43;P( \\backsim D)P(\u0026#43;| \\backsim D) \\)   \\( = 0.01 \\times 0.97 \u0026#43; 0.99 \\times 0.06 \\)   \\( = 0.691 \\)   \\( P(D|\u0026#43;) = \\frac {P(D)P(\u0026#43;|D)}{P(\u0026#43;)} \\)   \\( = \\frac {0.01 \\times 0.97}{0.691} \\)   \\( = 0.014 \\)       practice 2 : ë”¸ê¸°ë§› ë¬¸ì œ ...   ì‚¬íƒ•ì£¼ë¨¸ë‹ˆ 1 : ë”¸ê¸°ë§› 30ê°œ, í¬ë„ë§› 10 ì‚¬íƒ•ì£¼ë¨¸ë‹ˆ 2 : ë”¸ê¸°ë§› 20ê°œ, í¬ë„ë§› 20 ë‘ ì£¼ë¨¸ë‹ˆì—ì„œ ì„ì˜ë¡œ ê³¨ëì„ë•Œ ë”¸ê¸°ì˜€ë‹¤ë©´ ì£¼ë¨¸ë‹ˆ1ì—ì„œ ë‚˜ì™”ì„ í™•ë¥ ì€?  \\( P(H) \\)  : ì‚¬ì „í™•ë¥  - ì£¼ë¨¸ë‹ˆ1ì„ ê³ ë¥¼ í™•ë¥   \\( P(D|H) \\)  : ìš°ë„ - ì£¼ë¨¸ë‹ˆ1ì˜ ë”¸ê¸° í™•ë¥   \\( P(D) \\)  : í•œì •ìƒìˆ˜ - ë”¸ê¸°ë¥¼ ê³ ë¥¼ í™•ë¥   \\( P(H|D)) \\)  : ì‚¬í›„í™•ë¥  - ì£¼ë¨¸ë‹ˆ1ì—ì„œ ë”¸ê¸°ë¥¼ ê³¨ëì„ í™•ë¥   \\( P(H|D)) = \\frac {P(H)P(D|H)}{P(D)} = \\frac { \\frac {1}{2} * \\frac {3}{4} }{ \\frac {5}{8} } = \\frac {3}{5} \\)  ì´ë•Œ í•œì •ìƒìˆ˜  \\( P(D) \\)  ëŠ” ìƒí˜¸ë°°ì œ(Mutually Exclusive)ì™€ ì „ì²´í¬ê´„(Collectively Exhaustive)ì˜ ì›ì¹™ì—ì„œ ê°™ì€ ê°’ì„ ì§€ë‹ˆë©° ê³„ì‚°ì´ ìƒëµë¨\n   Point Theory1 Theory2     ì‚¬ì „í™•ë¥  P(H) 1/2 1/2   ìš°ë„P(D H) 3/4   ì‚¬ì „í™•ë¥ *ìš°ë„ 3/8 1/4   í•œì •ìƒìˆ˜P(D) 5/8 5/8   ì‚¬í›„í™•ë¥ P(H D) 3/5     \\( P(H|D) = \\frac {P(H)P(D|H)}{P(D)} = \\frac { \\frac {1}{2} * \\frac {3}{4} }{ \\frac {5}{8} } = \\frac {3}{5} \\)       practice 3 : ì¹´ë“œë¬¸ì œ ...   ì¹´ë“œA : ì•ë’¤ ëª¨ë‘ ê²€ì€ìƒ‰\nì¹´ë“œB : ì•ì€ ê²€ì€ìƒ‰, ë’¤ëŠ” í•˜ì–€ìƒ‰\nì¹´ë“œC : ì•ë’¤ ëª¨ë‘ í•˜ì–€ìƒ‰\nì„¸ì¹´ë“œ ì¤‘ í•˜ë‚˜ë¥¼ ë½‘ì•„ì„œ ë³¸ë©´ì´ ê²€ì€ìƒ‰ì¼ ë•Œ ê·¸ ë’·ë©´ë„ ê²€ì€ìƒ‰ì´ ë‚˜ì˜¬ í™•ë¥ \n \\( P(A|Black) = \\frac {P(A)P(Black|A)}{P(Black)} \\)   \\( = \\frac {P(A)P(Black|A)}{P(A)P(Black|A)\u0026#43;P(B)P(Black|B)} \\)   \\( = \\frac { \\frac {1}{3} * 1 }{ \\frac {1}{3} * 1 \u0026#43; \\frac{1}{3} * \\frac{1}{2} } = \\frac {2}{3} \\)       practice 4 : ëª¬í‹°í™€ë¬¸ì œ ...   ë¬¸A1 : ìƒê¸ˆ\në¬¸A2 : ê½\në¬¸A3 : ê½\nì¶œì—°ìê°€ ë¬¸ì„ í•˜ë‚˜ ì„ íƒí•œ í›„\nì‚¬íšŒìê°€ ë‹¤ë¥¸ ë¬¸ì„ ì—´ì–´ ë¬¸ì„ í•˜ë‚˜ ë³´ì—¬ì¤€ í›„\nì¶œì—°ìê°€ ì„ íƒì„ ë°”ê¾¸ì§€ ì•Šê³  ë¬¸ì„ ì—´ë•Œì˜ ë‹¹ì²¨ í™•ë¥ \n \\( P(A1|B) = \\frac {P(A1 \\cap B))}{P(B)} = \\frac {P(A1 \\cap B)}{P(A1 \\cap B) \u0026#43; P(A2 \\cap B) \u0026#43; P(A3 \\cap B)} = \\frac { P(A1)P(B|A1) }{ P(A1)P(B|A1) \u0026#43; P(A2)P(A2|B) \u0026#43; P(A3)P(A3|B) } = \\frac {\\frac {2}{3} * \\frac {1}{2}}{\\frac {1}{3}* \\frac {1}{2} \u0026#43; \\frac {1}{3}*0 \\frac {1}{3}*1} = \\frac {1}{3} \\)      Logit / Sigmoid / Softmax #  Logit /Sigmoid /Softmaxì˜ ê´€ê³„ë¥¼ ì •ë¦¬\n ì°¸ê³ 1 gwkoo.log - Logit,Sigmoid,Softmaxì˜ ê´€ê³„ ì°¸ê³ 2 í•œ í˜ì´ì§€ ë¨¸ì‹ ëŸ¬ë‹ Logit,Sigmoid,Softmaxì˜ ê´€ê³„ ì°¸ê³ 3 ChaCha\u0026rsquo;s blog - Sigmoid, Logit and Softmax ì°¸ê³ 4 ratsgo\u0026rsquo;s blog - ë² ì´ì¦ˆ ê·œì¹™ê³¼ ë‹¤ì–‘í•œ ë¬¸ì œë“¤  Logitì€ Log Oddsë¥¼ ì˜ë¯¸.\nOddsë€ ë„ë°•ì—ì„œ ì–»ì„ í™•ë¥ ê³¼ ìƒì„ í™•ë¥ ì˜ ë¹„ìœ¨ì„ ì˜ë¯¸\n odds / logit ...    \\( Classes : C_1, C_2 \\)   \\( Probability of C_1 given X : y = P(C_1|X) \\)   \\( Probability of C_2 given X : 1-y = P(C_2|X) \\)  Odds =  \\( \\frac {y}{1-y} = \\frac {P(C_1|X)}{1-P(C_1|X)} \\)  Choose =  \\( \\begin{cases} C_1 \\space if \\space \\frac {y}{1-y} \u0026gt; 1 \\\\ C_2 \\space if \\space \\frac {y}{1-y} \u0026lt; 1 \\end{cases} \\)      ì´ë¥¼ Logë¥¼ ì´ìš©í•´ 0~1ì˜ ë²”ìœ„ë¡œ í•œì •ëœ Logitì„ ë„ì¶œ\n logits ...    \\( = log \\Big( \\frac{y}{1-y} \\Big) \\)   \\( = log \\Big( \\frac{P(C_1|X)}{1-P(C_1|X)} \\Big) \\)   \\( z = log( \\frac {y}{1-y} \\)  ,  \\( e^z = ( \\frac {y}{1-y} ) \\)   \\( e^z = ( \\frac {y}{1-y} ) \\)   \\( \\frac {1}{e^z} = \\frac {1-y}{y} \\)  =  \\( \\frac {1}{e^z} = \\frac {1}{y} -1 \\)   \\( \\frac {1}{e^z} \u0026#43; 1 = \\frac {1}{y} \\)  =  \\( \\frac {1 \u0026#43; e^z}{e^z} = \\frac {1}{y} \\)   \\( \\frac {e^z}{1 \u0026#43; e^z} = y \\)   \\( \\frac {e^z / e^z}{(1 \u0026#43; e^z)/e^z} = y \\)   \\( \\frac {1}{ \\frac{1}{e^z} \u0026#43; 1 } = y \\)   \\( \\frac {1}{ e^{-z} \u0026#43; 1 } = y \\)   \\( \\therefore y = \\frac {1}{ 1 \u0026#43; e^{-z}} \\)      ì—¬ê¸°ì„œ SoftmaxëŠ” Sigmoidë¥¼ Kê°œ ì´ìƒìœ¼ë¡œ ì¼ë°˜í™” í•˜ì—¬ ìœ ë„.\n softmax ...    \\( \\frac {P(C_1|X)}{P(C_2|X)} = e^z \\)   \\( \\frac {P(C_i|X)}{P(C_k|X)} = e^{z_i} \\)   \\( \\displaystyle\\sum_{i=1}^{k-1} \\frac { P(C_i|X)}{P(C_k|X)} = \\displaystyle\\sum_{i=1}^{k-1} e^{z_i} \\)   \\( \\frac {P(C_1|X)\u0026#43;P(C_2|X)\u0026#43;...\u0026#43;P(C_{k-1}|X)}{P(C_k|X)} = \\displaystyle\\sum_{i=1}^{k-1} e^{z_i} \\)   \\( \\frac {1-P(C_k|X)}{P(C_k|X)} = \\displaystyle\\sum_{i=1}^{k-1} e^{z_i} \\)   \\( P(C_k|X) = \\frac{1}{1\u0026#43; \\textstyle\\sum_{i=1}^{k-1} e^{z_i} } \\)   \\( P(C_i|X) = e^{z_i} P(C_k|X) \\)   \\( P(C_i|X) = \\frac {e^{z_i}}{1 \u0026#43; \\textstyle\\sum_{i=1}^{k-1} e^{z_i}} \\)   \\( P(C_i|X) = \\frac {e^{z_i}}{e^{z_i} \u0026#43; \\textstyle\\sum_{i=1}^{k-1} e^{z_i}} = \\frac {e^{z_i}}{ \\textstyle\\sum_{i=1}^{k} e^{z_i}} \\)      Learning Rate #  Learning Rateì™€ í•™ìŠµì˜ ìƒê´€ê´€ê³„ëŠ” ì•„ë˜ì™€ ê°™ìŒ. Learning rateê°€ ë„ˆë¬´ ë†’ì„ ê²½ìš° Overshootingì´ ë°œìƒë˜ë©° ë„ˆë¬´ ì‘ì„ ê²½ìš° í•™ìŠµì´ ë§¤ìš° ë”ë””ê²Œ ì§„í–‰ë¨.\n í•™ìŠµì‹œì ì— ë”°ë¼ Learning rateë¥¼ ì¡°ì •í•˜ëŠ” decayë°©ì‹ì´ ìˆìŒ.\n Step deacy : N epoch or validation loss Exponential decay :  \\( \\alpha = \\apha 0 \\epsilon - kt \\)   1/t decay :  \\( \\alpha = \\apha 0 (1\u0026#43;kt) \\)   inverse time decay natural exponential decay piecewise constant polynomical decay  Data Preprocessing #  Dataì˜ ë¶„í¬ê°€ í¸ì¤‘ë˜ì–´ ìˆì„ ê²½ìš° í‘œì¤€í™”(Standardization)ì™€ ì •ê·œí™”(Normalization)ì„ í†µí•´ì„œ ì „ì²˜ë¦¬.\nFeature Scaling Standardization :  \\( x_{new} = \\frac {x - \\mu}{\\sigma} \\)  Normalization :  \\( x_{new} = \\frac {x - x_{min}}{x_{max} - - x_{min}} \\)  Noisy Data í•™ìŠµì— í•„ìš”í•œ Dataë¡œ ì •ì œí•˜ëŠ” ì „ì²˜ë¦¬ ê³¼ì •.\nOverfitting #  Underfitting : High bias ìƒíƒœ. í•™ìŠµì´ ëœëœ ìƒíƒœ. Overfitting : High variance. í•™ìŠµì´ ë„ˆë¬´ë˜ì–´ ì£¼ì–´ì§„ dataì—ë§Œ ë§ì¶°ì§„ ìƒíƒœ.\n Solution\n Set features : Get more training data Set features : Smaller set of features (PCA) Set features : Add additional features (for underfitting) Regularization (Add term to loss)  Linear regression with regularization  softmax ...    \\( h_{\\theta}(x) = \\theta_0 \u0026#43; \\theta_1 x \u0026#43; \\theta_2 x^2 \u0026#43; \\theta_3 x^3 \u0026#43; \\theta_4 x^4 \\)   \\( J(\\theta) = \\frac{1}{2m}\\displaystyle\\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^2 \u0026#43; \\frac{ \\lambda }{2m}\\displaystyle\\sum_{j=1}^m \\theta_{j}^2 \\)   \\( \\lambda-- \\)  : fixed high bias (under fitting)  \\( \\lambda\u0026#43;\u0026#43; \\)  : fixed high variance (over fitting)     XOR Prolem #  1958ë…„ Frank Rosenblattì— ì˜ì— ì œì•ˆëœ ì‹ ê²½ë§ ì‹œìŠ¤í…œì˜ ëª¨ë¸ë¡œ McGullock, Pitts, Hebbì˜ ì—°êµ¬ë¥¼ ê¸°ì´ˆë¡œ í•˜ê³  ìˆìŒ.\n í•˜ì§€ë§Œ XORì˜ ë¬¸ì œëŠ” Linear Regressionìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ì—†ë‹¤ëŠ” ìˆ˜í•™ì  ê²°ë¡ ì„ ë‚´ë¦¼. (Perceptrons by Marvin Minsky)\n AND    X1 X2 T     0 0 0   1 0 0   0 1 0   1 1 1    OR    X1 X2 T     0 0 0   1 0 1   0 1 1   1 1 1    NAND    X1 X2 T     0 0 1   1 0 1   0 1 1   1 1 0    NOR    X1 X2 T     0 0 1   1 0 0   0 1 0   1 1 0     Flow  mermaid.initialize({ \"flowchart\": { \"useMaxWidth\":true }, \"theme\": \"default\" } ) graph LR A(NAND)--C(AND)--D(XOR) B(_OR_)--C style A fill:#ffffff,stroke:#000000,stroke-width:1px style B fill:#ffffff,stroke:#000000,stroke-width:1px style C fill:#ffffff,stroke:#000000,stroke-width:1px style D fill:#ffffff,stroke:#000000,stroke-width:1pxXOR NOR\n   X1 X2 NAND OR AND     0 0 1 0 0   1 0 1 1 1   0 1 1 1 1   1 1 0 1 0     Flow graph LR E(NOR)--F(AND)--G(XOR) H(AND)--F style E fill:#ffffff,stroke:#000000,stroke-width:1px style F fill:#ffffff,stroke:#000000,stroke-width:1px style G fill:#ffffff,stroke:#000000,stroke-width:1px style H fill:#ffffff,stroke:#000000,stroke-width:1pxXOR NOR\n   X1 X2 AND NOR AND     0 0 0 1 0   1 0 0 0 1   0 1 0 0 1   1 1 1 0 0     Backpropagation #  1974, 1982 Paul Werbos, 1986 Hintonì— ì˜í•´ ì •ë¦½ëœ ê°œë…. ìˆœì „íŒŒì—ì„œ ëª©í‘œê°’ê³¼ ëª¨ë¸ì˜ ê³„ì‚°ê°’ì˜ ì˜¤ì°¨ë¥¼ êµ¬í•œí›„ ê·¸ ì˜¤ì°¨ê°’ì„ ì—­ë°©í–¥ìœ¼ë¡œ ì „íŒŒí•˜ë©° ë…¸ë“œë“¤ì˜ ë³€ìˆ˜ë“¤ì„ ê°±ì‹ í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜. ë‹¤ë§Œ ë³µì¡í•œ Neural Networks í™˜ê²½ì—ì„œëŠ” ì—­ì „íŒŒë˜ëŠ” ê°’ë“¤ì´ ì†Œì‹¤ë˜ëŠ” Vanishing Gradient í˜„ìƒì´ ë°œìƒë¨. 2006ë…„ 2007ë…„ Hintonê³¼ Bengioì— ì˜í•´ ì´ˆê¸° Parameterì— ì˜í•œ ì˜í–¥ì´ ì£¼ëª©ë˜ë©° Deep Learningì´ë¼ëŠ” ìš©ì–´ê°€ ë“±ì¥í•¨.\nGeoffrey Hinton\u0026rsquo;s summary\n Our labeled datasets were thousands of times too small. Our computers were millions of times too slow. We initialized the wieghts in a stupid way. We used the wrong type of non-linearity.  Node Notation\n  node notation ...    \\( W_{2,1}^{(2)} \\)  : ê³„ì¸µ2ì— ì ìš©ë˜ëŠ” ê°€ì¤‘ì¹˜ë¡œ 1ê³„ì¸µ ë…¸ë“œ 1ì—ì„œ 2ê³„ì¸µ ë…¸ë“œ 2ë¡œ ì „ë‹¬ë¨ì„ ì˜ë¯¸  \\( B_{1}^{(2)} \\)  : ê³„ì¸µ2ì˜ ë…¸ë“œ1ì— ì ìš© Biasë¥¼ ì˜ë¯¸  \\( Z_{2}^{(2)} \\)  : ê³„ì¸µ2ì˜ ë…¸ë“œ2ì˜ ê³„ì‚°ê°’ì„ ì˜ë¯¸ (  \\( Z_{2}^{(2)} = X_1 W_{2,1}^{(2)} \u0026#43; X_2 W_{2,2}^{(2)} \u0026#43; b_2^{(2)} \\)  )  \\( a_{2}^{(2)} \\)  : ê³„ì¸µ2ì˜ ë…¸ë“œ2ì˜ ì¶œë ¥ê°’ì„ ì˜ë¯¸ (  \\( sigmoid(Z_2^{(2)}) \\)  )     Backpropagation\n Chain Ruleì„ ì´ìš©í•œ êµ­ì†Œ ë¯¸ë¶„ëœ ì˜¤ì°¨ ì—­ì „íŒŒ\n derivative of backpropagation ...    \\( W^{(2)} = W^{(2)} - \\alpha \\frac {\\partial E}{\\partial W^{(2)}} = W^{(2)} - \\alpha ( \\frac {\\partial E}{\\partial A^{(3)}} \\cdotp \\frac {\\partial A^{(3)}}{\\partial A^{(2)}} \\cdotp \\frac {\\partial A^{(2)}}{\\partial W^{(2)}} ) \\)      Sigmoid ë¯¸ë¶„\n derivative of sigmoid ...    \\( \\frac {\\partial sigmoid(z)}{\\partial z} = \\frac {\\partial }{\\partial z} (\\frac {1}{1\u0026#43; e^{-z}} ) \\)  Chian rule ì ìš© :  \\( \\frac {\\partial f}{\\partial u} \\cdotp \\frac {\\partial u}{\\partial x} \\)  ì´ë•Œ  \\( u = e^{-x}\u0026#43;1 \\)  ì´ë©´  \\( f = \\frac {1}{u} \\)  ì´ê³   \\( \\therefore \\frac {\\partial}{\\partial u} (\\frac {1}{u}) \\cdotp \\frac {\\partial u}{\\partial x} \\)   \\( = - \\frac {1}{(1\u0026#43;e^{-x})^2} \\cdotp \\frac {\\partial (1\u0026#43;e^{-x})}{\\partial x} \\)   \\( = - \\frac {1}{(1\u0026#43;e^{-x})^2} \\cdotp (\\frac {\\partial}{\\partial x} (1) \u0026#43; \\frac {\\partial}{\\partial x} (e^{-x})) \\)  Chian rule ì ìš© :  \\( u = -x \\)  ì´ë©´  \\( \\frac {\\partial}{\\partial x}(e^{-x}) = \\frac {\\partial e^u}{\\partial u} \\cdotp \\frac {\\partial u}{\\partial x} \\)   \\( \\frac {\\partial}{\\partial u}(e^u) = e^u \\)   \\( \\therefore \\frac {\\partial}{\\partial x}(e^{-x}) = e^{-x} \\)   \\( \\frac {\\partial}{\\partial x}(-x) = -1 \\)   \\( = - \\frac {1}{(1\u0026#43;e^{-x})^2} \\cdotp (\\frac {\\partial}{\\partial x} (1) \u0026#43; \\frac {\\partial}{\\partial x} (e^{-x})) = - \\frac {1}{(1\u0026#43;e^{-x})^2} \\cdotp (0 \u0026#43; -1 e^{-x}) \\)   \\( = \\frac {e^{-x}}{(1\u0026#43;e^{-x})^2} \\)   \\( = \\frac {1\u0026#43;e^{-x}-1}{(1\u0026#43;e^{-x})^2} \\)   \\( = \\frac {1\u0026#43;e^{-x}}{(1\u0026#43;e^{-x})^2} - \\frac {1}{(1\u0026#43;e^{-x})^2} \\)   \\( = \\frac {1}{(1\u0026#43;e^{-x})} - \\frac {1}{(1\u0026#43;e^{-x})^2} \\)   \\( = \\frac {1}{(1\u0026#43;e^{-x})} (1 - \\frac {1}{(1\u0026#43;e^{-x})}) \\)   \\( = sigmoid(x)(1-sigmoid(x)) \\)       using derivative of sigmoid ...    \\( \\frac {\\partial E_{(1)}}{\\partial w_{(1,1)}^{(3)}} = \\frac {\\partial E_{(1)}}{\\partial a_{(1)}^{(3)}} * \\frac {\\partial a_{(1)}^{(3)}}{\\partial z_{(1)}^{(3)}} * \\frac {\\partial z_{(1)}^{(3)}}{\\partial w_{(1,1)}^{(3)}} \\)   \\( = \\frac {\\partial E_{(1)}}{\\partial a_{(1)}^{(3)}} * \\frac {\\partial sigmoid(z_{(1)}^{(3)}}{\\partial z_{(1)}^{(3)}} * \\frac {\\partial z_{(1)}^{(3)}}{\\partial w_{(1,1)}^{(3)}} \\)   \\( = \\frac {\\partial E_{(1)}}{\\partial a_{(1)}^{(3)}} * sigmoid(z_{(1)}^{(3)}) * (1-sigmoid(z_{(1)}^{(3)})) * \\frac {\\partial z_{(1)}^{(3)}}{\\partial w_{(1,1)}^{(3)}} \\)   \\( = \\frac {\\partial E_{(1)}}{\\partial a_{(1)}^{(3)}} * a_{(1)}^{(3)} * (1-a_{(1)}^{(3)}) * \\frac {\\partial z_{(1)}^{(3)}}{\\partial w_{(1,1)}^{(3)}} \\)       preparation of backpropagation ...   ì…ë ¥ì¸µ ì„ í˜•íšŒê·€ê°’ (z) * ì—†ìŒ ì…ë ¥ì¸µ ì¶œë ¥ê°’ (a)\n   \\( a_{1}^{(1)} = x_{1} \\)       \\( a_{2}^{(1)} = x_{2} \\)    ì€ë‹‰ì¸µ ì„ í˜•íšŒê·€ê°’ (z)\n   \\( z_{1}^{(2)} = a_{1}^{(1)} w_{1,1}^{(2)} \u0026#43; a_{2}^{(1)} w_{1,2}^{(2)} \u0026#43; b_{1}^{(2)} \\)       \\( z_{2}^{(2)} = a_{1}^{(1)} w_{2,1}^{(2)} \u0026#43; a_{2}^{(1)} w_{2,2}^{(2)} \u0026#43; b_{2}^{(2)} \\)    ì€ë‹‰ì¸µ ì¶œë ¥ê°’ (a)\n   \\( a_{1}^{(2)} = sigmoidz(z_{1}{(2)} \\)       \\( a_{2}^{(2)} = sigmoidz(z_{2}{(2)} \\)    ì¶œë ¥ì¸µ ì„ í˜•íšŒê·€ê°’ (z)\n   \\( z_{1}^{(3)} = a_{1}^{(2)} w_{1,1}^{(3)} \u0026#43; a_{2}^{(2)} w_{1,2}^{(3)} \u0026#43; b_{1}^{(3)} \\)       \\( z_{2}^{(3)} = a_{1}^{(2)} w_{2,1}^{(3)} \u0026#43; a_{2}^{(2)} w_{2,2}^{(3)} \u0026#43; b_{2}^{(3)} \\)    ì¶œë ¥ì¸µ ì¶œë ¥ê°’ (a)\n   \\( a_{1}^{(3)} = sigmoidz(z_{1}{(3)} \\)       \\( a_{2}^{(3)} = sigmoidz(z_{2}{(3)} \\)       \\( E = \\frac {1}{n} \\displaystyle\\sum_{i=1}^{n}(t_{i}^{(3)} - a_{i}^{(3)})^2 = \\frac {1}{2} ( (t_{1}^{(3)} - a_{1}^{(3)})^2 \u0026#43; (t_{2}^{(3)} - a_{2}^{(3)})^2 ) \\)       \\( E_1 \u0026#43; E_2 = \\frac {1}{2} (t_{1}^{(3)}-a_{1}^{(3)})^2 \u0026#43; \\frac {1}{2} (t_{2}^{(3)}-a_{2}^{(3)})^2 \\)       \\( W^{(2)} = \\begin{pmatrix} w_{(1,1)}^{(2)} \u0026amp; w_{(2,1)}^{(2)} \\\\ w_{(1,2)}^{(2)} \u0026amp; w_{(2,2)}^{(2)} \\end{pmatrix} , W^{(3)} = \\begin{pmatrix} w_{(1,1)}^{(3)} \u0026amp; w_{(2,1)}^{(3)} \\\\ w_{(1,2)}^{(3)} \u0026amp; w_{(2,2)}^{(3)} \\end{pmatrix} \\)       \\( W^{(2)} = W^{(2)} - \\alpha \\frac {\\partial E}{\\partial W^{(2)}} \\to \\frac {\\partial E}{\\partial W^{(2)}} = \\frac {\\partial E}{\\partial W_{(1,1)}^{(2)}} , \\frac {\\partial E}{\\partial W_{(2,1)}^{(2)}} , \\frac {\\partial E}{\\partial W_{(1,2)}^{(2)}} , \\frac {\\partial E}{\\partial W_{(2,2)}^{(2)}} \\)       \\( W^{(3)} = W^{(3)} - \\alpha \\frac {\\partial E}{\\partial W^{(3)}} \\to \\frac {\\partial E}{\\partial W^{(3)}} = \\frac {\\partial E}{\\partial W_{(1,1)}^{(3)}} , \\frac {\\partial E}{\\partial W_{(2,1)}^{(2)}} , \\frac {\\partial E}{\\partial W_{(1,2)}^{(3)}} , \\frac {\\partial E}{\\partial W_{(2,2)}^{(3)}} \\)       \\( b^{(2)} = \\begin{pmatrix} b_{(1)}^{(2)} \u0026amp; b_{(2)}^{(2)} \\end{pmatrix} , b^{(3)} = \\begin{pmatrix} b_{(1)}^{(3)} \u0026amp; b_{(2)}^{(3)} \\end{pmatrix} \\)       \\( b^{(2)} = b^{(2)} - \\alpha \\frac {\\partial E}{\\partial b^{(2)}} \\to \\frac {\\partial E}{\\partial b^{(2)}} = \\frac {\\partial E}{\\partial b_{(1)}^{(2)}} , \\frac {\\partial E}{\\partial b_{(2)}^{(2)}} \\)       \\( b^{(3)} = b^{(3)} - \\alpha \\frac {\\partial E}{\\partial b^{(3)}} \\to \\frac {\\partial E}{\\partial b^{(3)}} = \\frac {\\partial E}{\\partial b_{(1)}^{(3)}} , \\frac {\\partial E}{\\partial b_{(2)}^{(3)}} \\)         output layer 1 ...    \\( \\frac {\\partial E} {\\partial W_{(1,1)}^{(3)}} = \\frac {\\partial E_{(1)}} {\\partial W_{(1,1)}^{(3)}} \u0026#43; \\frac {\\partial E_{(2)}} {\\partial W_{(1,1)}^{(3)}} = \\frac {\\partial E_{(1)}} {\\partial a_{(1)}^{(3)}} * \\frac {\\partial a_{(1)}^{(3)}}{\\partial z_{(1)}^{(3)}} * \\frac {\\partial z_{(1)}^{(3)}}{\\partial w_{(1,1)}^{(3)}} \\)   \\( = \\frac {\\partial \\begin{Bmatrix} \\frac{1}{2} (t_{(1)}^{(3)} - a_{(1)}^{(3)})^2 ) \\end{Bmatrix} } {\\partial a_{(1)}^{(3)}} * \\frac { \\partial sigmoid (z_{(1)}^{(3)}) } { \\partial z_{(1)}^{(3)} } * \\frac { \\partial ( a_{(1)}^{(2)} w_{(1,1)}^{(3)} \u0026#43; a_{(2)}^{(2)} w_{(1,2)}^{(3)} \u0026#43; b_{(1)}^{(3)} ) } { \\partial w_{(1,1)}^{(3)} } \\)  1)  \\( \\frac {\\partial \\begin{Bmatrix} \\frac{1}{2} (t_{(1)}^{(3)} - a_{(1)}^{(3)})^2 ) \\end{Bmatrix} } {\\partial a_{(1)}^{(3)}} = \\frac{1}{2} \\cdotp \\frac {\\partial (t_{(1)}^{(3)})^2 \u0026#43; (a_{(1)}^{(3)})^2 -2 t_{(1)}^{(3)} a_{(1)}^{(3)} } { \\partial a_{(1)}^{(3)} }\\)   \\( \\space \\space \\space \\space = \\frac{1}{2} \\cdotp (0 \u0026#43; 2*(a_{(1)}^{(3)})^{(2-1)} \u0026#43; -2 t_{(1)}^{(3)}) = a_{(1)}^{(3)} - t_{(1)}^{(3)}\\)  2)  \\( \\frac { \\partial sigmoid (z_{(1)}^{(3)}) } { \\partial z_{(1)}^{(3)} } = sigmoid(z_{(1)}^{(3)}) * (1-sigmoid(z_{(1)}^{(3)}))\\)  3)  \\( \\frac { \\partial ( a_{(1)}^{(2)} w_{(1,1)}^{(3)} \u0026#43; a_{(2)}^{(2)} w_{(1,2)}^{(3)} \u0026#43; b_{(1)}^{(3)} ) } { \\partial w_{(1,1)}^{(3)} } = a_{(1)}^{(2)} \u0026#43; 0 \u0026#43; 0\\)   \\( = (a_{(1)}^{(3)} - t_{(1)}^{(3)}) * sigmoid(z_{(1)}^{(3)}) * (1-sigmoid(z_{(1)}^{(3)})) * a_{(1)}^{(2)}\\)  ì—¬ê¸°ì„œ  \\( sigmoid(z_{(1)}^{(3)}) = a_{(1)}^{(3)}\\)   \\( = (a_{(1)}^{(3)} - t_{(1)}^{(3)}) * a_{(1)}^{(3)} * ( 1 - a_{(1)}^{(3)} ) * a_{(1)}^{(2)}\\)    \\( \\frac {\\partial E} {\\partial W_{(2,1)}^{(3)}} = \\frac {\\partial E_{(1)}} {\\partial W_{(2,1)}^{(3)}} \u0026#43; \\frac {\\partial E_{(2)}} {\\partial W_{(2,1)}^{(3)}} = \\frac {\\partial E_{(2)}} {\\partial a_{(2)}^{(3)}} * \\frac {\\partial a_{(2)}^{(3)}}{\\partial z_{(2)}^{(3)}} * \\frac {\\partial z_{(2)}^{(3)}}{\\partial w_{(2,1)}^{(3)}} \\)   \\( = \\frac {\\partial \\begin{Bmatrix} \\frac{1}{2} (t_{(2)}^{(3)} - a_{(2)}^{(3)})^2 ) \\end{Bmatrix} } {\\partial a_{(2)}^{(3)}} * \\frac { \\partial sigmoid (z_{(2)}^{(3)}) } { \\partial z_{(2)}^{(3)} } * \\frac { \\partial ( a_{(1)}^{(2)} w_{(2,1)}^{(3)} \u0026#43; a_{(2)}^{(2)} w_{(2,2)}^{(3)} \u0026#43; b_{(2)}^{(3)} ) } { \\partial w_{(2,1)}^{(3)} } \\)  1)  \\( \\frac {\\partial \\begin{Bmatrix} \\frac{1}{2} (t_{(2)}^{(3)} - a_{(2)}^{(3)})^2 ) \\end{Bmatrix} } {\\partial a_{(2)}^{(3)}} = \\frac{1}{2} \\cdotp \\frac {\\partial (t_{(2)}^{(3)})^2 \u0026#43; (a_{(2)}^{(3)})^2 -2 t_{(2)}^{(3)} a_{(2)}^{(3)} } { \\partial a_{(2)}^{(3)} }\\)   \\( \\space \\space \\space \\space = \\frac{1}{2} \\cdotp (0 \u0026#43; 2*(a_{(2)}^{(3)})^{(2-1)} \u0026#43; -2 t_{(2)}^{(3)}) = a_{(2)}^{(3)} - t_{(2)}^{(3)}\\)  2)  \\( \\frac { \\partial sigmoid (z_{(2)}^{(3)}) } { \\partial z_{(2)}^{(3)} } = sigmoid(z_{(2)}^{(3)}) * (1-sigmoid(z_{(2)}^{(3)}))\\)  3)  \\( \\frac { \\partial ( a_{(1)}^{(2)} w_{(2,1)}^{(3)} \u0026#43; a_{(2)}^{(2)} w_{(2,2)}^{(3)} \u0026#43; b_{(2)}^{(3)} ) } { \\partial w_{(2,1)}^{(3)} } = a_{(1)}^{(2)} \u0026#43; 0 \u0026#43; 0\\)   \\( = (a_{(2)}^{(3)} - t_{(2)}^{(3)}) * sigmoid(z_{(2)}^{(3)}) * (1-sigmoid(z_{(2)}^{(3)})) * a_{(1)}^{(2)}\\)  ì—¬ê¸°ì„œ  \\( sigmoid(z_{(2)}^{(3)}) = a_{(2)}^{(3)}\\)   \\( = (a_{(2)}^{(3)} - t_{(2)}^{(3)}) * a_{(2)}^{(3)} * ( 1 - a_{(2)}^{(3)} ) * a_{(1)}^{(2)}\\)    \\( \\frac {\\partial E} {\\partial W_{(1,2)}^{(3)}} = \\frac {\\partial E_{(1)}} {\\partial W_{(1,2)}^{(3)}} \u0026#43; \\frac {\\partial E_{(2)}} {\\partial W_{(1,2)}^{(3)}} = \\frac {\\partial E_{(1)}} {\\partial a_{(1)}^{(3)}} * \\frac {\\partial a_{(1)}^{(3)}}{\\partial z_{(1)}^{(3)}} * \\frac {\\partial z_{(1)}^{(3)}}{\\partial w_{(1,2)}^{(3)}} \\)   \\( = \\frac {\\partial \\begin{Bmatrix} \\frac{1}{2} (t_{(1)}^{(3)} - a_{(1)}^{(3)})^2 ) \\end{Bmatrix} } {\\partial a_{(1)}^{(3)}} * \\frac { \\partial sigmoid (z_{(1)}^{(3)}) } { \\partial z_{(1)}^{(3)} } * \\frac { \\partial ( a_{(1)}^{(2)} w_{(1,1)}^{(3)} \u0026#43; a_{(2)}^{(2)} w_{(1,2)}^{(3)} \u0026#43; b_{(1)}^{(3)} ) } { \\partial w_{(1,2)}^{(3)} } \\)  1)  \\( \\frac {\\partial \\begin{Bmatrix} \\frac{1}{2} (t_{(1)}^{(3)} - a_{(1)}^{(3)})^2 ) \\end{Bmatrix} } {\\partial a_{(1)}^{(3)}} = \\frac{1}{2} \\cdotp \\frac {\\partial (t_{(1)}^{(3)})^2 \u0026#43; (a_{(1)}^{(3)})^2 -2 t_{(1)}^{(3)} a_{(1)}^{(3)} } { \\partial a_{(1)}^{(3)} }\\)   \\( \\space \\space \\space \\space = \\frac{1}{2} \\cdotp (0 \u0026#43; 2*(a_{(1)}^{(3)})^{(2-1)} \u0026#43; -2 t_{(1)}^{(3)}) = a_{(1)}^{(3)} - t_{(1)}^{(3)}\\)  2)  \\( \\frac { \\partial sigmoid (z_{(1)}^{(3)}) } { \\partial z_{(1)}^{(3)} } = sigmoid(z_{(1)}^{(3)}) * (1-sigmoid(z_{(1)}^{(3)}))\\)  3)  \\( \\frac { \\partial ( a_{(1)}^{(2)} w_{(1,1)}^{(3)} \u0026#43; a_{(2)}^{(2)} w_{(1,2)}^{(3)} \u0026#43; b_{(1)}^{(3)} ) } { \\partial w_{(1,2)}^{(3)} } = a_{(1)}^{(2)} \u0026#43; 0 \u0026#43; 0\\)   \\( = (a_{(1)}^{(3)} - t_{(1)}^{(3)}) * sigmoid(z_{(1)}^{(3)}) * (1-sigmoid(z_{(1)}^{(3)})) * a_{(2)}^{(2)}\\)  ì—¬ê¸°ì„œ  \\( sigmoid(z_{(1)}^{(3)}) = a_{(1)}^{(3)}\\)   \\( = (a_{(1)}^{(3)} - t_{(1)}^{(3)}) * a_{(1)}^{(3)} * ( 1 - a_{(1)}^{(3)} ) * a_{(2)}^{(2)}\\)    \\( \\frac {\\partial E} {\\partial W_{(2,2)}^{(3)}} = \\frac {\\partial E_{(1)}} {\\partial W_{(2,2)}^{(3)}} \u0026#43; \\frac {\\partial E_{(2)}} {\\partial W_{(2,2)}^{(3)}} = \\frac {\\partial E_{(2)}} {\\partial a_{(2)}^{(3)}} * \\frac {\\partial a_{(2)}^{(3)}}{\\partial z_{(2)}^{(3)}} * \\frac {\\partial z_{(2)}^{(3)}}{\\partial w_{(2,2)}^{(3)}} \\)   \\( = \\frac {\\partial \\begin{Bmatrix} \\frac{1}{2} (t_{(2)}^{(3)} - a_{(2)}^{(3)})^2 ) \\end{Bmatrix} } {\\partial a_{(2)}^{(3)}} * \\frac { \\partial sigmoid (z_{(2)}^{(3)}) } { \\partial z_{(2)}^{(3)} } * \\frac { \\partial ( a_{(1)}^{(2)} w_{(2,1)}^{(3)} \u0026#43; a_{(2)}^{(2)} w_{(2,2)}^{(3)} \u0026#43; b_{(2)}^{(3)} ) } { \\partial w_{(2,2)}^{(3)} } \\)  1)  \\( \\frac {\\partial \\begin{Bmatrix} \\frac{1}{2} (t_{(2)}^{(3)} - a_{(2)}^{(3)})^2 ) \\end{Bmatrix} } {\\partial a_{(2)}^{(3)}} = \\frac{1}{2} \\cdotp \\frac {\\partial (t_{(2)}^{(3)})^2 \u0026#43; (a_{(2)}^{(3)})^2 -2 t_{(2)}^{(3)} a_{(2)}^{(3)} } { \\partial a_{(2)}^{(3)} }\\)   \\( \\space \\space \\space \\space = \\frac{1}{2} \\cdotp (0 \u0026#43; 2*(a_{(2)}^{(3)})^{(2-1)} \u0026#43; -2 t_{(2)}^{(3)}) = a_{(2)}^{(3)} - t_{(2)}^{(3)}\\)  2)  \\( \\frac { \\partial sigmoid (z_{(2)}^{(3)}) } { \\partial z_{(2)}^{(3)} } = sigmoid(z_{(2)}^{(3)}) * (1-sigmoid(z_{(2)}^{(3)}))\\)  3)  \\( \\frac { \\partial ( a_{(1)}^{(2)} w_{(2,1)}^{(3)} \u0026#43; a_{(2)}^{(2)} w_{(2,2)}^{(3)} \u0026#43; b_{(2)}^{(3)} ) } { \\partial w_{(2,2)}^{(3)} } = a_{(2)}^{(2)} \u0026#43; 0 \u0026#43; 0\\)   \\( = (a_{(2)}^{(3)} - t_{(2)}^{(3)}) * sigmoid(z_{(2)}^{(3)}) * (1-sigmoid(z_{(2)}^{(3)})) * a_{(2)}^{(2)}\\)  ì—¬ê¸°ì„œ  \\( sigmoid(z_{(2)}^{(3)}) = a_{(2)}^{(3)}\\)   \\( = (a_{(2)}^{(3)} - t_{(2)}^{(3)}) * a_{(2)}^{(3)} * ( 1 - a_{(2)}^{(3)} ) * a_{(2)}^{(2)}\\)    \\( \\frac {\\partial E} {\\partial b_{(1)}^{(3)}} = \\frac {\\partial E_{(1)}} {\\partial b_{(1)}^{(3)}} \u0026#43; \\frac {\\partial E_{(2)}} {\\partial b_{(1)}^{(3)}} = \\frac {\\partial E_{(1)}} {\\partial a_{(1)}^{(3)}} * \\frac {\\partial a_{(1)}^{(3)}}{\\partial z_{(1)}^{(3)}} * \\frac {\\partial z_{(1)}^{(3)}}{\\partial b_{(1)}^{(3)}} \\)   \\( = \\frac {\\partial \\begin{Bmatrix} \\frac{1}{2} (t_{(1)}^{(3)} - a_{(1)}^{(3)})^2 ) \\end{Bmatrix} } {\\partial a_{(1)}^{(3)}} * \\frac { \\partial sigmoid (z_{(1)}^{(3)}) } { \\partial z_{(1)}^{(3)} } * \\frac { \\partial ( a_{(1)}^{(2)} w_{(1,1)}^{(3)} \u0026#43; a_{(2)}^{(2)} w_{(1,2)}^{(3)} \u0026#43; b_{(1)}^{(3)} ) } { \\partial b_{(1)}^{(3)} } \\)  1)  \\( \\frac {\\partial \\begin{Bmatrix} \\frac{1}{2} (t_{(1)}^{(3)} - a_{(1)}^{(3)})^2 ) \\end{Bmatrix} } {\\partial a_{(1)}^{(3)}} = \\frac{1}{2} \\cdotp \\frac {\\partial (t_{(1)}^{(3)})^2 \u0026#43; (a_{(1)}^{(3)})^2 -2 t_{(1)}^{(3)} a_{(1)}^{(3)} } { \\partial a_{(1)}^{(3)} }\\)   \\( \\space \\space \\space \\space = \\frac{1}{2} \\cdotp (0 \u0026#43; 2*(a_{(1)}^{(3)})^{(2-1)} \u0026#43; -2 t_{(1)}^{(3)}) = a_{(1)}^{(3)} - t_{(1)}^{(3)}\\)  2)  \\( \\frac { \\partial sigmoid (z_{(1)}^{(3)}) } { \\partial z_{(1)}^{(3)} } = sigmoid(z_{(1)}^{(3)}) * (1-sigmoid(z_{(1)}^{(3)}))\\)  3)  \\( \\frac { \\partial ( a_{(1)}^{(2)} w_{(1,1)}^{(3)} \u0026#43; a_{(2)}^{(2)} w_{(1,2)}^{(3)} \u0026#43; b_{(1)}^{(3)} ) } { \\partial b_{(1)}^{(3)} } = 0 \u0026#43; 0 \u0026#43; 1\\)   \\( = (a_{(1)}^{(3)} - t_{(1)}^{(3)}) * sigmoid(z_{(1)}^{(3)}) * (1-sigmoid(z_{(1)}^{(3)})) * 1\\)  ì—¬ê¸°ì„œ  \\( sigmoid(z_{(1)}^{(3)}) = a_{(1)}^{(3)}\\)   \\( = (a_{(1)}^{(3)} - t_{(1)}^{(3)}) * a_{(1)}^{(3)} * ( 1 - a_{(1)}^{(3)} ) * 1\\)    \\( \\frac {\\partial E} {\\partial b_{(2)}^{(3)}} = \\frac {\\partial E_{(1)}} {\\partial b_{(2)}^{(3)}} \u0026#43; \\frac {\\partial E_{(2)}} {\\partial b_{(2)}^{(3)}} = \\frac {\\partial E_{(2)}} {\\partial a_{(2)}^{(3)}} * \\frac {\\partial a_{(2)}^{(3)}}{\\partial z_{(2)}^{(3)}} * \\frac {\\partial z_{(2)}^{(3)}}{\\partial b_{(2)}^{(3)}} \\)   \\( = \\frac {\\partial \\begin{Bmatrix} \\frac{1}{2} (t_{(2)}^{(3)} - a_{(2)}^{(3)})^2 ) \\end{Bmatrix} } {\\partial a_{(2)}^{(3)}} * \\frac { \\partial sigmoid (z_{(2)}^{(3)}) } { \\partial z_{(2)}^{(3)} } * \\frac { \\partial ( a_{(1)}^{(2)} w_{(2,1)}^{(3)} \u0026#43; a_{(2)}^{(2)} w_{(2,2)}^{(3)} \u0026#43; b_{(2)}^{(3)} ) } { \\partial b_{(2)}^{(3)} } \\)  1)  \\( \\frac {\\partial \\begin{Bmatrix} \\frac{1}{2} (t_{(2)}^{(3)} - a_{(2)}^{(3)})^2 ) \\end{Bmatrix} } {\\partial a_{(2)}^{(3)}} = \\frac{1}{2} \\cdotp \\frac {\\partial (t_{(2)}^{(3)})^2 \u0026#43; (a_{(2)}^{(3)})^2 -2 t_{(2)}^{(3)} a_{(2)}^{(3)} } { \\partial a_{(2)}^{(3)} }\\)   \\( \\space \\space \\space \\space = \\frac{1}{2} \\cdotp (0 \u0026#43; 2*(a_{(2)}^{(3)})^{(2-1)} \u0026#43; -2 t_{(2)}^{(3)}) = a_{(2)}^{(3)} - t_{(2)}^{(3)}\\)  2)  \\( \\frac { \\partial sigmoid (z_{(2)}^{(3)}) } { \\partial z_{(2)}^{(3)} } = sigmoid(z_{(2)}^{(3)}) * (1-sigmoid(z_{(2)}^{(3)}))\\)  3)  \\( \\frac { \\partial ( a_{(1)}^{(2)} w_{(2,1)}^{(3)} \u0026#43; a_{(2)}^{(2)} w_{(2,2)}^{(3)} \u0026#43; b_{(2)}^{(3)} ) } { \\partial b_{(2)}^{(3)} } = 0 \u0026#43; 0 \u0026#43; 1\\)   \\( = (a_{(2)}^{(3)} - t_{(2)}^{(3)}) * sigmoid(z_{(2)}^{(3)}) * (1-sigmoid(z_{(2)}^{(3)})) * 1\\)  ì—¬ê¸°ì„œ  \\( sigmoid(z_{(2)}^{(3)}) = a_{(2)}^{(3)}\\)   \\( = (a_{(2)}^{(3)} - t_{(2)}^{(3)}) * a_{(2)}^{(3)} * ( 1 - a_{(2)}^{(3)} ) * 1\\)       output layer 2 ...    \\( \\frac {\\partial E} {\\partial W^{(3)}} = \\begin{bmatrix} \\frac {\\partial E}{\\partial W_{(1,1)}^{(3)}} \u0026amp; \\frac {\\partial E}{\\partial W_{(2,1)}^{(3)}} \\\\ \\frac {\\partial E}{\\partial W_{(1,2)}^{(3)}} \u0026amp; \\frac {\\partial E}{\\partial W_{(2,2)}^{(3)}} \\end{bmatrix}\\)   \\( = \\begin{bmatrix} (a_{(1)}^{(3)}-t_{(1)}^{(3)})a_{(1)}^{(3)}(1-a_{(1)}^{(3)})a_{(1)}^{(2)} \u0026amp; (a_{(2)}^{(3)}-t_{(2)}^{(3)})a_{(2)}^{(3)}(1-a_{(2)}^{(3)})a_{(1)}^{(2)} \\\\ (a_{(1)}^{(3)}-t_{(1)}^{(3)})a_{(1)}^{(3)}(1-a_{(1)}^{(3)})a_{(2)}^{(2)} \u0026amp; (a_{(2)}^{(3)}-t_{(2)}^{(3)})a_{(2)}^{(3)}(1-a_{(2)}^{(3)})a_{(2)}^{(2)} \\end{bmatrix}\\)   \\( = \\begin{bmatrix} a_{(1)}^{(2)}(a_{(1)}^{(3)}-t_{(1)}^{(3)})a_{(1)}^{(3)}(1-a_{(1)}^{(3)}) \u0026amp; a_{(1)}^{(2)}(a_{(2)}^{(3)}-t_{(2)}^{(3)})a_{(2)}^{(3)}(1-a_{(2)}^{(3)}) \\\\ a_{(2)}^{(2)}(a_{(1)}^{(3)}-t_{(1)}^{(3)})a_{(1)}^{(3)}(1-a_{(1)}^{(3)}) \u0026amp; a_{(2)}^{(2)}(a_{(2)}^{(3)}-t_{(2)}^{(3)})a_{(2)}^{(3)}(1-a_{(2)}^{(3)}) \\end{bmatrix}\\)   \\( = \\begin{bmatrix} a_{(1)}^{(2)} \\\\ a_{(2)}^{(2)} \\end{bmatrix} \\cdotp \\begin{bmatrix} (a_{(1)}^{(3)}-t_{(1)}^{(3)})a_{(1)}^{(3)}(1-a_{(1)}^{(3)}) \u0026amp; (a_{(2)}^{(3)}-t_{(2)}^{(3)})a_{(2)}^{(3)}(1-a_{(2)}^{(3)}) \\end{bmatrix}\\)   \\( A2 = (a_{(1)}^{(2)} a_{(2)}^{(2)}) \\)   \\( loss\\_3 = \\begin{bmatrix} (a_{(1)}^{(3)}-t_{(1)}^{(3)})a_{(1)}^{(3)}(1-a_{(1)}^{(3)}) \u0026amp; (a_{(2)}^{(3)}-t_{(2)}^{(3)})a_{(2)}^{(3)}(1-a_{(2)}^{(3)}) \\end{bmatrix}\\)   \\( = A2^{T} \\cdotp loss\\_3 \\)    \\( \\frac {\\partial E} {\\partial b^{(3)}} = \\begin{bmatrix} \\frac {\\partial E}{\\partial b_{(1)}^{(3)}} \\\\ \\frac {\\partial E}{\\partial b_{(2)}^{(3)}} \\end{bmatrix}\\)   \\( = \\begin{bmatrix} ((a_{(1)}^{(3)}-t_{(1)}^{(3)})a_{(1)}^{(3)}(1-a_{(1)}^{(3)})) \\\\ ((a_{(2)}^{(3)}-t_{(2)}^{(3)})a_{(2)}^{(3)}(1-a_{(2)}^{(3)})) \\end{bmatrix}\\)   \\( loss\\_3 = \\begin{bmatrix} (a_{(1)}^{(3)}-t_{(1)}^{(3)})a_{(1)}^{(3)}(1-a_{(1)}^{(3)}) \u0026amp; (a_{(2)}^{(3)}-t_{(2)}^{(3)})a_{(2)}^{(3)}(1-a_{(2)}^{(3)}) \\end{bmatrix}\\)       output layer 3 ...    \\( W^{(3)} = W^{(3)} - \\alpha \\frac {\\partial E}{\\partial W^{(3)}} = W^{(3)} - \\alpha \\times (A2^T \\cdotp loss\\_3 )\\)   \\( b^{(3)} = b^{(3)} - \\alpha \\frac {\\partial E}{\\partial b^{(3)}} = b^{(3)} - \\alpha \\times loss\\_3\\)       hidden layer 1 ...    \\( W^{(2)} = W^{(2)} - \\alpha \\frac {\\partial E}{\\partial W^{(2)}} = \\frac {\\partial E}{\\partial W_{(1,1)}^{(2)}} , \\frac {\\partial E}{\\partial W_{(2,1)}^{(2)}} , \\frac {\\partial E}{\\partial W_{(1,2)}^{(2)}} , \\frac {\\partial E}{\\partial W_{(2,2)}^{(2)}} \\)   \\( b^{(2)} = b^{(2)} - \\alpha \\frac {\\partial E}{\\partial b^{(2)}} = \\frac {\\partial E}{\\partial b_{(1)}^{(2)}} , \\frac {\\partial E}{\\partial b_{(2)}^{(2)}} \\)    \\( \\frac {\\partial E}{\\partial W_{(1,1)}^{(2)}} = \\frac {\\partial E_{(1)}}{\\partial W_{(1,1)}^{(2)}} \u0026#43; \\frac {\\partial E_{(2)}}{\\partial W_{(1,1)}^{(2)}}\\)   \\( = \\frac {\\partial E_{(1)}}{\\partial a_{(1)}^{(3)}} \\times \\frac {\\partial a_{(1)}^{(3)}}{\\partial z_{(1)}^{(3)}} \\times \\frac {\\partial z_{(1)}^{(3)}}{\\partial a_{(1)}^{(2)}} \\times \\frac {\\partial a_{(1)}^{(2)}}{\\partial z_{(1)}^{(2)}} \\times \\frac {\\partial z_{(1)}^{(2)}}{\\partial w_{(1,1)}^{(2)}}\\)   \\( \u0026#43; \\frac {\\partial E_{(2)}}{\\partial a_{(2)}^{(3)}} \\times \\frac {\\partial a_{(2)}^{(3)}}{\\partial z_{(2)}^{(3)}} \\times \\frac {\\partial z_{(2)}^{(3)}}{\\partial a_{(1)}^{(2)}} \\times \\frac {\\partial a_{(1)}^{(2)}}{\\partial z_{(1)}^{(3)}} \\times \\frac {\\partial z_{(1)}^{(2)}}{\\partial w_{(1,1)}^{(2)}} \\)   \\( = ( a_{(1)}^{(3)}- t_{(1)}^{(3)} \\times sigmoid(z_{(1)}^{(3)}) (1-sigmoid(z_{(1)}^{(3)})) \\times w_{(1,1)}^{(3)} \\times sigmoid(z_{(1)}^{(2)}) (1-sigmoid(z_{(1)}^{(2)})) \\times a_{(1)}^{(1)} )\\)   \\( \\space \\space \\space \u0026#43; ( a_{(2)}^{(3)}- t_{(2)}^{(3)} \\times sigmoid(z_{(2)}^{(3)}) (1-sigmoid(z_{(2)}^{(3)})) \\times w_{(2,1)}^{(3)} \\times sigmoid(z_{(1)}^{(2)}) (1-sigmoid(z_{(1)}^{(2)})) \\times a_{(1)}^{(1)} )\\)   \\( = ( a_{(1)}^{(3)}- t_{(1)}^{(3)} \\times a_{(1)}^{(3)}(1-a_{(1)}^{(3)}) \\times w_{(1,1)}^{(3)} \\times a_{(1)}^{(2)}(1-a_{(1)}^{(2)}) \\times a_{(1)}^{(1)} )\\)   \\( \\space \\space \\space \u0026#43; ( a_{(2)}^{(3)}- t_{(2)}^{(3)} \\times a_{(2)}^{(3)}(1-a_{(2)}^{(3)}) \\times w_{(1,1)}^{(3)} \\times a_{(1)}^{(2)}(1-a_{(1)}^{(2)}) \\times a_{(1)}^{(1)} )\\)    \\( \\frac {\\partial E}{\\partial W_{(2,1)}^{(2)}} = \\frac {\\partial E_{(1)}}{\\partial W_{(2,1)}^{(2)}} \u0026#43; \\frac {\\partial E_{(2)}}{\\partial W_{(2,1)}^{(2)}}\\)   \\( = \\frac {\\partial E_{(1)}}{\\partial a_{(1)}^{(3)}} \\times \\frac {\\partial a_{(1)}^{(3)}}{\\partial z_{(1)}^{(3)}} \\times \\frac {\\partial z_{(1)}^{(3)}}{\\partial a_{(2)}^{(2)}} \\times \\frac {\\partial a_{(2)}^{(2)}}{\\partial z_{(2)}^{(2)}} \\times \\frac {\\partial z_{(2)}^{(2)}}{\\partial w_{(2,1)}^{(2)}}\\)   \\( \u0026#43; \\frac {\\partial E_{(2)}}{\\partial a_{(2)}^{(3)}} \\times \\frac {\\partial a_{(2)}^{(3)}}{\\partial z_{(2)}^{(3)}} \\times \\frac {\\partial z_{(2)}^{(3)}}{\\partial a_{(2)}^{(2)}} \\times \\frac {\\partial a_{(2)}^{(2)}}{\\partial z_{(2)}^{(3)}} \\times \\frac {\\partial z_{(2)}^{(2))}}{\\partial w_{(2,1)}^{(2)}} \\)   \\( = ( a_{(1)}^{(3)}- t_{(1)}^{(3)} \\times sigmoid(z_{(1)}^{(3)}) (1-sigmoid(z_{(1)}^{(3)})) \\times w_{(1,2)}^{(3)} \\times sigmoid(z_{(2)}^{(2)}) (1-sigmoid(z_{(2)}^{(2)})) \\times a_{(1)}^{(1)} )\\)   \\( \\space \\space \\space \u0026#43; ( a_{(2)}^{(3)}- t_{(2)}^{(3)} \\times sigmoid(z_{(2)}^{(3)}) (1-sigmoid(z_{(2)}^{(3)})) \\times w_{(2,2)}^{(3)} \\times sigmoid(z_{(2)}^{(2)}) (1-sigmoid(z_{(2)}^{(2)})) \\times a_{(1)}^{(1)} )\\)   \\( = ( a_{(1)}^{(3)}- t_{(1)}^{(3)} \\times a_{(1)}^{(3)}(1-a_{(1)}^{(3)}) \\times w_{(1,2)}^{(3)} \\times a_{(2)}^{(2)}(1-a_{(2)}^{(2)}) \\times a_{(1)}^{(1)} )\\)   \\( \\space \\space \\space \u0026#43; ( a_{(2)}^{(3)}- t_{(2)}^{(3)} \\times a_{(2)}^{(3)}(1-a_{(2)}^{(3)}) \\times w_{(2,2)}^{(3)} \\times a_{(2)}^{(2)}(1-a_{(2)}^{(2)}) \\times a_{(1)}^{(1)} )\\)    \\( \\frac {\\partial E}{\\partial W_{(1,1)}^{(2)}} = \\frac {\\partial E_{(1)}}{\\partial W_{(1,2)}^{(2)}} \u0026#43; \\frac {\\partial E_{(2)}}{\\partial W_{(1,2)}^{(2)}}\\)   \\( = \\frac {\\partial E_{(1)}}{\\partial a_{(1)}^{(3)}} \\times \\frac {\\partial a_{(1)}^{(3)}}{\\partial z_{(1)}^{(3)}} \\times \\frac {\\partial z_{(1)}^{(3)}}{\\partial a_{(1)}^{(2)}} \\times \\frac {\\partial a_{(1)}^{(2)}}{\\partial z_{(1)}^{(2)}} \\times \\frac {\\partial z_{(1)}^{(2)}}{\\partial w_{(1,2)}^{(2)}}\\)   \\( \u0026#43; \\frac {\\partial E_{(2)}}{\\partial a_{(2)}^{(3)}} \\times \\frac {\\partial a_{(2)}^{(3)}}{\\partial z_{(2)}^{(3)}} \\times \\frac {\\partial z_{(2)}^{(3)}}{\\partial a_{(1)}^{(2)}} \\times \\frac {\\partial a_{(1)}^{(2)}}{\\partial z_{(1)}^{(3)}} \\times \\frac {\\partial z_{(1)}^{(2))}}{\\partial w_{(1,2)}^{(2)}} \\)   \\( = ( a_{(1)}^{(3)}- t_{(1)}^{(3)} \\times sigmoid(z_{(1)}^{(3)}) (1-sigmoid(z_{(1)}^{(3)})) \\times w_{(1,1)}^{(3)} \\times sigmoid(z_{(1)}^{(2)}) (1-sigmoid(z_{(1)}^{(2)})) \\times a_{(2)}^{(1)} )\\)   \\( \\space \\space \\space \u0026#43; ( a_{(2)}^{(3)}- t_{(2)}^{(3)} \\times sigmoid(z_{(2)}^{(3)}) (1-sigmoid(z_{(2)}^{(3)})) \\times w_{(2,1)}^{(3)} \\times sigmoid(z_{(1)}^{(2)}) (1-sigmoid(z_{(1)}^{(2)})) \\times a_{(2)}^{(1)} )\\)   \\( = ( a_{(1)}^{(3)}- t_{(1)}^{(3)} \\times a_{(1)}^{(3)}(1-a_{(1)}^{(3)}) \\times w_{(1,1)}^{(3)} \\times a_{(1)}^{(2)}(1-a_{(1)}^{(2)}) \\times a_{(2)}^{(1)} )\\)   \\( \\space \\space \\space \u0026#43; ( a_{(2)}^{(3)}- t_{(2)}^{(3)} \\times a_{(2)}^{(3)}(1-a_{(2)}^{(3)}) \\times w_{(2,1)}^{(3)} \\times a_{(1)}^{(2)}(1-a_{(1)}^{(2)}) \\times a_{(2)}^{(1)} )\\)    \\( \\frac {\\partial E}{\\partial W_{(2,2)}^{(2)}} = \\frac {\\partial E_{(1)}}{\\partial W_{(2,2)}^{(2)}} \u0026#43; \\frac {\\partial E_{(2)}}{\\partial W_{(2,2)}^{(2)}}\\)   \\( = \\frac {\\partial E_{(1)}}{\\partial a_{(1)}^{(3)}} \\times \\frac {\\partial a_{(1)}^{(3)}}{\\partial z_{(1)}^{(3)}} \\times \\frac {\\partial z_{(1)}^{(3)}}{\\partial a_{(2)}^{(2)}} \\times \\frac {\\partial a_{(2)}^{(2)}}{\\partial z_{(2)}^{(2)}} \\times \\frac {\\partial z_{(2)}^{(2)}}{\\partial w_{(2,2)}^{(2)}}\\)   \\( \u0026#43; \\frac {\\partial E_{(2)}}{\\partial a_{(2)}^{(3)}} \\times \\frac {\\partial a_{(2)}^{(3)}}{\\partial z_{(2)}^{(3)}} \\times \\frac {\\partial z_{(2)}^{(3)}}{\\partial a_{(2)}^{(2)}} \\times \\frac {\\partial a_{(2)}^{(2)}}{\\partial z_{(2)}^{(3)}} \\times \\frac {\\partial z_{(2)}^{(2))}}{\\partial w_{(2,2)}^{(2)}} \\)   \\( = ( a_{(1)}^{(3)}- t_{(1)}^{(3)} \\times sigmoid(z_{(1)}^{(3)}) (1-sigmoid(z_{(1)}^{(3)})) \\times w_{(1,2)}^{(3)} \\times sigmoid(z_{(2)}^{(2)}) (1-sigmoid(z_{(2)}^{(2)})) \\times a_{(2)}^{(1)} )\\)   \\( \\space \\space \\space \u0026#43; ( a_{(2)}^{(3)}- t_{(2)}^{(3)} \\times sigmoid(z_{(2)}^{(3)}) (1-sigmoid(z_{(2)}^{(3)})) \\times w_{(2,2)}^{(3)} \\times sigmoid(z_{(2)}^{(2)}) (1-sigmoid(z_{(2)}^{(2)})) \\times a_{(2)}^{(1)} )\\)   \\( = ( a_{(1)}^{(3)}- t_{(1)}^{(3)} \\times a_{(1)}^{(3)}(1-a_{(1)}^{(3)}) \\times w_{(1,2)}^{(3)} \\times a_{(2)}^{(2)}(1-a_{(2)}^{(2)}) \\times a_{(2)}^{(1)} )\\)   \\( \\space \\space \\space \u0026#43; ( a_{(2)}^{(3)}- t_{(2)}^{(3)} \\times a_{(2)}^{(3)}(1-a_{(2)}^{(3)}) \\times w_{(2,2)}^{(3)} \\times a_{(2)}^{(2)}(1-a_{(2)}^{(2)}) \\times a_{(2)}^{(1)} )\\)    \\( \\frac {\\partial E}{\\partial b_{(1)}^{(2)}} = \\frac {\\partial E_{(1)}}{\\partial b_{(1)}^{(2)}} \u0026#43; \\frac {\\partial E_{(2)}}{\\partial b_{(1)}^{(2)}}\\)   \\( = \\frac {\\partial E_{(1)}}{\\partial a_{(1)}^{(3)}} \\times \\frac {\\partial a_{(1)}^{(3)}}{\\partial z_{(1)}^{(3)}} \\times \\frac {\\partial z_{(1)}^{(3)}}{\\partial a_{(1)}^{(2)}} \\times \\frac {\\partial a_{(1)}^{(3)}}{\\partial z_{(1)}^{(2)}} \\times \\frac {\\partial z_{(1)}^{(2)}}{\\partial b_{(1)}^{(2)}}\\)   \\( \u0026#43; \\frac {\\partial E_{(2)}}{\\partial a_{(2)}^{(3)}} \\times \\frac {\\partial a_{(2)}^{(3)}}{\\partial z_{(2)}^{(3)}} \\times \\frac {\\partial z_{(2)}^{(3)}}{\\partial a_{(1)}^{(2)}} \\times \\frac {\\partial a_{(1)}^{(2)}}{\\partial z_{(1)}^{(3)}} \\times \\frac {\\partial z_{(1)}^{(2))}}{\\partial w_{(1)}^{(2)}} \\)   \\( = ( a_{(1)}^{(3)}- t_{(1)}^{(3)} \\times sigmoid(z_{(1)}^{(3)}) (1-sigmoid(z_{(1)}^{(3)})) \\times w_{(1,1)}^{(3)} \\times sigmoid(z_{(1)}^{(2)}) (1-sigmoid(z_{(1)}^{(2)})) \\times 1 )\\)   \\( \\space \\space \\space \u0026#43; ( a_{(2)}^{(3)}- t_{(2)}^{(3)} \\times sigmoid(z_{(2)}^{(3)}) (1-sigmoid(z_{(2)}^{(3)})) \\times w_{(2,1)}^{(3)} \\times sigmoid(z_{(1)}^{(2)}) (1-sigmoid(z_{(1)}^{(2)})) \\times 1 )\\)   \\( = ( a_{(1)}^{(3)}- t_{(1)}^{(3)} \\times a_{(1)}^{(3)}(1-a_{(1)}^{(3)}) \\times w_{(1,1)}^{(3)} \\times a_{(1)}^{(2)}(1-a_{(1)}^{(2)}) \\times 1 )\\)   \\( \\space \\space \\space \u0026#43; ( a_{(2)}^{(3)}- t_{(2)}^{(3)} \\times a_{(2)}^{(3)}(1-a_{(2)}^{(3)}) \\times w_{(2,1)}^{(3)} \\times a_{(1)}^{(2)}(1-a_{(1)}^{(2)}) \\times 1 )\\)    \\( \\frac {\\partial E}{\\partial b_{(2)}^{(2)}} = \\frac {\\partial E_{(1)}}{\\partial b_{(2)}^{(2)}} \u0026#43; \\frac {\\partial E_{(2)}}{\\partial b_{(2)}^{(2)}}\\)   \\( = \\frac {\\partial E_{(1)}}{\\partial a_{(1)}^{(3)}} \\times \\frac {\\partial a_{(1)}^{(3)}}{\\partial z_{(1)}^{(3)}} \\times \\frac {\\partial z_{(1)}^{(3)}}{\\partial a_{(2)}^{(2)}} \\times \\frac {\\partial a_{(2)}^{(3)}}{\\partial z_{(2)}^{(2)}} \\times \\frac {\\partial z_{(2)}^{(2)}}{\\partial b_{(2)}^{(2)}}\\)   \\( \u0026#43; \\frac {\\partial E_{(2)}}{\\partial a_{(2)}^{(3)}} \\times \\frac {\\partial a_{(2)}^{(3)}}{\\partial z_{(2)}^{(3)}} \\times \\frac {\\partial z_{(2)}^{(3)}}{\\partial a_{(2)}^{(2)}} \\times \\frac {\\partial a_{(2)}^{(2)}}{\\partial z_{(2)}^{(3)}} \\times \\frac {\\partial z_{(2)}^{(2))}}{\\partial w_{(2)}^{(2)}} \\)   \\( = ( a_{(1)}^{(3)}- t_{(1)}^{(3)} \\times sigmoid(z_{(1)}^{(3)}) (1-sigmoid(z_{(1)}^{(3)})) \\times w_{(1,3)}^{(3)} \\times sigmoid(z_{(2)}^{(2)}) (1-sigmoid(z_{(2)}^{(2)})) \\times 1 )\\)   \\( \\space \\space \\space \u0026#43; ( a_{(2)}^{(3)}- t_{(2)}^{(3)} \\times sigmoid(z_{(2)}^{(3)}) (1-sigmoid(z_{(2)}^{(3)})) \\times w_{(2,2)}^{(3)} \\times sigmoid(z_{(2)}^{(2)}) (1-sigmoid(z_{(2)}^{(2)})) \\times 1 )\\)   \\( = ( a_{(1)}^{(3)}- t_{(1)}^{(3)} \\times a_{(1)}^{(3)}(1-a_{(1)}^{(3)}) \\times w_{(1,2)}^{(3)} \\times a_{(2)}^{(2)}(1-a_{(2)}^{(2)}) \\times 1 )\\)   \\( \\space \\space \\space \u0026#43; ( a_{(2)}^{(3)}- t_{(2)}^{(3)} \\times a_{(2)}^{(3)}(1-a_{(2)}^{(3)}) \\times w_{(2,2)}^{(3)} \\times a_{(2)}^{(2)}(1-a_{(2)}^{(2)}) \\times 1 )\\)       hidden layer 2 ...    \\( \\frac {\\partial E} {\\partial W^{(2)}} = \\begin{bmatrix} \\frac {\\partial E}{\\partial W_{(1,1)}^{(2)}} \u0026amp; \\frac {\\partial E}{\\partial W_{(2,1)}^{(2)}} \\\\ \\frac {\\partial E}{\\partial W_{(1,2)}^{(2)}} \u0026amp; \\frac {\\partial E}{\\partial W_{(2,2)}^{(2)}} \\end{bmatrix}\\)  1)  \\( \\frac {\\partial E}{\\partial W_{(1,1)}^{(2)}}\\)   \\( = a_{(1)}^{(1)}(a_{(1)}^{(3)}-t_{(1)}^{(3)})a_{(1)}^{(3)}(1-a_{(1)}^{(3)})w_{(1,1)}^{(3)}a_{(1)}^{(2)}(1-a_{(1)}^{(2)})\\)   \\( \u0026#43; a_{(1)}^{(1)}(a_{(2)}^{(3)}-t_{(2)}^{(3)})a_{(2)}^{(3)}(1-a_{(2)}^{(3)})w_{(2,1)}^{(3)}a_{(1)}^{(2)}(1-a_{(1)}^{(2)})\\)  2)  \\( \\frac {\\partial E}{\\partial W_{(2,1)}^{(2)}}\\)   \\( = a_{(1)}^{(1)}(a_{(1)}^{(3)}-t_{(1)}^{(3)})a_{(1)}^{(3)}(1-a_{(1)}^{(3)})w_{(1,2)}^{(3)}a_{(2)}^{(2)}(1-a_{(2)}^{(2)})\\)   \\( \u0026#43; a_{(1)}^{(1)}(a_{(2)}^{(3)}-t_{(2)}^{(3)})a_{(2)}^{(3)}(1-a_{(2)}^{(3)})w_{(2,2)}^{(3)}a_{(2)}^{(2)}(1-a_{(2)}^{(2)})\\)  3)  \\( \\frac {\\partial E}{\\partial W_{(1,2)}^{(2)}}\\)   \\( = a_{(2)}^{(1)}(a_{(1)}^{(3)}-t_{(1)}^{(3)})a_{(1)}^{(3)}(1-a_{(1)}^{(3)})w_{(1,1)}^{(3)}a_{(1)}^{(2)}(1-a_{(1)}^{(2)})\\)   \\( \u0026#43; a_{(2)}^{(1)}(a_{(2)}^{(3)}-t_{(2)}^{(3)})a_{(2)}^{(3)}(1-a_{(2)}^{(3)})w_{(2,1)}^{(3)}a_{(1)}^{(2)}(1-a_{(1)}^{(2)})\\)  4)  \\( \\frac {\\partial E}{\\partial W_{(2,2)}^{(2)}}\\)   \\( = a_{(2)}^{(1)}(a_{(1)}^{(3)}-t_{(1)}^{(3)})a_{(1)}^{(3)}(1-a_{(1)}^{(3)})w_{(1,2)}^{(3)}a_{(2)}^{(2)}(1-a_{(2)}^{(2)})\\)   \\( \u0026#43; a_{(2)}^{(1)}(a_{(2)}^{(3)}-t_{(2)}^{(3)})a_{(2)}^{(3)}(1-a_{(2)}^{(3)})w_{(2,2)}^{(3)}a_{(2)}^{(2)}(1-a_{(2)}^{(2)})\\)   \\( = \\begin{bmatrix} a_{(1)}^{(1)} \\\\ a_{(2)}^{(1)} \\end{bmatrix}\\)   \\( \\cdotp \\Big( \\begin{bmatrix} (a_{(1)}^{(3)}-t_{(1)}^{(3)})a_{(1)}^{(3)}(1-a_{(1)}^{(3)}) \u0026amp; (a_{(2)}^{(3)}-t_{(2)}^{(3)})a_{(2)}^{(3)}(1-a_{(2)}^{(3)}) \\end{bmatrix}\\)   \\( \\cdotp \\begin{bmatrix} w_{(1,1)}^{(3)} \u0026amp; w_{(1,2)}^{(3)} \\\\ w_{(2,1)}^{(3)} \u0026amp; w_{(2,2)}^{(3)} \\end{bmatrix}\\)   \\( \\times \\begin{bmatrix} a_{(1)}^{(2)}(1-a_{(1)}^{(2)}) \u0026amp; a_{(2)}^{(2)}(1-a_{(2)}^{(2)}) \\end{bmatrix} \\Big)\\)   \\( = A1^{T} \\cdotp ( (loss\\_3 \\cdotp W3^{T}) \\times (A2 \\times (1-A2)) )\\)   \\( = A1^{T} \\cdotp loss\\_2 \\)    \\( \\frac {\\partial E} {\\partial b^{(2)}} = \\begin{bmatrix} \\frac {\\partial E}{\\partial b_{(1)}^{(2)}} \\\\ \\frac {\\partial E}{\\partial b_{(2)}^{(2)}} \\end{bmatrix}\\)  1)  \\( = (a_{(1)}^{(3)}-t_{(1)}^{(3)})a_{(1)}^{(3)}(1-a_{(1)}^{(3)}w_{(1,1)}^{(3)}a_{(1)}^{(2)}(1-a_{(1)}^{(2)}\\)   \\( \u0026#43; (a_{(2)}^{(3)}-t_{(2)}^{(3)})a_{(2)}^{(3)}(1-a_{(2)}^{(3)}w_{(2,1)}^{(3)}a_{(1)}^{(2)}(1-a_{(1)}^{(2)}\\)  2)  \\( = (a_{(1)}^{(3)}-t_{(1)}^{(3)})a_{(1)}^{(3)}(1-a_{(1)}^{(3)}w_{(1,2)}^{(3)}a_{(2)}^{(2)}(1-a_{(2)}^{(2)}\\)   \\( \u0026#43; (a_{(2)}^{(3)}-t_{(2)}^{(3)})a_{(2)}^{(3)}(1-a_{(2)}^{(3)}w_{(2,2)}^{(3)}a_{(2)}^{(2)}(1-a_{(2)}^{(2)}\\)  3)  \\( = (a_{(1)}^{(3)}-t_{(1)}^{(3)})a_{(1)}^{(3)}(1-a_{(1)}^{(3)}w_{(1,1)}^{(3)}a_{(1)}^{(2)}(1-a_{(1)}^{(2)}\\)   \\( \u0026#43; (a_{(2)}^{(3)}-t_{(2)}^{(3)})a_{(2)}^{(3)}(1-a_{(2)}^{(3)}w_{(2,1)}^{(3)}a_{(1)}^{(2)}(1-a_{(1)}^{(2)}\\)  4)  \\( = (a_{(1)}^{(3)}-t_{(1)}^{(3)})a_{(1)}^{(3)}(1-a_{(1)}^{(3)}w_{(1,2)}^{(3)}a_{(2)}^{(2)}(1-a_{(2)}^{(2)}\\)   \\( \u0026#43; (a_{(2)}^{(3)}-t_{(2)}^{(3)})a_{(2)}^{(3)}(1-a_{(2)}^{(3)}w_{(2,2)}^{(3)}a_{(2)}^{(2)}(1-a_{(2)}^{(2)}\\)   \\( = (a_{(1)}^{(3)}-t_{(1)}^{(3)})a_{(1)}^{(3)}(1-a_{(1)}^{(3)}\\)   \\( \\space \\space \\space (a_{(2)}^{(3)}-t_{(2)}^{(3)})a_{(2)}^{(3)}(1-a_{(2)}^{(3)})\\)   \\( \\cdotp \\begin{bmatrix} W_{(1,1)}^{(3)} \u0026amp; W_{(1,2)}^{(3)} \\\\ W_{(2,1)}^{(3)} \u0026amp; W_{(2,2)}^{(3)} \\end{bmatrix}\\)   \\( \\times \\begin{bmatrix} a_{(1)}^{(2)}(1-a_{(1)}{(2)}) \\\\ a_{(2)}^{(2)}(1-a_{(2)}{(2)}) \\end{bmatrix}\\)   \\( = ((loss\\_3 \\cdotp W3^{T}) \\times (A2 \\times (1-A2)) )\\)   \\( = loss\\_2\\)       hidden layer 3 ...    \\( W^{(2)} = W^{(2)} - \\alpha \\frac {\\partial E}{\\partial W^{(2)}} = W^{(2)} - \\alpha \\times (A1^{T} \\cdotp loss\\_2 )\\)   \\( b^{(2)} = b^{(2)} - \\alpha \\frac {\\partial E}{\\partial b^{(2)}} = b^{(2)} - \\alpha \\times loss\\_2\\)        loss, weight and bias update ...    \\( loss\\_4 = (A4 - Target) \\times A4(1-A4)\\)   \\( W4 = W4 - \\alpha \\frac {\\partial E}{\\partial W4} = W4 - \\alpha \\times (A3^{T} \\cdotp loss\\_4)\\)   \\( b4 = b4 - \\alpha \\frac {\\partial E}{\\partial b4} = b4 - \\alpha \\times loss\\_4\\)    \\( loss\\_3 = ( loss\\_4 \\cdotp W4^{T} ) \\times A3(1-A3)\\)   \\( W3 = W3 - \\alpha \\frac {\\partial E}{\\partial W3} = W3 - \\alpha \\times (A2^{T} \\cdotp loss\\_3)\\)   \\( b3 = b3 - \\alpha \\frac {\\partial E}{\\partial b3} = b3 - \\alpha \\times loss\\_3\\)    \\( loss\\_2 = ( loss\\_3 \\cdotp W3^{T} ) \\times A2(1-A2)\\)   \\( W2 = W2 - \\alpha \\frac {\\partial E}{\\partial W2} = W2 - \\alpha \\times (A1^{T} \\cdotp loss\\_2)\\)   \\( b2 = b2 - \\alpha \\frac {\\partial E}{\\partial b2} = b2 - \\alpha \\times loss\\_2\\)      Vanishing Gradient\n Sigmoidë‚˜ Tanhë“± Activation Functionì˜ ì œí•œì  ì‹¤ìˆ˜ ë²”ìœ„ì˜ ì„ íƒìœ¼ë¡œ Depthê°€ ê¹Šì€ Layerì—ì„œëŠ” ì°¨ì´ê°€ Squashingë˜ì–´ ì†Œì‹¤ ë˜ëŠ” íŠ¹ì„±ì„ ì§€ë‹˜\nSolution 1 : Relu, Leaky Relu  \\( f(x) = max(0,x)\\)  Solution 2 : Weight Initialization (Xavier)\nSolution 3 : Dropout\nSolution 4 : Batch Normalization\n  OpenTutorial \u0026#x21a9;\u0026#xfe0e;\n \u0026ldquo;Regression toward the mean\u0026rdquo;, Sir Francis Galton (1822~1911) \u0026#x21a9;\u0026#xfe0e;\n Global í•´ë¥¼ ì°¾ëŠ” ê³¼ì • \u0026#x21a9;\u0026#xfe0e;\n   "});index.add({'id':27,'href':'/docs/documents/deeplearning/tensorflow/','title':"Tensorflow",'section':"Deep Learning",'content':"Tensor Flow #   í…ì € í”Œë¡œìš°ëŠ” 2015ë…„ êµ¬ê¸€ì—ì„œ ê³µê°œí•œ ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬. ì¼€ë¼ìŠ¤ëŠ” ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ Backendë¡œ í•˜ëŠ” ì‹ ê²½ë§ ëª¨ë¸ êµ¬ì„± ë¼ì´ë¸ŒëŸ¬ë¦¬. êµ¬ê¸€ì€ ì¥¬í”¼í„° ë…¸íŠ¸ë¶ì´ë¼ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ ì›¹ ì–´í”Œë¦¬ì¼€ì´ì…˜ì„ ì½”ë©ì´ë¼ëŠ” ì„œë¹„ìŠ¤ë¥¼ í†µí•´ ì œê³µí•˜ê³  ìˆìŒ.\nTensor Flow\nKeras\nPython\nJupyter Notebook\nColab\nì´ë•Œ TensorëŠ” í˜ëŸ¬ë‹¤ë‹ˆëŠ” ë°ì´í„°ë¥¼ ì˜ë¯¸í•¨.\n mermaid.initialize({ \"flowchart\": { \"useMaxWidth\":true }, \"theme\": \"default\" } ) graph LR A((X:Tensor))--|Edge|C((+:Node))--|Edge|D((X+Y:Tensor)) B((Y:Tensor))--|Edge|C style A fill:#ffffff,stroke:#000000,stroke-width:1px style B fill:#ffffff,stroke:#000000,stroke-width:1px style C fill:#ffffff,stroke:#000000,stroke-width:1px style D fill:#ffffff,stroke:#000000,stroke-width:1px ëª¨ë‘ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹ ì‹œì¦Œ2 #  ì•„ë˜ëŠ” ëª¨ë‘ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹ ì‹œì¦Œ2ì˜ Tensorflow 2 Codeë¥¼ ë¶„ì„í•œ ë‚´ìš©ì„.\n lab-02-1 ...   import tensorflow as tf import numpy as np print(tf.__version__) # x_data, y_data ì •ì˜ x_data = [1, 2, 3, 4, 5] y_data = [1, 2, 3, 4, 5] # matplot import import matplotlib.pyplot as plt plt.plot(x_data, y_data, \u0026#39;o\u0026#39;) plt.ylim(0, 8) # reduce_meanì˜ ì°¨ì›ê°ì†Œ í‰ê·  ì‚¬ìš© v =[1., 2., 3., 4.] tf.reduce_mean(v) # 2.5 # 3ì˜ ì œê³±ê·¼ tf.square(3) # 9 # x_data, y_data ì •ì˜ x_data = [1, 2, 3, 4, 5] y_data = [1, 2, 3, 4, 5] # Weight Bias ì •ì˜ W = tf.Variable(2.0) b = tf.Variable(0.5) # ê°€ì„¤ ì •ì˜ hypothesis = W * x_data + b # Weight Bias Numpy ì¶œë ¥ W.numpy(), b.numpy() # Hypothesis Numpy ì¶œë ¥ hypothesis.numpy() # x_data, hypothesis -\u0026gt; R # x_data, y_data -\u0026gt; O plt.plot(x_data, hypothesis.numpy(), \u0026#39;r-\u0026#39;) plt.plot(x_data, y_data, \u0026#39;o\u0026#39;) plt.ylim(0, 12) plt.show() # Cost í•¨ìˆ˜ ì •ì˜ Average((H-Y)^2) cost = tf.reduce_mean(tf.square(hypothesis - y_data)) # ê²½ì‚¬ í•˜ê°•ë²• Tape ì •ì˜ with tf.GradientTape() as tape: hypothesis = W * x_data + b cost = tf.reduce_mean(tf.square(hypothesis - y_data)) # Weight, Bias W_grad, b_grad = tape.gradient(cost, [W, b]) W_grad.numpy(), b_grad.numpy() # í•™ìŠµìœ¨ learning_rate = 0.01 # Weight - (Weight * Learning Rate) W.assign_sub(learning_rate * W_grad) # Bias - (Bias * Learning Rate) b.assign_sub(learning_rate * b_grad) W.numpy(), b.numpy() plt.plot(x_data, hypothesis.numpy(), \u0026#39;r-\u0026#39;) plt.plot(x_data, y_data, \u0026#39;o\u0026#39;) plt.ylim(0, 12) # Weight / Bias ì •ì˜ W = tf.Variable(2.9) b = tf.Variable(0.5) # for loop ìˆ˜í–‰ for i in range(100): # Gradient Tap eì •ì˜ with tf.GradientTape() as tape: hypothesis = W * x_data + b cost = tf.reduce_mean(tf.square(hypothesis - y_data)) # Weight Bias ì •ì˜ W_grad, b_grad = tape.gradient(cost, [W, b]) # Weight - (Learning Rate * Weight) W.assign_sub(learning_rate * W_grad) # Bias - (Learning Rate * Bias) b.assign_sub(learning_rate * b_grad) # Step 10ì—ì„œ ì¶œë ¥ if i % 10 == 0: print(\u0026#34;{:5}|{:10.4f}|{:10.4f}|{:10.6f}\u0026#34;.format(i, W.numpy(), b.numpy(), cost)) # x_data, y_data ì›ë³¸ plt.plot(x_data, y_data, \u0026#39;o\u0026#39;) # x_data, hypothesis ê°€ì„¤ plt.plot(x_data, hypothesis.numpy(), \u0026#39;r-\u0026#39;) plt.ylim(0, 18) print(W * 5 + b) print(W * 2.5 + b)      lab-03-1 ...   # import library import tensorflow as tf import numpy as np # tensor flow version í™•ì¸ print(tf.__version__) # X,Y 1,2,3 np array ì„ ì–¸  X = np.array([1, 2, 3]) Y = np.array([1, 2, 3]) # cost_func ì •ì˜ (Pythoní˜•íƒœ) # Weight, X, Yë¥¼ ì¸ìë¡œ ì „ë‹¬ def cost_func_py(W, X, Y): # c ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™” c = 0 # X ë§Œí¼ Loopë¥¼ ëŒë©° (W*X - Y) ^ 2 -\u0026gt; Cë¡œ ëŒ€ì… # C (ì˜¤ì°¨ ì œê³±ê·¼)ì„ Xë¡œ í‰ê· ë‚¸ ê°’ì„ return for i in range(len(X)): c += (W * X[i] - Y[i]) ** 2 return c / len(X) # cost_func ì •ì˜ (tensorflowí˜•íƒœ) # Weight, X, Yë¥¼ ì¸ìë¡œ ì „ë‹¬ def cost_func_tf(W, X, Y): hypothesis = X * W return tf.reduce_mean(tf.square(hypothesis - Y)) # Weightë¥¼ -3ë¶€í„° 5ê¹Œì§€ 9ê°œì˜ ìš”ì†Œë¡œ ì œê³µ  W_values = np.linspace(-10, 12, num=23) cost_values = [] # numpy.linspace (start,end,number) # ì œê³µëœ Weight, X, Yë¡œ Costë¥¼ ê³„ì‚° for feed_W in W_values: curr_cost_py = cost_func_py(feed_W, X, Y) curr_cost_tf = cost_func_tf(feed_W, X, Y) cost_values.append(curr_cost_tf) print(\u0026#34;{:6.3f} | {:10.5f} | {:10.5f}\u0026#34;.format(feed_W, curr_cost_py, curr_cost_tf)) # matplot import import matplotlib.pyplot as plt # figure size ì§€ì • plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (8,6) plt.plot(W_values, cost_values, \u0026#34;b\u0026#34;) plt.ylabel(\u0026#39;Cost(W)\u0026#39;) plt.xlabel(\u0026#39;W\u0026#39;) plt.show() ################################################################## # random ì‚¬ìš©ì‹œ Seed ì„¤ì • tf.random.set_seed(0) # for reproducibility x_data = [1., 2., 3., 4.] y_data = [1., 3., 5., 7.] # random.normal((ë°°ì—´),mean,stddev) W = tf.Variable(tf.random.normal((1,), -100., 100.)) # 300ê¹Œì§€ for  for step in range(300): # ê°€ì„¤ì„ W*Xë¡œ ì •ì˜ hypothesis = W * X # TFì˜ Cost í•¨ìˆ˜ ì‚¬ìš© cost = tf.reduce_mean(tf.square(hypothesis - Y)) # Learning Rate 0.01 alpha = 0.01 # ê²½ì‚¬ í•˜ê°•ë²• : 1/m * (W*X - Y) * X gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X)) # Weight - Learning Rate * Weight descent = W - tf.multiply(alpha, gradient) # Assing New Weight W.assign(descent) # 10 step ë§ˆë‹¤ ë¡œê·¸ if step % 10 == 0: print(\u0026#39;{:5} | {:15.6f} | {:10.6f}\u0026#39;.format( step, cost.numpy(), W.numpy()[0]) ) print(5.0 * W) print(2.5 * W) ################################################################## x_data = [1., 2., 3., 4.] y_data = [1., 3., 5., 7.] # Weight 5ë¶€í„° ì ‘ê·¼ W = tf.Variable([5.0]) # 300ê¹Œì§€ for  for step in range(300): # ê°€ì„¤ì„ W*Xë¡œ ì •ì˜ hypothesis = W * X # TFì˜ Cost í•¨ìˆ˜ ì‚¬ìš© cost = tf.reduce_mean(tf.square(hypothesis - Y)) # Learning Rate 0.01 alpha = 0.01 # ê²½ì‚¬ í•˜ê°•ë²• : 1/m * (W*X - Y) * X gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X)) # Weight - Learning Rate * Weight descent = W - tf.multiply(alpha, gradient) # Assing New Weight W.assign(descent) # 10 step ë§ˆë‹¤ ë¡œê·¸ if step % 10 == 0: print(\u0026#39;{:5} | {:10.4f} | {:10.6f}\u0026#39;.format(step, cost.numpy(), W.numpy()[0]))      lab-05-1 ...   # LIbrary ì„ ì–¸ import numpy as np import matplotlib.pyplot as plt %matplotlib inline import tensorflow as tf tf.random.set_seed(777) # for reproducibility print(tf.__version__) # x_train data, y_train data ì •ì˜ x_train = [[1., 2.], [2., 3.], [3., 1.], [4., 3.], [5., 3.], [6., 2.]] y_train = [[0.], [0.], [1.], [1.], [1.], [1.]] # x_test data, y_test data x_test = [[2.,2.]] y_test = [[0.]] # x_train dataì˜ xì˜ 1ì—´ì´ x1 # x_train dataì˜ xì˜ 2ì—´ì´ x2 x1 = [x[0] for x in x_train] x2 = [x[1] for x in x_train] # y_train dataì˜ yì˜ 0ì—´ì˜ 3ë‚˜ëˆ„ê¸° ë‚˜ë¨¸ì§€ colors = [int(y[0] % 3) for y in y_train] # x_train data scatter plot plt.scatter(x1,x2, c=colors , marker=\u0026#39;x\u0026#39;) # x_test data scatter plot # Test ë°ì´í„°ëŠ” ë¶‰ì€ìƒ‰ì˜ ìœ„ì¹˜ì™€ ê°™ì´ ì¶”ë¡ ì‹œ 1ì˜ ê°’ì„ ê°€ì§€ê²Œ ë©ë‹ˆë‹¤. plt.scatter(x_test[0][0],x_test[0][1], c=\u0026#34;red\u0026#34;) plt.xlabel(\u0026#34;x1\u0026#34;) plt.ylabel(\u0026#34;x2\u0026#34;) plt.show() # í•™ìŠµ dataset ì •ì˜  dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))#.repeat() # Weight 2*1 í–‰ë ¬ì˜ 0ê°’ìœ¼ë¡œ ì´ˆê¸°í™” # Bias 1 ë²¡í„°ì˜ 0ê°’ìœ¼ë¡œ ì´ˆê¸°í™” W = tf.Variable(tf.zeros([2,1]), name=\u0026#39;weight\u0026#39;) b = tf.Variable(tf.zeros([1]), name=\u0026#39;bias\u0026#39;) # hypothesis = ( 1 / (1 + exponential ((fetures * W) + b)) ) \u0026lt;- Sigmoid # https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg def logistic_regression(features): hypothesis = tf.divide(1., 1. + tf.exp(tf.matmul(features, W) + b)) return hypothesis # ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ - ( y * log( hypothesis(x) ) + (1- y) log(1-hypothesis) ) def loss_fn(hypothesis, features, labels): cost = -tf.reduce_mean(labels * tf.math.log(logistic_regression(features)) + (1 - labels) * tf.math.log(1 - hypothesis)) return cost # Activation SGDë¡œ ì„¤ì •  # Learning Rate 0.01ë¡œ ì„¤ì • optimizer = tf.keras.optimizers.SGD(learning_rate=0.01) # Accuracy Functionì€ hypothesis, labes(Y)ë¥¼ ì¸ìë¡œ ë°›ìŒ # predicted = Type Cast (Hypothesis \u0026gt; 0.5 ì¼ ê²½ìš° Float 32) # accuracy = predicted, labelsë¥¼ ë¹„êµí•˜ì—¬ booleanì„ ë°˜ë‚©í›„ int 32ë¡œ castí•œ ê°’ì˜ í‰ê· ì„ êµ¬í•¨ def accuracy_fn(hypothesis, labels): predicted = tf.cast(hypothesis \u0026gt; 0.5, dtype=tf.float32) accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32)) return accuracy # Gradient Tapeì„ í†µí•´ ê²½ì‚¬ê°’ì„ ê³„ì‚° def grad(features, labels): with tf.GradientTape() as tape: # ì†ì‹¤ ê°’ì€ loss_fn(logistic_regression(X), X, Y) ìœ¼ë¡œ ì •ì˜ # loss_fnì€ Sigmoid loss_value = loss_fn(logistic_regression(features),features,labels) return tape.gradient(loss_value, [W,b]) # 1000íšŒ ë°˜ë³µ ìˆ˜í–‰ EPOCHS = 1001 # for loop 1000íšŒ for step in range(EPOCHS): for features, labels in iter(dataset): # Grad (X, Y) grads = grad(features, labels) # SGD Optimizer Vriable ì„¤ì • optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b])) # 100 Step ë§ˆë‹¤ loss log if step % 100 == 0: print(\u0026#34;Iter: {}, Loss: {:.4f}\u0026#34;.format(step, loss_fn(logistic_regression(features),features,labels))) # Accuracy Functionì— Y\u0026#39;, Yë¥¼ ëŒ€ì…í•˜ê³  Test Accuracy ì‚°ì¶œí›„ ì¶œë ¥ test_acc = accuracy_fn(logistic_regression(x_test),y_test) print(\u0026#34;Testset Accuracy: {:.4f}\u0026#34;.format(test_acc)) x_test = [[5.,2.]] y_test = [[1.]] plt.scatter(x_test[0][0],x_test[0][1], c=\u0026#34;red\u0026#34;) plt.xlabel(\u0026#34;x1\u0026#34;) plt.ylabel(\u0026#34;x2\u0026#34;) plt.show() # Accuracy Functionì— Y\u0026#39;, Yë¥¼ ëŒ€ì…í•˜ê³  Test Accuracy ì‚°ì¶œí›„ ì¶œë ¥ test_acc = accuracy_fn(logistic_regression(x_test),y_test) print(\u0026#34;Testset Accuracy: {:.4f}\u0026#34;.format(test_acc)) print(\u0026#34;Y: \u0026#34;,format(y_test),format(logistic_regression(x_test)) )      lab-05-2 ...   import numpy as np import matplotlib.pyplot as plt %matplotlib inline import tensorflow as tf tf.random.set_seed(777) # for reproducibility print(tf.__version__) # Google Driver ì‚¬ìš©ì„ ìœ„í•œ Mount from google.colab import drive drive.mount(\u0026#39;/content/drive\u0026#39;) # csv data load xy = np.loadtxt(\u0026#39;/content/drive/My Drive/Colab Notebooks/dl4all_2/data-03-diabetes.csv\u0026#39;, delimiter=\u0026#39;,\u0026#39;, dtype=np.float32) # x_train ì „ì²´í–‰ 1ì—´ë¶€í„° ë§ˆì§€ë§‰ ì „ì—´ x_train = xy[:, 0:-1] # y_train ì „ì²´í–‰ ë§ˆì§€ë§‰ ì—´ y_train = xy[:, [-1]] # shape ì¶œë ¥ print(x_train.shape, y_train.shape) # xy ì¶œë ¥ print(xy) dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train)) # Weightì™€ Biasë¥¼ Randomìœ¼ë¡œ ì„¤ì • W = tf.Variable(tf.random.normal((8, 1)), name=\u0026#39;weight\u0026#39;) b = tf.Variable(tf.random.normal((1,)), name=\u0026#39;bias\u0026#39;) # Activation : Hypothesis = 1 / ( 1 + exponential ( X matmul W ) + b ) def logistic_regression(features): hypothesis = tf.divide(1., 1. + tf.exp(tf.matmul(features, W) + b)) return hypothesis # ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ - ( y * log( hypothesis(x) ) + (1- y) log(1-hypothesis) ) def loss_fn(hypothesis, features, labels): cost = -tf.reduce_mean(labels * tf.math.log(logistic_regression(features)) + (1 - labels) * tf.math.log(1 - hypothesis)) return cost # Activation SGD  # Learning Rate 0.01 optimizer = tf.keras.optimizers.SGD(learning_rate=0.01) # Sigmoid í•¨ìˆ˜ë¥¼ í†µí•´ ì˜ˆì¸¡ê°’ì´ 0.5ë³´ë‹¤ í¬ë©´ 1ì„ ë°˜í™˜í•˜ê³  0.5ë³´ë‹¤ ì‘ìœ¼ë©´ 0ìœ¼ë¡œ ë°˜í™˜ def accuracy_fn(hypothesis, labels): predicted = tf.cast(hypothesis \u0026gt; 0.5, dtype=tf.float32) accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32)) return accuracy # GradientTapeì„ í†µí•´ ê²½ì‚¬ê°’ì„ ê³„ì‚° def grad(hypothesis, features, labels): with tf.GradientTape() as tape: loss_value = loss_fn(logistic_regression(features),features,labels) return tape.gradient(loss_value, [W,b]) # ë°˜ë³µíšŸìˆ˜ 1000íšŒ EPOCHS = 1001 # Epochs ë§Œí¼ for loop for step in range(EPOCHS): # datasetì— feturesì™€ labels ë§Œí¼ for loop for features, labels in iter(dataset): # gradients ì •ì˜ grads = grad(logistic_regression(features), features, labels) # SGD Optimizer Variable Setting optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b])) # 100 Step ë§ˆë‹¤ ë¡œê·¸ if step % 100 == 0: print(\u0026#34;Iter: {}, Loss: {:.4f}\u0026#34;.format(step, loss_fn(logistic_regression(features),features,labels)))      lab-06-1 ...   import tensorflow as tf import numpy as np print(tf.__version__) tf.random.set_seed(777) # for reproducibility # x_data, y_data ì •ì˜ x_data = [[1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4], [4, 1, 5, 5], [1, 7, 5, 5], [1, 2, 5, 6], [1, 6, 6, 6], [1, 7, 7, 7]] y_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]] #convert into numpy and float format # numpy array float32 dataë¡œ converting x_data = np.asarray(x_data, dtype=np.float32) y_data = np.asarray(y_data, dtype=np.float32) # datasetì„ ì„ ì–¸í•©ë‹ˆë‹¤. dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data)) dataset = dataset.repeat().batch(2) nb_classes = 3 #classì˜ ê°œìˆ˜ì…ë‹ˆë‹¤. print(x_data.shape) print(y_data.shape) #Weight and bias setting # Weight random.normal (4,3) # Bias random.normal (3) W = tf.Variable(tf.random.normal((4, nb_classes)), name=\u0026#39;weight\u0026#39;) b = tf.Variable(tf.random.normal((nb_classes,)), name=\u0026#39;bias\u0026#39;) variables = [W, b] print(W,b) # tf.nn.softmax computes softmax activations # softmax = exp(logits) / reduce_sum(exp(logits), dim) # softmax softmax ( X * W + B ) def hypothesis(X): return tf.nn.softmax(tf.matmul(X, W) + b) # h(x) print print(hypothesis(x_data)) # Softmax onehot test # Softmax í…ŒìŠ¤íŠ¸ sample_db = [[8,2,1,4]] sample_db = np.asarray(sample_db, dtype=np.float32) print(hypothesis(sample_db)) # Cost Function def cost_fn(X, Y): # H(X) = Logit logits = hypothesis(X) # Cost = SUM ( Y * log(H(X)) )  cost = -tf.reduce_sum(Y * tf.math.log(logits), axis=1) # cost_mean = mean( cost ) cost_mean = tf.reduce_mean(cost) return cost_mean print(cost_fn(x_data, y_data)) # x = 3 x = tf.constant(3.0) # GradientTape ì •ì˜ with tf.GradientTape() as g: # g.watch : Ensures that tensor is being traced by this tape. g.watch(x) y = x * x # x^2 # delta y , delta x dy_dx = g.gradient(y, x) # Will compute to 6.0 print(dy_dx) # Gradient Funtion def grad_fn(X, Y): with tf.GradientTape() as tape: # loss = cost_mean loss = cost_fn(X, Y) # gradient ( loss, ) grads = tape.gradient(loss, variables) return grads print(grad_fn(x_data, y_data)) # fitting : X,Y, 2000 epochs, verbose 100 def fit(X, Y, epochs=2000, verbose=100): # learning rate setting optimizer = tf.keras.optimizers.SGD(learning_rate=0.1) # for loop  for i in range(epochs): # gradient function X, Y grads = grad_fn(X, Y) # Optimizer Variable Seting optimizer.apply_gradients(zip(grads, variables)) # verbose 100 step log if (i==0) | ((i+1)%verbose==0): print(\u0026#39;Loss at epoch %d: %f\u0026#39; %(i+1, cost_fn(X, Y).numpy())) # start fitting loop fit(x_data, y_data) # Sample 2, 1, 3, 2 -\u0026gt; 0,0,1 sample_data = [[2,1,3,2]] # answer_label [[0,0,1]] sample_data = np.asarray(sample_data, dtype=np.float32) a = hypothesis(sample_data) print(a) print(tf.argmax(a, 1)) #index: 2 sample_data = [[1,6,4,4]] sample_data = np.asarray(sample_data, dtype=np.float32) a = hypothesis(sample_data) print(a) print(tf.argmax(a, 1)) #index: 2 b = hypothesis(x_data) print(b) print(tf.argmax(b, 1)) print(tf.argmax(y_data, 1)) # matches with y_data # softmax classifer class softmax_classifer(tf.keras.Model): # class initilization def __init__(self, nb_classes): super(softmax_classifer, self).__init__() self.W = tf.Variable(tf.random.normal((4, nb_classes)), name=\u0026#39;weight\u0026#39;) self.b = tf.Variable(tf.random.normal((nb_classes,)), name=\u0026#39;bias\u0026#39;) # softmax regression def softmax_regression(self, X): return tf.nn.softmax(tf.matmul(X, self.W) + self.b) # cost_function def cost_fn(self, X, Y): logits = self.softmax_regression(X) cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.math.log(logits), axis=1)) return cost # gradient function def grad_fn(self, X, Y): with tf.GradientTape() as tape: cost = self.cost_fn(x_data, y_data) grads = tape.gradient(cost, self.variables) return grads # fitting def fit(self, X, Y, epochs=2000, verbose=500): optimizer = tf.keras.optimizers.SGD(learning_rate=0.1) for i in range(epochs): grads = self.grad_fn(X, Y) optimizer.apply_gradients(zip(grads, self.variables)) if (i==0) | ((i+1)%verbose==0): print(\u0026#39;Loss at epoch %d: %f\u0026#39; %(i+1, self.cost_fn(X, Y).numpy())) model = softmax_classifer(nb_classes) model.fit(x_data, y_data)      lab-06-2 ...   import tensorflow as tf import numpy as np print(tf.__version__) tf.random.set_seed(777) # for reproducibility # Google Driver ì‚¬ìš©ì„ ìœ„í•œ Mount from google.colab import drive drive.mount(\u0026#39;/content/drive\u0026#39;) # csv data load xy = np.loadtxt(\u0026#39;/content/drive/My Drive/Colab Notebooks/dl4all_2/data-04-zoo.csv\u0026#39;, delimiter=\u0026#39;,\u0026#39;, dtype=np.float32) # x_data ì „ì²´í–‰ 1ì—´ë¶€í„° ë§ˆì§€ë§‰ ì „ì—´ x_data = xy[:, 0:-1] # y_data ì „ì²´í–‰ ë§ˆì§€ë§‰ì—´ y_data = xy[:, -1] # nb_classes 7  nb_classes = 7 # 0 ~ 6 # Make Y data as onehot shape # Y Data One-Hot Encoding Y_one_hot = tf.one_hot(y_data.astype(np.int32), nb_classes) print(x_data.shape, Y_one_hot.shape, y_data.shape, Y_one_hot, y_data) # Weight and bias setting # Weight = 16, 7 # Bias = 16 W = tf.Variable(tf.random.normal((16, nb_classes)), name=\u0026#39;weight\u0026#39;) b = tf.Variable(tf.random.normal((nb_classes,)), name=\u0026#39;bias\u0026#39;) variables = [W, b] # tf.nn.softmax computes softmax activations # softmax = exp(logits) / reduce_sum(exp(logits), dim) # Logit Function = X * W + B # cross_entropy_with_logistì—ì„œ logit_fnì„ ë°›ìŒ def logit_fn(X): return tf.matmul(X, W) + b # Hypothesis = Softmax ( Logit Function ) # predictionì—ì„œ hypothesis ë°›ìŒ def hypothesis(X): return tf.nn.softmax(logit_fn(X)) # Cost Function  # Logits = Logit Function # Cost_i = Cross Entropy ( Y , Logit ) # Cost Reduce Mean ( Cost_i ) def cost_fn(X, Y): logits = logit_fn(X) cost_i = tf.keras.losses.categorical_crossentropy(y_true=Y, y_pred=logits, from_logits=True) cost = tf.reduce_mean(cost_i) return cost # Gradient Tape def grad_fn(X, Y): with tf.GradientTape() as tape: # Cost Function ( X, Y ) loss = cost_fn(X, Y) grads = tape.gradient(loss, variables) return grads # Prediction : ArgMax (Hypothesis(X), 1) # Correct Prediction # Accuracy = Average ( Prediction Float 32 Type Casting ) def prediction(X, Y): # Argument ì¤‘ Maxê°’ì„ Prediction Valueë¡œ ì„ íƒ pred = tf.argmax(hypothesis(X), 1) # Predictionê³¼ Argument Maxì˜ ë™ì¼ ì—¬ë¶€ë¥¼ ì„ íƒ correct_prediction = tf.equal(pred, tf.argmax(Y, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) return accuracy # fitting function def fit(X, Y, epochs=1000, verbose=100): # SGD Optimizer : Learning Rate 0.1 optimizer = tf.keras.optimizers.SGD(learning_rate=0.1) e # Epochs for loop for i in range(epochs): grads = grad_fn(X, Y) optimizer.apply_gradients(zip(grads, variables)) if (i==0) | ((i+1)%verbose==0): # print(\u0026#39;Loss at epoch %d: %f\u0026#39; %(i+1, cost_fn(X, Y).numpy())) acc = prediction(X, Y).numpy() loss = cost_fn(X, Y).numpy() print(\u0026#39;Steps: {} Loss: {}, Acc: {}\u0026#39;.format(i+1, loss, acc)) fit(x_data, Y_one_hot)      lab-07-1 ...   # import import numpy as np import matplotlib.pyplot as plt %matplotlib inline import tensorflow as tf from mpl_toolkits.mplot3d import Axes3D tf.random.set_seed(777) # for reproducibility print(tf.__version__) # x_train data x_train = [[1, 2, 1], [1, 3, 2], [1, 3, 4], [1, 5, 5], [1, 7, 5], [1, 2, 5], [1, 6, 6], [1, 7, 7]] # y_train data y_train = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]] # Evaluation our model using this test dataset # x_test data x_test = [[2, 1, 1], [3, 1, 2], [3, 3, 4]] # y_test data y_test = [[0, 0, 1], [0, 0, 1], [0, 0, 1]] # x_train data x1 = [x[0] for x in x_train] x2 = [x[1] for x in x_train] x3 = [x[2] for x in x_train] # plot figure 3d fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) ax.scatter(x1, x2, x3, c=y_train, marker=\u0026#39;^\u0026#39;) # test data  ax.scatter(x_test[0][0], x_test[0][1], x_test[0][2], c=\u0026#34;red\u0026#34;, marker=\u0026#39;x\u0026#39;) ax.scatter(x_test[1][0], x_test[1][1], x_test[1][2], c=\u0026#34;red\u0026#34;, marker=\u0026#39;x\u0026#39;) ax.scatter(x_test[2][0], x_test[2][1], x_test[2][2], c=\u0026#34;red\u0026#34;, marker=\u0026#39;x\u0026#39;) ax.set_xlabel(\u0026#39;X Label\u0026#39;) ax.set_ylabel(\u0026#39;Y Label\u0026#39;) ax.set_zlabel(\u0026#39;Z Label\u0026#39;) plt.show() # Tensorflow data APIë¥¼ í†µí•´ í•™ìŠµì‹œí‚¬ ê°’ì„ ë‹´ëŠ”ë‹¤ (Batch SizeëŠ” í•œë²ˆì— í•™ìŠµì‹œí‚¬ Sizeë¡œ ì •í•¨) # dataset ì •ì˜ dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))#.repeat() # Weight Bias Random ì •ì˜ W = tf.Variable(tf.random.normal((3, 3))) b = tf.Variable(tf.random.normal((3,))) # Softmaxë¥¼ ê°€ì„¤ë¡œ ì„ ì–¸ : Softmaxë¥¼ í†µí•´ ê°€ì¥ ë†’ì€ ê°’ì„ êµ¬í•¨ # softmax_fn ì„ ì–¸ (Hypothesis = Softmax( F(x)ë¥¼ Return)) def softmax_fn(features): # hypothesis = softmax ( fetures (x) * Weight + Bias ) hypothesis = tf.nn.softmax(tf.matmul(features, W) + b) return hypothesis # ê°€ì„¤ì„ ê²€ì¦í•  Cost í•¨ìˆ˜ë¥¼ ì •ì˜ # Average ( - SUM ( Labels (Y) * Log (Softmax(H(X))) ) ) def loss_fn(hypothesis, features, labels): cost = tf.reduce_mean(-tf.reduce_sum(labels * tf.math.log(hypothesis), axis=1)) return cost # Boolean is_decay ì„¤ì • is_decay = True # Start Learning Rate 0.1 ì„¤ì • starter_learning_rate = 0.1 # Decaying the learning rate # tf.train.exponential_decay : This function applies an exponential decay function to a provided initial learning rate # tf.train.inverse_time_decay : It is often recommended to lower the learning rate as the training progresses # tf.train.natural_exp_decay : This function applies an exponential decay function to a provided initial learning rate # tf.train.piecewise_constant : Piecewise constant from boundaries and interval values. # tf.train.polynomial_decay : It is commonly observed that a monotonically decreasing learning rate, whose degree of change is carefully chosen, results in a better performing model. # tf.train.cosine_decay : When training a model, it is often recommended to lower the learning rate as the training progresses. # tf.train.linear_cosine_decay : Note that linear cosine decay is more aggressive than cosine decay and larger initial learning rates can typically be used. # tf.train.noisy_linear_cosine_decay : Note that linear cosine decay is more aggressive than cosine decay and larger initial learning rates can typically be used. # is_decay true ì¼ê²½ìš° if(is_decay): # Learning Rate = Exponential Deacy ( ì‹œì‘ LR, Step, Rate, Staricase ì—¬ë¶€ ) # exp_rate = starter_rate * exp ( decay_rate * t ) # initial_learning_rate : The inital Learning Rate # global_step : í˜„ì¬ í•™ìŠµ íšŸìˆ˜, Global step to use for the decay computation. Must not be negative. # decay_steps : ì´í•™ìŠµ íšŸìˆ˜, Must be positive # decay_rate : ê°ì†Œ ë¹„ìœ¨, * decay_rate # staricase : ì´ì‚°ì  í•™ìŠµ ì†ë„ ê°ì† ìœ ë¬´ ( global_step / decay_steps ) ì˜ ì œê³± ì ìš© ì—¬ë¶€ # decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps) learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=starter_learning_rate, decay_steps=1000, decay_rate=0.96, staircase=True) # SGD Optimizer optimizer = tf.keras.optimizers.SGD(learning_rate) else: # is_decayê°€ falseë©´ ì´ˆê¸° Learning Rate ì ìš© optimizer = tf.keras.optimizers.SGD(learning_rate=starter_learning_rate) # Gradient Tape def grad(hypothesis, features, labels): with tf.GradientTape() as tape: # Loss Value = Loss Function ( softmax (X) , x , y ) loss_value = loss_fn(softmax_fn(features),features,labels) # Gradient Tape ( cost , [w, b] ) return tape.gradient(loss_value, [W,b]) # Accuracy Function def accuracy_fn(hypothesis, labels): # Prediction = Hypothesisì¤‘ Max prediction = tf.argmax(hypothesis, 1) # Equal íŒì • is_correct = tf.equal(prediction, tf.argmax(labels, 1)) # ì •í™•ë„ ì¸¡ì • accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32)) # Accuracy Return return accuracy # epochs 1000 EPOCHS = 1001 # epoch ë§Œí¼ for loop for step in range(EPOCHS): # dataset ì˜ x, y ë§Œí¼ for loop for features, labels in iter(dataset): # features (x) type cast features = tf.cast(features, tf.float32) # labes (y) type cast labels = tf.cast(labels, tf.float32) # Gradient Tape grads = grad(softmax_fn(features), features, labels) # Optimizer optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b])) # verbose 100  if step % 100 == 0: print(\u0026#34;Iter: {}, Loss: {:.4f}\u0026#34;.format(step, loss_fn(softmax_fn(features),features,labels))) # Test Set ìˆ˜í–‰ x_test = tf.cast(x_test, tf.float32) y_test = tf.cast(y_test, tf.float32) test_acc = accuracy_fn(softmax_fn(x_test),y_test) print(\u0026#34;Testset Accuracy: {:.4f}\u0026#34;.format(test_acc))      lab-07-2 ...   # import import numpy as np import matplotlib.pyplot as plt %matplotlib inline import tensorflow as tf tf.random.set_seed(777) # for reproducibility print(tf.__version__) # xy ì •ì˜ xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973], [823.02002, 828.070007, 1828100, 821.655029, 828.070007], [819.929993, 824.400024, 1438100, 818.97998, 824.159973], [816, 820.958984, 1008100, 815.48999, 819.23999], [819.359985, 823, 1188100, 818.469971, 818.97998], [819, 823, 1198100, 816, 820.450012], [811.700012, 815.25, 1098100, 809.780029, 813.669983], [809.51001, 816.659973, 1398100, 804.539978, 809.559998]]) # x_train ì „í–‰, 1ì—´ë¶€í„° ë§ˆì§€ë§‰ ì „ì—´ # y_train ì „í–‰, ë§ˆì§€ë§‰ì—´ x_train = xy[:, 0:-1] y_train = xy[:, [-1]] # ì´ìƒì¹˜ì— ì˜í•´ ê°’ì´ ì™œê³¡ë¨ plt.plot(x_train, \u0026#39;ro\u0026#39;) plt.plot(y_train) plt.show() # Tensorflow data APIë¥¼ í†µí•´ í•™ìŠµì‹œí‚¬ ê°’ì„ ë‹´ìŒ (Batch SizeëŠ” í•œë²ˆì— í•™ìŠµì‹œí‚¬ Sizeë¡œ ì •í•¨) # X(features),Y(labels)ëŠ” ì‹¤ì¬ í•™ìŠµì— ì“°ì¼ Data (ì—°ì‚°ì„ ìœ„í•´ Typeë¥¼ ë§ì¶°ì¤Œ) dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train)) # Weight, Bias Random ì´ˆê¸°í™” W = tf.Variable(tf.random.normal((4, 1)), dtype=tf.float32) b = tf.Variable(tf.random.normal((1,)), dtype=tf.float32) # Linear Regression Hypothesis ì •ì˜ : X * Weight + Bias def linearReg_fn(features): hypothesis = tf.matmul(features, W) + b return hypothesis # Loss Function ì •ì˜ : Average ( ( F(x) - Y ) ^ 2 ) def loss_fn(hypothesis, features, labels): cost = tf.reduce_mean(tf.square(hypothesis - labels)) return cost # SGD Optimizer ì„ íƒ : Learning Rate 0.00005 optimizer = tf.keras.optimizers.SGD(learning_rate=1e-5) # Gradient Tape  def grad(hypothesis, features, labels): with tf.GradientTape() as tape: # Loss Value = Loss Function ( LinearRegression ( x ) , x, y ) loss_value = loss_fn(linearReg_fn(features),features,labels) return tape.gradient(loss_value, [W,b]), loss_value # Epochs 100íšŒ EPOCHS = 101 # For loop  for step in range(EPOCHS): # Dataset X, Y for features, labels in dataset: # X ì˜ Type Cast features = tf.cast(features, tf.float32) # Y ì˜ Type Case labels = tf.cast(labels, tf.float32) # Hypothesis Value = Linear Regression Function ( X ) hypo_value = linearReg_fn(features) # GradientDecenst Value grads, loss_value = grad(linearReg_fn(features), features, labels) optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b])) # Log without Verbose print(\u0026#34;Iter: {}, Loss: {:.4f}, Prediction: {}\u0026#34;.format(step, loss_value, hypo_value)) # xy ì •ì˜ xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973], [823.02002, 828.070007, 1828100, 821.655029, 828.070007], [819.929993, 824.400024, 1438100, 818.97998, 824.159973], [816, 820.958984, 1008100, 815.48999, 819.23999], [819.359985, 823, 1188100, 818.469971, 818.97998], [819, 823, 1198100, 816, 820.450012], [811.700012, 815.25, 1098100, 809.780029, 813.669983], [809.51001, 816.659973, 1398100, 804.539978, 809.559998]]) # Normalization ì •ì˜  def normalization(data): # data ë¥¼ ì¸ìë¡œ ë°›ìŒ # numerator = ê°œë³„ data - ì—´ì˜ minê°’  numerator = data - np.min(data, 0) # denominator = ì—´ì˜ Maxê°’ - ì—´ì˜ Minê°’ denominator = np.max(data, 0) - np.min(data, 0) # print(\u0026#34;data : \u0026#34;, data, \u0026#34;np_min : \u0026#34;, np.min(data,0), \u0026#34;np_max : \u0026#34;, np.max(data,0),\u0026#34;num : \u0026#34;, numerator, \u0026#34;den : \u0026#34;, denominator) # ì—´ì˜ ê°’ì„ Min 0 ~ Max 1ì˜ ë¹„ìœ¨ë¡œ Normalization return numerator / denominator print(xy) xy = normalization(xy) print(xy) x_train = xy[:, 0:-1] y_train = xy[:, [-1]] plt.plot(x_train, \u0026#39;ro\u0026#39;) plt.plot(y_train) plt.show() # dataset, Weight, Bias ì •ì˜ dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train)) W = tf.Variable(tf.random.normal((4, 1)), dtype=tf.float32) b = tf.Variable(tf.random.normal((1,)), dtype=tf.float32) # L2 Norm Function ì •ì˜ # https://junklee.tistory.com/29 def l2_loss(loss, beta = 0.01): # Weight Regularization * Beta (Weight ì´ìƒì¹˜ì˜ ì˜í–¥ì„ ìµœì†Œí™”í•˜ëŠ” ì •ê·œí™”) W_reg = tf.nn.l2_loss(W) # output = sum(t ** 2) / 2 loss = tf.reduce_mean(loss + W_reg * beta) return loss # Flagê°€ Trueì¼ë•Œë§Œ L2 Norm ìœ¼ë¡œ Cost ì •ì˜ def loss_fn2(hypothesis, features, labels, flag = False): cost = tf.reduce_mean(tf.square(hypothesis - labels)) if(flag): cost = l2_loss(cost) return cost # Learning Rate Decay True is_decay = True # 0.1 starter_learning_rate = 0.1 # 50 Steap, 0.96, ì´ì‚° í•™ìŠµì†ë„ ê°ì† ì ìš© # decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps) if(is_decay): learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=starter_learning_rate, decay_steps=50, decay_rate=0.96, staircase=True) optimizer = tf.keras.optimizers.SGD(learning_rate) else: optimizer = tf.keras.optimizers.SGD(learning_rate=starter_learning_rate) # Gradient Tape ì •ì˜ def grad(hypothesis, features, labels, l2_flag): with tf.GradientTape() as tape: loss_value = loss_fn2(linearReg_fn(features),features,labels, l2_flag) return tape.gradient(loss_value, [W,b]), loss_value # 100íšŒ EPOCHS = 101 for step in range(EPOCHS): for features, labels in dataset: # X, Y Type Case features = tf.cast(features, tf.float32) labels = tf.cast(labels, tf.float32) # Gradient Tape grads, loss_value = grad(linearReg_fn(features), features, labels, False) optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b])) # Verbose 10 if step % 10 == 0: print(\u0026#34;Iter: {}, Loss: {:.4f}\u0026#34;.format(step, loss_value)) # 100íšŒ EPOCHS = 101 for step in range(EPOCHS): for features, labels in dataset: # X, Y Type Case features = tf.cast(features, tf.float32) labels = tf.cast(labels, tf.float32) # Gradient Tape grads, loss_value = grad(linearReg_fn(features), features, labels, True) optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b])) # Verbose 10 if step % 10 == 0: print(\u0026#34;Iter: {}, Loss: {:.4f}\u0026#34;.format(step, loss_value))      lab-07-4 ...   import numpy as np import tensorflow as tf tf.random.set_seed(777) # for reproducibility print(tf.__version__) # MNIST Dataset ì •ì˜ mnist = tf.keras.datasets.mnist # Train Dataì™€ Test Data Load (x_train, y_train),(x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 # Kera Model Data model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(), tf.keras.layers.Dense(512, activation=tf.nn.relu), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(10, activation=tf.nn.softmax) ]) # Model : Adam Optimizer / Cros Entropy Loss ì„ ì–¸ model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;sparse_categorical_crossentropy\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) # 5 Epoch í•™ìŠµ ìˆ˜í–‰ model.fit(x_train, y_train, epochs=5) import matplotlib.pyplot as plt %matplotlib inline sample = 1 image = x_test[sample] fig = plt.figure plt.imshow(image, cmap=\u0026#39;gray_r\u0026#39;) plt.show num = 10 images = x_train[:num] labels = y_train[:num] num_row = 2 num_col = 5 fig, axes = plt.subplots(num_row,num_col, figsize = (1.5*num_col,2*num_row)) for i in range(num) : ax = axes[i//num_col, i%num_col] ax.imshow(images[i], cmap=\u0026#39;gray_r\u0026#39;) ax.set_title(\u0026#39;Label: {}\u0026#39;.format(labels[i])) plt.tight_layout() plt.show() # ëª¨ë¸ í‰ê°€ model.evaluate(x_test, y_test) img_index = 1 plt.imshow(x_test[img_index].reshape(28,28), cmap=\u0026#39;gray_r\u0026#39;) pred = model.predict(x_test[img_index].reshape(1,28,28,1)) print(pred.argmax()) num = 100 images = x_test[:num] labels = y_test[:num] num_row = 10 num_col = 10 fig, axes = plt.subplots(num_row,num_col, figsize = (1.5*num_col,2*num_row)) for i in range(num) : ax = axes[i//num_col, i%num_col] ax.imshow(images[i], cmap=\u0026#39;gray_r\u0026#39;) pred = model.predict(x_test[i].reshape(1,28,28,1)) ax.set_title(\u0026#39;Label: {}\u0026#39;.format(pred.argmax())) plt.tight_layout() plt.show() print(x_test.shape)      lab-07-5 ...   import numpy as np import tensorflow as tf import matplotlib.pyplot as plt from tensorflow import keras tf.random.set_seed(777) # for reproducibility print(tf.__version__) # fashion_mnist ì„ ì–¸ fashion_mnist = tf.keras.datasets.fashion_mnist # train_data, train_label, test_data, test_label ì„ ì–¸ (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() # 0 - T-shirt/Top # 1 - Trouser # 2 - Pullover # 3 - Dress # 4 - Coat # 5 - Sandal # 6 - Shirt # 7 - Sneaker # 8 - Bag # 9 - Ankle Boot class_names = [\u0026#39;T-shirt/top\u0026#39;, \u0026#39;Trouser\u0026#39;, \u0026#39;Pullover\u0026#39;, \u0026#39;Dress\u0026#39;, \u0026#39;Coat\u0026#39;, \u0026#39;Sandal\u0026#39;, \u0026#39;Shirt\u0026#39;, \u0026#39;Sneaker\u0026#39;, \u0026#39;Bag\u0026#39;, \u0026#39;Ankle boot\u0026#39;] plt.figure() plt.imshow(train_images[0]) plt.colorbar() plt.grid(False) num = 20 images = train_images[:num]/255.0 labels = train_labels[:num]/255.0 num_col = 10 num_row = int(num / num_col) fig, axes = plt.subplots(num_row, num_col, figsize = (1.5*num_col, 2*num_row)) for i in range(num) : ax = axes[i//num_col, i%num_col] ax.imshow(train_images[i], cmap=plt.cm.binary) ax.spines[\u0026#39;bottom\u0026#39;].set_color(\u0026#39;white\u0026#39;) ax.spines[\u0026#39;top\u0026#39;].set_color(\u0026#39;white\u0026#39;) ax.yaxis.label.set_color(\u0026#39;white\u0026#39;) ax.xaxis.label.set_color(\u0026#39;white\u0026#39;) ax.title.set_color(\u0026#39;white\u0026#39;) ax.tick_params(axis=\u0026#39;y\u0026#39;, colors=\u0026#34;white\u0026#34;) ax.tick_params(axis=\u0026#39;x\u0026#39;, colors=\u0026#39;white\u0026#39;) ax.set_title(\u0026#39;Label: {}\u0026#39;.format(train_labels[i])) plt.tight_layout() plt.figure() plt.show() train_images = train_images / 255.0 test_images = test_images / 255.0 plt.figure(figsize=(10,10)) for i in range(25): plt.subplot(5,5,i+1) plt.xticks([]) plt.yticks([]) plt.grid(True) plt.imshow(train_images[i], cmap=plt.cm.binary) plt.xlabel(class_names[train_labels[i]]) model = keras.Sequential([ keras.layers.Flatten(input_shape=(28, 28)), keras.layers.Dense(128, activation=tf.nn.relu), keras.layers.Dense(10, activation=tf.nn.softmax) ]) model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;sparse_categorical_crossentropy\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) model.fit(train_images, train_labels, epochs=5) test_loss, test_acc = model.evaluate(test_images, test_labels) print(\u0026#39;Test accuracy:\u0026#39;, test_acc)      lab-07-6 ...   import numpy as np import tensorflow as tf from tensorflow import keras tf.random.set_seed(777) # for reproducibility print(tf.__version__) # 50,000 Movie Reviews IMDB (10000ê°œì˜ ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ë‹¨ì–´ë¥¼ í•™ìŠµì‹œ Vectorì— ì‚¬ìš©) imdb = keras.datasets.imdb # train_data, train_labels, test_data, test_labels (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000) # train_data, train_labels print(\u0026#34;Training entries: {}, labels: {}\u0026#34;.format(len(train_data), len(train_labels))) print(train_data[0]) # IMDB Dataë¥¼ Vectorì„ ì‹¤ì¬ ê°’ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥ # A dictionary mapping words to an integer index # ë”•ì…”ë„ˆë¦¬ ë‹¨ì–´ë¥¼ ì¸í‹°ì ¸ ì¸ë±ìŠ¤ë¡œ ë§µí•‘ word_index = imdb.get_word_index() # The first indices are reserved # Data Check : Start 1 and \u0026#39;the\u0026#39; for k,v in word_index.items() : #print(len(k)) if k == \u0026#39;bad\u0026#39; : print(\u0026#34;K : \u0026#34;,k, \u0026#34; V : \u0026#34;, v+3) if v == 0 : print(\u0026#34;K : \u0026#34;,k, \u0026#34; V : \u0026#34;, v) if v == 1 : print(\u0026#34;K : \u0026#34;,k, \u0026#34; V : \u0026#34;, v) # ìµœì´ˆ ìµë±ìŠ¤ë“¤ì€ Reserved : 0~3 ì˜ˆì•½ì–´ë¡œ ì €ì¥ word_index = {k:(v+3) for k,v in word_index.items()} word_index[\u0026#34;\u0026lt;PAD\u0026gt;\u0026#34;] = 0 word_index[\u0026#34;\u0026lt;START\u0026gt;\u0026#34;] = 1 word_index[\u0026#34;\u0026lt;UNK\u0026gt;\u0026#34;] = 2 # unknown word_index[\u0026#34;\u0026lt;UNUSED\u0026gt;\u0026#34;] = 3 reverse_word_index = dict([(value, key) for (key, value) in word_index.items()]) # Word List : 0~3, 4~88587 print(\u0026#39;check : \u0026#39;, reverse_word_index.get(0,\u0026#39;?\u0026#39;)) print(\u0026#39;check : \u0026#39;, reverse_word_index.get(88587,\u0026#39;?\u0026#39;)) print(len(reverse_word_index)) # Reviewë¥¼ Decode def decode_review(text): return \u0026#39; \u0026#39;.join([reverse_word_index.get(i, \u0026#39;?\u0026#39;) for i in text]) check_data_id = 3 print(len(train_data[check_data_id])) # Train Data Value Print print(train_data[check_data_id]) # Print label  print(train_labels[check_data_id]) # Train Data Decode Print decode_review(train_data[check_data_id]) # í•™ìŠµê³¼ í‰ê°€ë¥¼ ìœ„í•´ ë™ì¼ê¸¸ì´ì¸ 256ê¸¸ì´ì˜ ë‹¨ì–´ë¡œ PADê°’ì„ ì£¼ì–´ ë§ì¶°ì¤Œ (ë’¤ì˜ ê¸¸ì´ëŠ” 0ê°’ìœ¼ë¡œ ë§ì¶°ì¤Œ) train_data = keras.preprocessing.sequence.pad_sequences(train_data, value=word_index[\u0026#34;\u0026lt;PAD\u0026gt;\u0026#34;], padding=\u0026#39;post\u0026#39;, maxlen=256) test_data = keras.preprocessing.sequence.pad_sequences(test_data, value=word_index[\u0026#34;\u0026lt;PAD\u0026gt;\u0026#34;], padding=\u0026#39;post\u0026#39;, maxlen=256) print(len(train_data[check_data_id]), len(test_data[check_data_id])) print(train_data[check_data_id]) # Tensorflow keras APIë¥¼ í†µí•´ ëª¨ë¸ì— ëŒ€í•œ ì •ì˜ # ì…ë ¥ Sizeì™€ í•™ìŠµì‹œí‚¬ Layerì˜ í¬ê¸°ì™€ Activation Function ì •ì˜ # input shape is the vocabulary count used for the movie reviews (10,000 words) vocab_size = 10000 model = keras.Sequential() model.add(keras.layers.Embedding(vocab_size, 16)) model.add(keras.layers.GlobalAveragePooling1D()) model.add(keras.layers.Dense(16, activation=tf.nn.relu)) model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid)) # Model Summary ì¶œë ¥ model.summary() # Adam Optimizerê³¼ Cross Entropy Loss ì„ ì–¸ model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) # ëª¨ë¸ì„ í‰ê°€í•  Test ë°ì´íƒ€ì— ëŒ€í•œ ì •ì˜(10000ì„ ê¸°ì¤€ìœ¼ë¡œ í•™ìŠµê³¼ í‰ê°€ ìˆ˜í–‰) # ì´ : 25000 í‰ê°€ ~10000 / í•™ìŠµ 10001~ x_val = train_data[:10000] partial_x_train = train_data[10000:] y_val = train_labels[:10000] partial_y_train = train_labels[10000:] history = model.fit(partial_x_train, partial_y_train, epochs=40, batch_size=512, validation_data=(x_val, y_val), verbose=1) results = model.evaluate(test_data, test_labels) print(results)      lab-09-1 ...   import numpy as np import matplotlib.pyplot as plt %matplotlib inline import tensorflow as tf tf.random.set_seed(777) # for reproducibility print(tf.__version__) # XOR : 00 -\u0026gt; 0, 01 -\u0026gt; 1, 10 -\u0026gt; 1, 11 -\u0026gt; 0 x_data = [[0, 0],[0, 1],[1, 0],[1, 1]] y_data = [[0] , [1], [1], [0]] plt.scatter(x_data[0][0],x_data[0][1], c=\u0026#39;red\u0026#39; , marker=\u0026#39;^\u0026#39;) plt.scatter(x_data[3][0],x_data[3][1], c=\u0026#39;orange\u0026#39; , marker=\u0026#39;^\u0026#39;) plt.scatter(x_data[1][0],x_data[1][1], c=\u0026#39;blue\u0026#39; , marker=\u0026#39;^\u0026#39;) plt.scatter(x_data[2][0],x_data[2][1], c=\u0026#39;green\u0026#39; , marker=\u0026#39;^\u0026#39;) plt.xlabel(\u0026#34;x1\u0026#34;) plt.ylabel(\u0026#34;x2\u0026#34;) plt.show() # Tensorflow data APIë¥¼ í†µí•´ í•™ìŠµì‹œí‚¬ ê°’ë“¤ì„ ë‹´ëŠ”ë‹¤ (Batch SizeëŠ” í•œë²ˆì— í•™ìŠµì‹œí‚¬ Sizeë¡œ ì •í•œë‹¤) # preprocess functionìœ¼ë¡œ features,labelsëŠ” ì‹¤ì¬ í•™ìŠµì— ì“°ì¼ Data ì—°ì‚°ì„ ìœ„í•´ Typeë¥¼ ë§ì¶°ì¤€ë‹¤ dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data)).batch(len(x_data)) def preprocess_data(features, labels): features = tf.cast(features, tf.float32) labels = tf.cast(labels, tf.float32) return features, labels # Weight Bias ì´ˆê¸°í™” W = tf.Variable(tf.zeros((2,1)), name=\u0026#39;weight\u0026#39;) b = tf.Variable(tf.zeros((1,)), name=\u0026#39;bias\u0026#39;) print(\u0026#34;W = {}, B = {}\u0026#34;.format(W.numpy(), b.numpy())) # Sigmoidë¥¼ ê°€ì„¤ë¡œ ì„ ì–¸ # 0ê³¼ 1ì˜ ê°’ë§Œì„ ë¦¬í„´ : tf.sigmoid(tf.matmul(X, W) + b) def logistic_regression(features): hypothesis = tf.divide(1., 1. + tf.exp(tf.matmul(features, W) + b)) return hypothesis # Cost í•¨ìˆ˜ ì •ì˜ def loss_fn(hypothesis, features, labels): cost = -tf.reduce_mean(labels * tf.math.log(logistic_regression(features)) + (1 - labels) * tf.math.log(1 - hypothesis)) return cost optimizer = tf.keras.optimizers.SGD(learning_rate=0.01) # Sigmoid í•¨ìˆ˜ë¥¼ í†µí•´ ì˜ˆì¸¡ê°’ì´ 0.5ë³´ë‹¤ í¬ë©´ 1ì„ ë°˜í™˜í•˜ê³  0.5ë³´ë‹¤ ì‘ìœ¼ë©´ 0ìœ¼ë¡œ ë°˜í™˜ def accuracy_fn(hypothesis, labels): predicted = tf.cast(hypothesis \u0026gt; 0.5, dtype=tf.float32) accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32)) return accuracy # GradientTapeì„ í†µí•´ ê²½ì‚¬ê°’ì„ ê³„ì‚° def grad(hypothesis, features, labels): with tf.GradientTape() as tape: loss_value = loss_fn(logistic_regression(features),features,labels) return tape.gradient(loss_value, [W,b]) # epoch 1000íšŒ EPOCHS = 1001 # epochì—ì„œ 1000íšŒ step for step in range(EPOCHS): # x, y for features, labels in dataset: # features, labelsë¥¼ ë°›ì•„ floatì„ ë§ì¶¤ features, labels = preprocess_data(features, labels) # gradient tape grads = grad(logistic_regression(features), features, labels) optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b])) # Verbose 100 if step % 100 == 0: print(\u0026#34;Iter: {}, Loss: {:.4f}\u0026#34;.format(step, loss_fn(logistic_regression(features),features,labels))) print(\u0026#34;W = {}, B = {}\u0026#34;.format(W.numpy(), b.numpy())) x_data, y_data = preprocess_data(x_data, y_data) test_acc = accuracy_fn(logistic_regression(x_data),y_data) print(\u0026#34;Testset Accuracy: {:.4f}\u0026#34;.format(test_acc)) plt.scatter(x_data[0][0],x_data[0][1], c=\u0026#39;red\u0026#39; , marker=\u0026#39;^\u0026#39;) plt.scatter(x_data[3][0],x_data[3][1], c=\u0026#39;orange\u0026#39; , marker=\u0026#39;^\u0026#39;) plt.scatter(x_data[1][0],x_data[1][1], c=\u0026#39;blue\u0026#39; , marker=\u0026#39;^\u0026#39;) plt.scatter(x_data[2][0],x_data[2][1], c=\u0026#39;green\u0026#39; , marker=\u0026#39;^\u0026#39;) plt.plot(x_data,logistic_regression(x_data), c=\u0026#39;gray\u0026#39;) plt.tick_params(axis=\u0026#39;y\u0026#39;, colors=\u0026#34;white\u0026#34;) plt.tick_params(axis=\u0026#39;x\u0026#39;, colors=\u0026#39;white\u0026#39;) plt.xlabel(\u0026#34;x1\u0026#34;,color=\u0026#34;white\u0026#34;) plt.ylabel(\u0026#34;x2\u0026#34;,color=\u0026#34;white\u0026#34;) plt.show() # W1,b1,W2,b2,W3,b3 ì •ì˜ W1 = tf.Variable(tf.random.normal((2, 1)), name=\u0026#39;weight1\u0026#39;) b1 = tf.Variable(tf.random.normal((1,)), name=\u0026#39;bias1\u0026#39;) W2 = tf.Variable(tf.random.normal((2, 1)), name=\u0026#39;weight2\u0026#39;) b2 = tf.Variable(tf.random.normal((1,)), name=\u0026#39;bias2\u0026#39;) W3 = tf.Variable(tf.random.normal((2, 1)), name=\u0026#39;weight3\u0026#39;) b3 = tf.Variable(tf.random.normal((1,)), name=\u0026#39;bias3\u0026#39;) # layer1 : sigmoid â”€â”¬â”€\u0026gt; layer3 # layer2 : sigmoid --â”˜ # hypothesis ê°€ì„¤ def neural_net(features): layer1 = tf.sigmoid(tf.matmul(features, W1) + b1) layer2 = tf.sigmoid(tf.matmul(features, W2) + b2) layer3 = tf.concat([layer1, layer2],-1) layer3 = tf.reshape(layer3, shape = [-1,2]) hypothesis = tf.sigmoid(tf.matmul(layer3, W3) + b3) return hypothesis # ì†ì‹¤í•¨ìˆ˜ def loss_fn(hypothesis, labels): cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1 - labels) * tf.math.log(1 - hypothesis)) return cost # Optimizer SGD optimizer = tf.keras.optimizers.SGD(learning_rate=0.01) # Accuracy ì¸¡ì • def accuracy_fn(hypothesis, labels): predicted = tf.cast(hypothesis \u0026gt; 0.5, dtype=tf.float32) accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32)) return accuracy # Gradient Tape def grad(hypothesis, features, labels): with tf.GradientTape() as tape: loss_value = loss_fn(neural_net(features),labels) return tape.gradient(loss_value, [W1, W2, W3, b1, b2, b3]) # epoch 50000 EPOCHS = 50000 for step in range(EPOCHS): for features, labels in dataset: # preprocess ì²˜ë¦¬ features, labels = preprocess_data(features, labels) # neural_net : layer1 + layer2 -\u0026gt; layer3 grads = grad(neural_net(features), features, labels) # optimizer optimizer.apply_gradients(grads_and_vars=zip(grads,[W1, W2, W3, b1, b2, b3])) # verbose 5000 if step % 5000 == 0: print(\u0026#34;Iter: {}, Loss: {:.4f}\u0026#34;.format(step, loss_fn(neural_net(features),labels))) x_data, y_data = preprocess_data(x_data, y_data) # test data test_acc = accuracy_fn(neural_net(x_data),y_data) print(\u0026#34;Testset Accuracy: {:.4f}\u0026#34;.format(test_acc)) plt.scatter(x_data[0][0],x_data[0][1], c=\u0026#39;red\u0026#39; , marker=\u0026#39;^\u0026#39;) plt.scatter(x_data[3][0],x_data[3][1], c=\u0026#39;orange\u0026#39; , marker=\u0026#39;^\u0026#39;) plt.scatter(x_data[1][0],x_data[1][1], c=\u0026#39;blue\u0026#39; , marker=\u0026#39;^\u0026#39;) plt.scatter(x_data[2][0],x_data[2][1], c=\u0026#39;green\u0026#39; , marker=\u0026#39;^\u0026#39;) plt.plot(x_data,neural_net(x_data), c=\u0026#39;gray\u0026#39;) plt.tick_params(axis=\u0026#39;y\u0026#39;, colors=\u0026#34;white\u0026#34;) plt.tick_params(axis=\u0026#39;x\u0026#39;, colors=\u0026#39;white\u0026#39;) plt.xlabel(\u0026#34;x1\u0026#34;,color=\u0026#34;white\u0026#34;) plt.ylabel(\u0026#34;x2\u0026#34;,color=\u0026#34;white\u0026#34;) plt.show() # XOR ë¬¸ì œë¥¼ Deep Neural Network í™œìš© í’€ì´ # dataset  dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data)).batch(len(x_data)) nb_classes = 10 class wide_deep_nn(): # ì´ˆê¸°í™” def __init__(self, nb_classes): # ì´ˆê¸°í™” super(wide_deep_nn, self).__init__() # W1,b1,W2,b2,W3,b3,W4,b4 ì •ì˜ self.W1 = tf.Variable(tf.random.normal((2, nb_classes)), name=\u0026#39;weight1\u0026#39;) self.b1 = tf.Variable(tf.random.normal((nb_classes,)), name=\u0026#39;bias1\u0026#39;) self.W2 = tf.Variable(tf.random.normal((nb_classes, nb_classes)), name=\u0026#39;weight2\u0026#39;) self.b2 = tf.Variable(tf.random.normal((nb_classes,)), name=\u0026#39;bias2\u0026#39;) self.W3 = tf.Variable(tf.random.normal((nb_classes, nb_classes)), name=\u0026#39;weight3\u0026#39;) self.b3 = tf.Variable(tf.random.normal((nb_classes,)), name=\u0026#39;bias3\u0026#39;) self.W4 = tf.Variable(tf.random.normal((nb_classes,1)), name=\u0026#39;weight4\u0026#39;) self.b4 = tf.Variable(tf.random.normal((1,)), name=\u0026#39;bias4\u0026#39;) self.variables = [self.W1, self.b1,self.W2, self.b2, self.W3, self.b3, self.W4, self.b4] # data preprocessing def preprocess_data(self, features, labels): features = tf.cast(features, tf.float32) labels = tf.cast(labels, tf.float32) return features, labels # 4Layerì˜ Neural Networkë¥¼ í†µí•´ í•™ìŠµì‹œí‚¨ í›„ ëª¨ë¸ì„ ìƒì„± # layer1-\u0026gt;layer2-\u0026gt;layer3-\u0026gt;hypothesis def deep_nn(self, features): layer1 = tf.sigmoid(tf.matmul(features, self.W1)+self.b1) layer2 = tf.sigmoid(tf.matmul(layer1, self.W2)+self.b2) layer3 = tf.sigmoid(tf.matmul(layer2, self.W3)+self.b3) hypothesis = tf.sigmoid(tf.matmul(layer3, self.W4)+self.b4) return hypothesis # loss function  def loss_fn(self, hypothesis, features, labels): cost = -tf.reduce_mean(labels*tf.math.log(hypothesis)+(1-labels)*tf.math.log(1-hypothesis)) return cost # accuracy function def accuracy_fn(self, hypothesis,labels): predicted = tf.cast(hypothesis \u0026gt; 0.5, dtype=tf.float32) accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32)) return accuracy # Gradient Tape  def grad(self, hypothesis, features, labels): with tf.GradientTape() as tape: loss_value=self.loss_fn(self.deep_nn(features),features,labels) return tape.gradient(loss_value,self.variables) # epochs 2000, verbose 500 def fit(self, dataset, EPOCHS=20000, verbose=500): optimizer = tf.keras.optimizers.SGD(learning_rate=0.01) for step in range(EPOCHS): for features, labels in dataset: features, labels = self.preprocess_data(features, labels) grads = self.grad(self.deep_nn(features),features,labels) optimizer.apply_gradients(grads_and_vars=zip(grads, self.variables)) if step % verbose == 0: print(\u0026#34;Iter: {}, Loss: {:.4f}\u0026#34;.format(step, self.loss_fn(self.deep_nn(features),features,labels))) # Model Test def test_model(self,x_data,y_data): # Test Data preprocessing x_data,y_data = self.preprocess_data(x_data,y_data) # Accuracy check test_acc = self.accuracy_fn(self.deep_nn(x_data),y_data) # Accuracy Printing print(\u0026#34;Testset Accuracy: {:.4f}\u0026#34;.format(test_acc)) # model ì •ì˜ model = wide_deep_nn(nb_classes) # fitting model.fit(dataset) # test data accuracy model.test_model(x_data, y_data) # dataset ì •ì˜ dataset = tf.data.Dataset.from_tensor_slices((x_data,y_data)).batch(len(x_data)) # data preprocessing ì •ì˜ def preprocess_data(features, labels): features = tf.cast(features, tf.float32) labels = tf.cast(labels, tf.float32) return features, labels # summary ê°’ì„ logsí´ë”ì— ì €ì¥í•˜ê³  ì•„ë˜ ëª…ë ¹ì–´ë¡œ ì‹¤í–‰í•´ì„œ í™•ì¸ # tensorboard --logdir=./logs/xor log_path = \u0026#34;./logs/xor\u0026#34; writer = tf.summary.create_file_writer(log_path) # ìœ„ì˜ Dataë¥¼ 4Layerì˜ Neural Networkë¥¼ í†µí•´ í•™ìŠµì‹œí‚¨ í›„ ëª¨ë¸ì„ ìƒì„± # ê°ê°ì˜ ê°’ì„ histogramìœ¼ë¡œ tensorboardì— ì €ì¥ (Model) # ê°ê°ì˜ ê°’ì„ scalarê°’ìœ¼ë¡œ tensorboardì— ì €ì¥ (cost, accuracy) # W1,b1,W2,b2,W3,b3,W4,b4 ì •ì˜ ë° ì„ ì–¸ W1 = tf.Variable(tf.random.normal((2,10)), name=\u0026#39;weight1\u0026#39;) b1 = tf.Variable(tf.random.normal((10,)), name=\u0026#39;bias1\u0026#39;) W2 = tf.Variable(tf.random.normal((10,10)),name=\u0026#39;weight2\u0026#39;) b2 = tf.Variable(tf.random.normal((10,)),name=\u0026#39;bias2\u0026#39;) W3 = tf.Variable(tf.random.normal((10,10)),name=\u0026#39;weight3\u0026#39;) b3 = tf.Variable(tf.random.normal((10,)),name=\u0026#39;bias3\u0026#39;) W4 = tf.Variable(tf.random.normal((10,1)), name=\u0026#39;weight4\u0026#39;) b4 = tf.Variable(tf.random.normal((1,)), name=\u0026#39;bias4\u0026#39;) # layer1 -\u0026gt; layer2 -\u0026gt; layer3 -\u0026gt; hypothesis def neural_net(features, step): layer1 = tf.sigmoid(tf.matmul(features, W1) + b1) layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2) layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3) hypothesis = tf.sigmoid(tf.matmul(layer3,W4) + b4) # tensorboardì— ì €ì¥ with writer.as_default(): tf.summary.histogram(\u0026#34;weights1\u0026#34;,W1,step=step) tf.summary.histogram(\u0026#34;biases1\u0026#34;,b1,step=step) tf.summary.histogram(\u0026#34;layer1\u0026#34;,layer1,step=step) tf.summary.histogram(\u0026#34;weights2\u0026#34;,W2,step=step) tf.summary.histogram(\u0026#34;biases2\u0026#34;,b2,step=step) tf.summary.histogram(\u0026#34;layer2\u0026#34;,layer2,step=step) tf.summary.histogram(\u0026#34;weight3\u0026#34;,W3,step=step) tf.summary.histogram(\u0026#34;biases3\u0026#34;,b3,step=step) tf.summary.histogram(\u0026#34;layer3\u0026#34;,layer3,step=step) tf.summary.histogram(\u0026#34;weights4\u0026#34;,W4,step=step) tf.summary.histogram(\u0026#34;biases4\u0026#34;,b4, step=step) tf.summary.histogram(\u0026#34;hypothesis\u0026#34;,hypothesis,step=step) return hypothesis # loss function def loss_fn(hypothesis,labels): cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1-labels) * tf.math.log(1-hypothesis)) with writer.as_default(): tf.summary.scalar(\u0026#39;loss\u0026#39;,cost,step=step) return cost # Optimizers SGD ì •ì˜ (Learning Rate 0.,1) optimizer = tf.keras.optimizers.SGD(learning_rate=0.1) # Accuracy Function def accuracy_fn(hypothesis, labels): predicted = tf.cast(hypothesis \u0026gt; 0.5, dtype=tf.float32) accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32)) return accuracy # GradientTape def grad(hypothesis, features, labels, step): with tf.GradientTape() as tape: loss_value = loss_fn(neural_net(features, step),labels) return tape.gradient(loss_value, [W1,W2,W3,W4,b1,b2,b3,b4]) # Epochs 3000íšŒ EPOCHS = 3000 # Step - 3000 for step in range(EPOCHS): # dataset x, y for features, labels in dataset: # data preprocessing features, labels = preprocess_data(features, labels) # gradient grads = grad(neural_net(features,step), features, labels, step) # optimizer optimizer.apply_gradients(grads_and_vars=zip(grads,[W1,W2,W3,W4,b1,b2,b3,b4])) # verbose 50 if step % 50 == 0: loss_value = loss_fn(neural_net(features, step), labels) print(\u0026#34;Iter: {}, Loss: {:4f}\u0026#34;.format(step, loss_value)) # preprocessing x_data, y_data = preprocess_data(x_data,y_data) # accuracy test_acc = accuracy_fn(neural_net(x_data,step),y_data) print(\u0026#34;Testset Accuracy: {:.4f}\u0026#34;.format(test_acc)) # Jupyter Notebookì—ì„œ Tensorboard ì‹¤í–‰ # Load the TensorBoard notebook extension %load_ext tensorboard \u0026#39;\u0026#39;\u0026#39;Start TensorBoard through the command line or within a notebook experience. The two interfaces are generally the same. In notebooks, use the %tensorboard line magic. On the command line, run the same command without \u0026#39;%\u0026#34;. \u0026#39;\u0026#39;\u0026#39; %tensorboard --logdir logs/xor      lab-10-1 ...   # tensorflow import import tensorflow as tf import numpy as np from tensorflow.keras.utils import to_categorical from tensorflow.keras.datasets import mnist from time import time import os print(tf.__version__) # ì „ì²´ ì¢‹ì€ ì„¤ëª… : https://engineer-mole.tistory.com/m/18?category=911427 # Load : Check point function  def load(model, checkpoint_dir): print(\u0026#34; [*] Reading checkpoints...\u0026#34;) # check point stateë¥¼ ì €ì¥ ckpt = tf.train.get_checkpoint_state(checkpoint_dir) # ckptê°€ ì¡´ì¬í•˜ë©´  if ckpt : # ckpt_name ê°€ì ¸ì˜´ ckpt_name = os.path.basename(ckpt.model_checkpoint_path) # checkpoint model checkpoint = tf.train.Checkpoint(dnn=model) # save path ì§€ì • restore checkpoint.restore(save_path=os.path.join(checkpoint_dir, ckpt_name)) # check point split \u0026#39;-\u0026#39; countrer counter = int(ckpt_name.split(\u0026#39;-\u0026#39;)[1]) print(\u0026#34; [*] Success to read {}\u0026#34;.format(ckpt_name)) return True, counter else: print(\u0026#34; [*] Failed to find a checkpoint\u0026#34;) return False, 0 # folder check def check_folder(dir): # ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš° ìƒì„± if not os.path.exists(dir): os.makedirs(dir) return dir # Data :Load def load_mnist() : # train_x, train_y, test_x, test_y : mnist data load (train_data, train_labels), (test_data, test_labels) = mnist.load_data() # train_data dimension ë³€ê²½ train_data = np.expand_dims(train_data, axis=-1) # [N, 28, 28] -\u0026gt; [N, 28, 28, 1] # train_data dimension ë³€ê²½ test_data = np.expand_dims(test_data, axis=-1) # [N, 28, 28] -\u0026gt; [N, 28, 28, 1] # train_data, test_data normalize # https://goodtogreate.tistory.com/entry/Neural-Network-%EC%A0%81%EC%9A%A9-%EC%A0%84%EC%97%90-Input-data%EB%A5%BC-Normalize-%ED%95%B4%EC%95%BC-%ED%95%98%EB%8A%94-%EC%9D%B4%EC%9C%A0 train_data, test_data = normalize(train_data, test_data) # train_y, test_y 10 categorization train_labels = to_categorical(train_labels, 10) # [N,] -\u0026gt; [N, 10] test_labels = to_categorical(test_labels, 10) # [N,] -\u0026gt; [N, 10] return train_data, train_labels, test_data, test_labels # Data pre-processing def normalize(train_data, test_data): # 0~255ë¥¼ 0~1ë¡œ ì •ê·œí™” train_data = train_data.astype(np.float32) / 255.0 test_data = test_data.astype(np.float32) / 255.0 return train_data, test_data # loss function def loss_fn(model, images, labels): # Logits ì •ì˜, Training = True ì¼ê²½ìš° Dropoutì„ ì‚¬ìš© (FalseëŠ” ë°˜ëŒ€) logits = model(images, training=True) # loss = average ( cross_entropy( logits, labels )) # Category ë¶„ë¥˜ì˜ ë¬¸ì œ : categorical crossentropy # Outputì´ logit ìƒíƒœê°€ ì•„ë‹˜ìœ¼ë¡œ from_logits= Trueë¡œ ì„¤ì •  # https://hwiyong.tistory.com/335 (softmaxë¥¼ ê±°ì¹˜ëŠëƒ ì•ˆê±°ì¹˜ëŠëƒë¡œ ì„¤ëª…ì¤‘) loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_pred=logits, y_true=labels, from_logits=True)) return loss # Accuracy Function def accuracy_fn(model, images, labels): # Logits ì •ì˜ logits = model(images, training=False) # Logitsì™€ Labelì˜ Equal ì—¬ë¶€ë¥¼ Predictionìœ¼ë¡œ ì •ì˜ prediction = tf.equal(tf.argmax(logits, -1), tf.argmax(labels, -1)) # Prediction Booleanì„ floatìœ¼ë¡œ ì „í™˜í›„ accuracy return accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32)) return accuracy # Gradient Tape def grad(model, images, labels): with tf.GradientTape() as tape: # Loss Function ì •ì˜ loss = loss_fn(model, images, labels) return tape.gradient(loss, model.variables) # Model Function # Layer Flatten ì²˜ë¦¬ def flatten() : return tf.keras.layers.Flatten() # Layerì˜ Dense ì„¤ì • # unitsëŠ” Output ì±„ë„ ê°œìˆ˜ë¥¼ ì„¤ì • # use_biasëŠ” bias ì‚¬ìš© ì—¬ë¶€ ì„¤ì • # kernel initializerëŠ” weight ì´ˆê¸°í™” def dense(label_dim, weight_init) : return tf.keras.layers.Dense(units=label_dim, use_bias=True, kernel_initializer=weight_init) # Layer Activation Sigmoid def sigmoid() : return tf.keras.layers.Activation(tf.keras.activations.sigmoid) # Layer Activation Relu def relu() : return tf.keras.layers.Activation(tf.keras.activations.relu) # overfitting ë°©ì§€ë¥¼ ìœ„í•´ 0~1ì‚¬ì´ì˜ rateë¡œ dropout ì‚¬ìš©ì„ ì„¤ì • def dropout(rate) : return tf.keras.layers.Dropout(rate) # overfitting ë°©ì§€ë¥¼ ìœ„í•´ Batch Normalizationì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì •ì˜ def batch_norm() : return tf.keras.layers.BatchNormalization() # model class ìƒì„± # keras Model ìƒì† í•„ìš” class create_model_class(tf.keras.Model): # logitsì˜ ìµœì¢… ì•„ì›ƒí’‹ ê°œìˆ˜ label_dim = 10 def __init__(self, label_dim): # class ëª¨ë¸ ì´ˆê¸°í™” super(create_model_class, self).__init__() # Weight ì´ˆê¸°í™” : RandomNormal() ì€ í‰ê· ì´ 0 ë¶„ì‚°ì´ 1ì¸ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¡œ ëœë¤í•œ ìˆ˜ë¥¼ ìƒì„±í•˜ì—¬ Weghtë¡œ ì„¤ì • # 10-1-1, 10-1-2 # weight_init = tf.keras.initializers.RandomNormal() # 10-2-1 xavier # https://flonelin.wordpress.com/2018/01/28/weight-initalizer-%EC%A2%85%EB%A5%98/ # Relu : 0ì´í•˜ì˜ ì‹ í˜¸ë¥¼ ì œê±° # Glorot : input / output neuron ìˆ˜ì— ê¸°ë°˜ ì´ˆê¸°í™”ì˜ ìŠ¤ì¼€ì¼ì„ ê²°ì • # He : Reluê°€ 0 ì´í•˜ì˜ ì‹ í˜¸ë¥¼ ì œê±°í•¨ìœ¼ë¡œ ë¶„ì‚°ì„ ë‘ë°° ì£¼ì–´ ë¶„ì‚°ì„ ìœ ì§€ # Orthogonal : SVD(Singular Value Decomposition)ìœ¼ë¡œ ê°€ì¤‘ì¹˜ í–‰ë ¬ ê°í–‰ì´ ëª¨ë‘ ìˆ˜ì§ìœ¼ë¡œ ë§Œë“¬ weight_init = tf.keras.initializers.glorot_uniform() # ë¦¬ìŠ¤íŠ¸ ìë£Œêµ¬ì¡° íƒ€ì… self.model = tf.keras.Sequential() # Convolutionì„ ì´ìš©í•  ê²½ìš° Flattenê³¼ì •ì´ í•„ìš”ì¹˜ ì•ŠìŒ self.model.add(flatten()) # [N,784] -\u0026gt; [N,256] -\u0026gt; [N,256] # 10-1-1, 10-1-2, 10-2-1 # for i in range(2): # 10-2-2 Xavier Deep for i in range(4) : # ì±„ë„ì„ 256ìœ¼ë¡œ í•˜ê³  Sigmoidë¥¼ ì“¸ìˆ˜ìˆê²Œ ë°”ê¿ˆ # 10-1-1, 10-1-2, 10-2-1 # self.model.add(dense(256, weight_init))  # 10-2-2 self.model.add(dense(512, weight_init)) # 10-1-1 sigmoid ì‚¬ìš©ì‹œ # self.model.add(sigmoid()) # 10-1-2 relu ì‚¬ìš©ì‹œ # self.model.add(relu()) # 10-3 dropout ì„¤ì • # self.model.add(dropout(rate=0.5)) # 10-4 batch normalization ì‚¬ìš© # drop out ì‚¬ìš©ì‹œì™€ ìˆœì„œê°€ ë‹¤ë¦„ self.model.add(batch_norm()) self.model.add(relu()) # logitì„ êµ¬í• ë•Œ 10ê°œ outputì´ ì¶œë ¥ë¨ self.model.add(dense(label_dim, weight_init)) # call í•¨ìˆ˜ë¥¼ í†µí•´ ì•„ì›ƒí’€ ì¶œë ¥ì„ ì •ì˜ def call(self, x, training=None, mask=None): x = self.model(x) return x # Function í˜•íƒœ Create Model def create_model_function(label_dim) : # Weight ì´ˆê¸°í™” # 10-1-1, 10-1-2 # weight_init = tf.keras.initializers.RandomNormal() # 10-2-1 xavier weight_init = tf.keras.initializers.glorot_uniform() model = tf.keras.Sequential() model.add(flatten()) # 10-1-1, 10-1-2, 10-2-1 # for i in range(2) : # 10-2-2 Xavier Deep for i in range(4) : # 10-1-1, 10-1-2, 10-2-1 # model.add(dense(256, weight_init)) # 10-2-2 model.add(dense(512,weight_init)) # 10-1-1 sigmoid ì‚¬ìš©ì‹œ # model.add(sigmoid()) # 10-1-2 relu ì‚¬ìš©ì‹œ # model.add(relu()) # 10-3 # model.add(dropout(rate=0.5)) # 10-4 Batch Normalization model.add(batch_norm()) model.add(relu()) model.add(dense(label_dim, weight_init)) return model # Define data \u0026amp; hyper-parameter # dataset load \u0026#34;\u0026#34;\u0026#34; dataset \u0026#34;\u0026#34;\u0026#34; train_x, train_y, test_x, test_y = load_mnist() # parameters ì„¤ì • \u0026#34;\u0026#34;\u0026#34; parameters \u0026#34;\u0026#34;\u0026#34; learning_rate = 0.001 batch_size = 128 training_epochs = 1 # 60000 / 128 = 468 training_iterations = len(train_x) // batch_size label_dim = 10 train_flag = True # dataë¥¼ batch_sizeë¡œ í•™ìŠµ # Shuffle : buffer_sizeëŠ” input data ë³´ë‹¤ ì»¤ì•¼í•¨ # prefetchëŠ” ë„¤íŠ¸ì›Œí¬ê°€ ì„¤ì •í•œ batch_sizeë§Œí¼ í•™ìŠµì‹œ ë©”ëª¨ë¦¬ì— batch_sizeë§Œí¼ ì ì¬ # batch_size ì„¤ì • # .\\ ì´í›„ spaceì— ì—ëŸ¬ ì¶œë ¥ë¨ \u0026#34;\u0026#34;\u0026#34; Graph Input using Dataset API \u0026#34;\u0026#34;\u0026#34; train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).\\ shuffle(buffer_size=100000).\\ prefetch(buffer_size=batch_size).\\ batch(batch_size, drop_remainder=True) test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).\\ shuffle(buffer_size=100000).\\ prefetch(buffer_size=len(test_x)).\\ batch(len(test_x)) # ëª¨ë¸ / ì˜µí‹°ë§ˆì´ì € / ë¡¸ì´í„° ì •ì˜ \u0026#34;\u0026#34;\u0026#34; Model \u0026#34;\u0026#34;\u0026#34; network = create_model_function(label_dim) # Adam Optimizer \u0026#34;\u0026#34;\u0026#34; Training \u0026#34;\u0026#34;\u0026#34; optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate) # Writer ì •ì˜ # 10-1-1 sigmoid ì‚¬ìš©ì‹œ # model_dir = \u0026#39;nn_softmax\u0026#39; # 10-1-2 relu ì‚¬ìš©ì‹œ # model_dir = \u0026#39;nn_relu\u0026#39; # 10-2-1 xavier ì‚¬ìš©ì‹œ # model_dir = \u0026#39;nn_xavier\u0026#39; # 10-2-2 xavier deep # model_dir = \u0026#39;nn_deep\u0026#39; # 10-3 relu dropout # model_dir = \u0026#39;nn_dropout\u0026#39; # 10-4 Batch Normalization \u0026#34;\u0026#34;\u0026#34; Writer \u0026#34;\u0026#34;\u0026#34; checkpoint_dir = \u0026#39;checkpoints\u0026#39; logs_dir = \u0026#39;logs\u0026#39; model_dir = \u0026#39;nn_batchnorm\u0026#39; checkpoint_dir = os.path.join(checkpoint_dir, model_dir) check_folder(checkpoint_dir) checkpoint_prefix = os.path.join(checkpoint_dir, model_dir) logs_dir = os.path.join(logs_dir, model_dir) # Train / Test ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸° if train_flag : # ì²´í¬í¬ì¸íŠ¸ëŠ” ì¬í•™ìŠµì‹œ ë³€ê²½ë˜ì—ˆë˜ Weightë¥¼ ë³µì›í•¨ checkpoint = tf.train.Checkpoint(dnn=network) # í…ì €ë³´ë“œë¥¼ ìœ„í•œ ë¡¸ì´í„° ìƒì„± # create writer for tensorboard summary_writer = tf.summary.create_file_writer(logdir=logs_dir) # ì‹œì‘ ì‹œê°„ start_time = time() # ì²´í¬í¬ì¸íŠ¸ ì¡´ì¬ì‹œ Resotre # restore check-point if it exits could_load, checkpoint_counter = load(network, checkpoint_dir) # Check point ì¡´ì¬ì‹œ í•´ë‹¹ ê°’ìœ¼ë¡œ ì„¤ì • if could_load: # start_epoch = (int)(checkpoint_counter / training_iterations)  # counter = checkpoint_counter  start_epoch = 0 start_iteration = 0 counter = 0 print(\u0026#34; [*] Load SUCCESS\u0026#34;) else: start_epoch = 0 start_iteration = 0 counter = 0 print(\u0026#34; [!] Load failed...\u0026#34;) # train phase with summary_writer.as_default(): # for tensorboard # epochì™€ iterationì— ëŒ€í•œ ì¤‘ì²© for ë¬¸ ìˆ˜í–‰ for epoch in range(start_epoch, training_epochs): for idx, (train_input, train_label) in enumerate(train_dataset): grads = grad(network, train_input, train_label) # optimizer.apply_gradientsë¥¼ í˜¸ì¶œí•˜ì—¬ gradientë¥¼ ì ìš©í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ë¥¼ í•™ìŠµ optimizer.apply_gradients(grads_and_vars=zip(grads, network.variables)) # Loss Function train_loss = loss_fn(network, train_input, train_label) # Accuracy Function train_accuracy = accuracy_fn(network, train_input, train_label) # Test Dataset for test_input, test_label in test_dataset: test_accuracy = accuracy_fn(network, test_input, test_label) # Summary Scalarí˜• í…ì„œ ì‚¬ìš© tf.summary.scalar(name=\u0026#39;train_loss\u0026#39;, data=train_loss, step=counter) tf.summary.scalar(name=\u0026#39;train_accuracy\u0026#39;, data=train_accuracy, step=counter) tf.summary.scalar(name=\u0026#39;test_accuracy\u0026#39;, data=test_accuracy, step=counter) print( \u0026#34;Epoch: [%2d] [%5d/%5d] time: %4.2f, tr_loss: %.4f, tr_acc: %.4f, ts_acc: %.4f\u0026#34; \\ % (epoch, idx, training_iterations, time() - start_time, train_loss, train_accuracy, test_accuracy)) counter += 1 # ì²´í¬ í¬ì¸íŠ¸ ì €ì¥ checkpoint.save(file_prefix=checkpoint_prefix + \u0026#39;-{}\u0026#39;.format(counter)) # test phase  else : _, _ = load(network, checkpoint_dir) for test_input, test_label in test_dataset: test_accuracy = accuracy_fn(network, test_input, test_label) print(\u0026#34;test_Accuracy: %.4f\u0026#34; % (test_accuracy))      lab-11-0 ...   from __future__ import absolute_import from __future__ import division from __future__ import print_function import numpy as np import tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt print(tf.__version__) print(keras.__version__) # 1X3X3X1 shape image = tf.constant([ [ [ [1],[2],[3] ],[ [4],[5],[6] ], [ [7],[8],[9] ] ] ], dtype=np.float32) print(image.shape) # Grey Shape plt.imshow(image.numpy().reshape(3,3), cmap=\u0026#39;Greys\u0026#39;) plt.show() # 12,16,24,28 shape -\u0026gt; plt.show (check) check = tf.constant([ [ [ [12.] ],[ [16.] ] ],[ [ [24.] ],[ [28.] ] ] ], dtype=np.float32) plt.imshow(check.numpy().reshape(2,2), cmap=\u0026#39;Greys\u0026#39;) plt.show() # Image : 1,3,3,1 Filter : 2,2,1,1 Stride : 1X1 Padding : Valid # 1 2 3 1 1 12 16 # 4 5 6 + 1 1 -\u0026gt; 24 28 # 7 8 9  # 1 filter (22,11) with padding : valid print(\u0026#34;image.shape\u0026#34;, image.shape) # Weight ì„¤ì • weight = np.array([ [ [ [1.] ],[ [1.] ] ],[ [ [1.] ],[ [1.] ] ] ]) print(\u0026#34;weight.shape\u0026#34;, weight.shape) weight_init = tf.constant_initializer(weight) conv2d = keras.layers.Conv2D(filters=1, kernel_size=2, padding=\u0026#39;VALID\u0026#39;, kernel_initializer=weight_init)(image) print(\u0026#34;conv2d.shape\u0026#34;, conv2d.shape) print(conv2d.numpy().reshape(2,2)) plt.imshow(conv2d.numpy().reshape(2,2), cmap=\u0026#39;gray\u0026#39;) plt.show() # Image : 1,3,3,1 Filter : 2,2,1,1 Stride : 1X1 Padding : SAME (3X3) # 1 2 3 0 1 1 12 16 9 # 4 5 6 0 -\u0026gt; 1 1 -\u0026gt; 24 28 15 # 7 8 9 0 15 17 9 # 0 0 0 0  print(\u0026#34;image.shape\u0026#34;, image.shape) weight = np.array([[[[1.]],[[1.]]],[[[1.]],[[1.]]]]) print(\u0026#34;weight.shape\u0026#34;, weight.shape) weight_init = tf.constant_initializer(weight) conv2d = keras.layers.Conv2D(filters=1, kernel_size=2, padding=\u0026#39;SAME\u0026#39;, kernel_initializer=weight_init)(image) print(\u0026#34;conv2d.shape\u0026#34;, conv2d.shape) print(conv2d.numpy().reshape(3,3)) plt.imshow(conv2d.numpy().reshape(3,3), cmap=\u0026#39;gray\u0026#39;) plt.show() # image shape 1,3,3,1 # print(\u0026#34;imag:\\n\u0026#34;, image) print(\u0026#34;image.shape\u0026#34;, image.shape) # wieght shape 2,2,1,3 weight = np.array([ [ [ [1.,10.,-1.] ] , [ [1.,10.,-1.] ] ] , [ [ [1.,10.,-1.] ], [ [1.,10.,-1.] ] ] ]) print(\u0026#34;weight.shape\u0026#34;, weight.shape) weight_init = tf.constant_initializer(weight) ################################################################################ # Image : 1,3,3,1 Filter : 2,2,1,(1) Stride : 1X1 Padding : SAME (3X3) # 1 2 3 1 1 12 16 9 # 4 5 6 -\u0026gt; 1 1 -\u0026gt; 24 28 15 # 7 8 9 15 17 9 ################################################################################ # Image : 1,3,3,1 Filter : 2,2,1,(2) Stride : 1X1 Padding : SAME (3X3) # 1 2 3 10 10 120 160 90 # 4 5 6 -\u0026gt; 10 10 -\u0026gt; 240 280 150 # 7 8 9 150 170 90 ################################################################################ # Image : 1,3,3,1 Filter : 2,2,1,(3) Stride : 1X1 Padding : SAME (3X3) # 1 2 3 -1 -1 -12 -16 -9 # 4 5 6 -\u0026gt; -1 -1 -\u0026gt; -24 -28 -15 # 7 8 9 -15 -17 -9 conv2d = keras.layers.Conv2D(filters=3, kernel_size=2, padding=\u0026#39;SAME\u0026#39;, kernel_initializer=weight_init)(image) print(\u0026#34;conv2d.shape\u0026#34;, conv2d.shape) # í–‰/ì—´ êµì²´ : 0 -\u0026gt; 4ì°¨ì› / 3 -\u0026gt; 1ì°¨ì›  feature_maps = np.swapaxes(conv2d, 0, 3) print(\u0026#34;feature_maps\u0026#34;, feature_maps.shape) # swapaxes / reshape / tranpose  print(\u0026#34;weight\u0026#34;, weight.shape) weight_swap = np.swapaxes(weight, 2, 3) # 2,2,1,3 -\u0026gt; 2,2,3,1 print(\u0026#34;weight_swap1\u0026#34;, weight_swap.shape) weight_swap = np.swapaxes(weight, 0, 3) # 2,2,3,1 -\u0026gt; 1,2,3,2 print(\u0026#34;weight_swap2\u0026#34;, weight_swap.shape) weight_reshape = weight.reshape(2,3,-1) # 2,2,1,3 -\u0026gt; 1,2,3,2 print(\u0026#34;weight_reshape1\u0026#34;, weight_reshape.shape) weight_reshape = weight.reshape(1,-1) # 2,2,1,3 -\u0026gt; 1,2,3,2 print(\u0026#34;weight_reshape2\u0026#34;, weight_reshape.shape) weight_trans = np.transpose(weight) print(\u0026#34;weight_trans\u0026#34;, weight_trans.shape) for i, feature_map in enumerate(feature_maps): print(feature_map.reshape(3,3)) plt.subplot(1,3,i+1), plt.imshow(feature_map.reshape(3,3), cmap=\u0026#39;gray\u0026#39;) plt.show() image = tf.constant([ [ [ [1], [1], [2], [4] ], [ [5], [6], [7], [8] ], [ [3], [2], [1], [0] ], [ [1], [2], [3], [4] ] ] ], dtype=np.float32) print(image.shape) # 1 1 2 4  # 5 6 7 8 -\u0026gt; 6 7 8 # 3 2 1 0 6 7 8 # 1 2 3 4 3 3 4 pool = keras.layers.MaxPool2D(pool_size=(2,2), strides=1, padding=\u0026#39;VALID\u0026#39;)(image) print(pool.shape) print(pool.numpy()) # 1 1 2 4  # 5 6 7 8 -\u0026gt; 6 8 # 3 2 1 0 3 4 # 1 2 3 4  pool = keras.layers.MaxPool2D(pool_size=(2,2), strides=2, padding=\u0026#39;VALID\u0026#39;)(image) print(pool.shape) print(pool.numpy()) # image : 1X2X2X1 # 4 3 -\u0026gt; 4 # 2 1 image = tf.constant([ [ [ [4], [3] ], [ [2],[1] ] ] ], dtype=np.float32) print(image.shape) pool = keras.layers.MaxPool2D(pool_size=(2,2), strides=1, padding=\u0026#39;VALID\u0026#39;)(image) print(pool.shape) print(pool.numpy()) # image : 1X2X2X1 # 4 3 0 -\u0026gt; 4 3 # 2 1 0 2 1 # 0 0 0 image = tf.constant([[[[4],[3]],[[2],[1]]]], dtype=np.float32) pool = keras.layers.MaxPool2D(pool_size=(2,2), strides=1, padding=\u0026#39;SAME\u0026#39;)(image) print(pool.shape) print(pool.numpy()) # header mnist = keras.datasets.mnist class_names = [\u0026#39;0\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;6\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;9\u0026#39;] #mnist = keras.datasets.fashion_mnist #class_names = [\u0026#39;T-shirt/top\u0026#39;, \u0026#39;Trouser\u0026#39;, \u0026#39;Pullover\u0026#39;, \u0026#39;Dress\u0026#39;, \u0026#39;Coat\u0026#39;, \u0026#39;Sandal\u0026#39;, \u0026#39;Shirt\u0026#39;, \u0026#39;Sneaker\u0026#39;, \u0026#39;Bag\u0026#39;, \u0026#39;Ankle boot\u0026#39;] # load data (train_images, train_labels), (test_images, test_labels) = mnist.load_data() # nomalizaiton train_images = train_images.astype(np.float32) / 255. test_images = test_images.astype(np.float32) / 255. # print image 0 img = train_images[0] plt.imshow(img, cmap=\u0026#39;gray\u0026#39;) plt.show() # img redefine img = train_images[0] # img reshape print(\u0026#34;Before reshape\u0026#34;, img.shape) img = img.reshape(-1,28,28,1) print(\u0026#34;After reshape\u0026#34;, img.shape) # imageë¥¼ tensorë¡œ ì •ì˜ # ë²¡í„° ì¶•ì— ëŒ€í•œ í–‰ë ¬ì„ ì¼ë°˜í™” img = tf.convert_to_tensor(img) # ë³€í˜•) Random ëŒ€ì‹  ì§€ì • # ì´ˆê¸° Weight Random ì €ì¥ # weight_init = keras.initializers.RandomNormal(stddev=0.01) # wieght shape 3,3,1,5 weight = np.array([ [ [ [ 1.,0.01,-1.,0.,10.] ], [ [ 1.,0.01,-1.,0.,10.] ], [ [ 1.,0.01,-1.,0.,10.] ] ], [ [ [ 1.,0.01,-1.,0.,10.] ], [ [ 1.,0.01,-1.,0.,10.] ], [ [ 1.,0.01,-1.,0.,10.] ] ], [ [ [ 1.,0.01,-1.,0.,10.] ], [ [ 1.,0.01,-1.,0.,10.] ], [ [ 1.,0.01,-1.,0.,10.] ] ] ]) print(\u0026#34;weight.shape\u0026#34;, weight.shape) weight_init = tf.constant_initializer(weight) # 1X28X28X1 -\u0026gt; 2X2 -\u0026gt; 1X14X14X5 conv2d = keras.layers.Conv2D(filters=5, kernel_size=3, strides=(2, 2), padding=\u0026#39;SAME\u0026#39;, kernel_initializer=weight_init)(img) print(conv2d.shape) # 4ì°¨ì› - 1ì°¨ì› Swap print(\u0026#34;Before swapaxes : \u0026#34;,conv2d.shape) feature_maps = np.swapaxes(conv2d, 0, 3) print(\u0026#34;After swapaxes : \u0026#34;,feature_maps.shape) # Filter ìˆ˜ë§Œí¼ For loop for i, feature_map in enumerate(feature_maps): plt.subplot(1,5,i+1), plt.imshow(feature_map.reshape(14,14), cmap=\u0026#39;gray\u0026#39;) plt.show() # maxpool ìˆ˜í–‰ pool = keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding=\u0026#39;SAME\u0026#39;)(conv2d) print(pool.shape) # swap ì¶• feature_maps = np.swapaxes(pool, 0, 3) # filter ë§Œí¼ for loop for i, feature_map in enumerate(feature_maps): plt.subplot(1,5,i+1), plt.imshow(feature_map.reshape(7, 7), cmap=\u0026#39;gray\u0026#39;) plt.show() # img redefine img = train_images[0] # img reshape print(\u0026#34;Before reshape\u0026#34;, img.shape) img = img.reshape(-1,28,28,1) print(\u0026#34;After reshape\u0026#34;, img.shape) # imageë¥¼ tensorë¡œ ì •ì˜ # ë²¡í„° ì¶•ì— ëŒ€í•œ í–‰ë ¬ì„ ì¼ë°˜í™” img = tf.convert_to_tensor(img) # ì´ˆê¸° Weight Random ì €ì¥ weight_init = keras.initializers.RandomNormal(stddev=0.01) # 1X28X28X1 -\u0026gt; 2X2 -\u0026gt; 1X14X14X5 conv2d = keras.layers.Conv2D(filters=5, kernel_size=3, strides=(2, 2), padding=\u0026#39;SAME\u0026#39;, kernel_initializer=weight_init)(img) print(conv2d.shape) # 4ì°¨ì› - 1ì°¨ì› Swap print(\u0026#34;Before swapaxes : \u0026#34;,conv2d.shape) feature_maps = np.swapaxes(conv2d, 0, 3) print(\u0026#34;After swapaxes : \u0026#34;,feature_maps.shape) # Filter ìˆ˜ë§Œí¼ For loop for i, feature_map in enumerate(feature_maps): plt.subplot(1,5,i+1), plt.imshow(feature_map.reshape(14,14), cmap=\u0026#39;gray\u0026#39;) plt.show() pool = keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding=\u0026#39;SAME\u0026#39;)(conv2d) print(pool.shape) feature_maps = np.swapaxes(pool, 0, 3) for i, feature_map in enumerate(feature_maps): plt.subplot(1,5,i+1), plt.imshow(feature_map.reshape(7, 7), cmap=\u0026#39;gray\u0026#39;) plt.show()      lab-11-1 ...   from __future__ import absolute_import from __future__ import division from __future__ import print_function import tensorflow as tf from tensorflow import keras from tensorflow.keras.utils import to_categorical import numpy as np import matplotlib.pyplot as plt import os print(tf.__version__) print(keras.__version__) # Hyper Parameters ì„¤ì • learning_rate = 0.001 training_epochs = 15 batch_size = 100 tf.random.set_seed(777) # Creating a Checkpoint Directory  cur_dir = os.getcwd() print(cur_dir) ckpt_dir_name = \u0026#39;checkpoints\u0026#39; model_dir_name = \u0026#39;minst_cnn_seq\u0026#39; checkpoint_dir = os.path.join(cur_dir, ckpt_dir_name, model_dir_name) print(checkpoint_dir) os.makedirs(checkpoint_dir, exist_ok=True) checkpoint_prefix = os.path.join(checkpoint_dir, model_dir_name) print(checkpoint_prefix) ## MNIST Dataset ######################################################### mnist = keras.datasets.mnist class_names = [\u0026#39;0\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;6\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;9\u0026#39;] ########################################################################## ## Fashion MNIST Dataset ################################################# #mnist = keras.datasets.fashion_mnist #class_names = [\u0026#39;T-shirt/top\u0026#39;, \u0026#39;Trouser\u0026#39;, \u0026#39;Pullover\u0026#39;, \u0026#39;Dress\u0026#39;, \u0026#39;Coat\u0026#39;, \u0026#39;Sandal\u0026#39;, \u0026#39;Shirt\u0026#39;, \u0026#39;Sneaker\u0026#39;, \u0026#39;Bag\u0026#39;, \u0026#39;Ankle boot\u0026#39;] ########################################################################## # Datasets Load (train_images, train_labels), (test_images, test_labels) = mnist.load_data() # Train Data Normalization train_images = train_images.astype(np.float32) / 255. test_images = test_images.astype(np.float32) / 255. # Train Data ë§ˆì§€ë§‰ ì°¨ì› ì¶”ê°€ print(\u0026#34;Before : \u0026#34;,train_images.shape) train_images = np.expand_dims(train_images, axis=-1) test_images = np.expand_dims(test_images, axis=-1) print(\u0026#34;After : \u0026#34;,train_images.shape) # Label ì¹´í…Œê³ ë¦¬ ì§€ì • # Converts a class vector (integers) to binary class matrix. train_labels = to_categorical(train_labels, 10) test_labels = to_categorical(test_labels, 10) # ì´ë¯¸ì§€ ê²½ë¡œ ë˜ëŠ” RAW ë°ì´í„°ë¥¼ TFë¥¼ ì‚¬ìš©í•´ DataSetìœ¼ë¡œ ì „í™˜ train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(buffer_size=100000).batch(batch_size) test_dataset = tf.data.Dataset.from_tensor_slices((test_images , test_labels )).batch(batch_size) # ëª¨ë¸ ìƒì„± (sequential-eager) def create_model_seq(): model = keras.Sequential() # 28 X 28 X 32 model.add(keras.layers.Conv2D(filters=32, kernel_size=3, activation=tf.nn.relu, padding=\u0026#39;SAME\u0026#39;, input_shape=(28, 28, 1))) model.add(keras.layers.MaxPool2D(padding=\u0026#39;SAME\u0026#39;)) # 14 X 14 X 64 model.add(keras.layers.Conv2D(filters=64, kernel_size=3, activation=tf.nn.relu, padding=\u0026#39;SAME\u0026#39;)) model.add(keras.layers.MaxPool2D(padding=\u0026#39;SAME\u0026#39;)) # 7 X 7 X 128 model.add(keras.layers.Conv2D(filters=128, kernel_size=3, activation=tf.nn.relu, padding=\u0026#39;SAME\u0026#39;)) model.add(keras.layers.MaxPool2D(padding=\u0026#39;SAME\u0026#39;)) # 4 X X 128 -\u0026gt; 2048 model.add(keras.layers.Flatten()) # 256 model.add(keras.layers.Dense(256, activation=tf.nn.relu)) # Dropout 0.4 model.add(keras.layers.Dropout(0.4)) # 10 model.add(keras.layers.Dense(10)) return model def create_model_fuc() : inputs = keras.Input(shape=(28,28,1)) conv1 = keras.layers.Conv2D(filters=32, kernel_size=[3,3],padding=\u0026#39;SAME\u0026#39;, activation=tf.nn.relu)(inputs) pool1 = keras.layers.MaxPool2D(padding=\u0026#39;SAME\u0026#39;)(conv1) conv2 = keras.layers.Conv2D(filters=64, kernel_size=[3,3],padding=\u0026#39;SAME\u0026#39;, activation=tf.nn.relu)(pool1) pool2 = keras.layers.MaxPool2D(padding=\u0026#39;SAME\u0026#39;)(conv2) conv3 = keras.layers.Conv2D(filters=128, kernel_size=[3,3],padding=\u0026#39;SAME\u0026#39;, activation=tf.nn.relu)(pool2) pool3 = keras.layers.MaxPool2D(padding=\u0026#39;SAME\u0026#39;)(conv3) pool3_flat = keras.layers.Flatten()(pool3) dense4 = keras.layers.Dense(units=256, activation=tf.nn.relu)(pool3_flat) drop4 = keras.layers.Dropout(rate=0.4)(dense4) logits = keras.layers.Dense(units=10)(drop4) return keras.Model(inputs=inputs, outputs=logits) #model = create_model_seq() model = create_model_fuc() # Model Summary ì¶œë ¥ model.summary() # tf.function : decorator  # ì¦‰ì‹œ ì‹¤í–‰ì´ ê°€ëŠ¥í•œ Tensorflow Graphë¡œ ì „í™˜ # Loss Function  @tf.function def loss_fn(model, images, labels): logits = model(images, training=True) loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy( y_pred=logits, y_true=labels, from_logits=True)) return loss # Calculating Gradient @tf.function def grad(model, images, labels): with tf.GradientTape() as tape: loss = loss_fn(model, images, labels) return tape.gradient(loss, model.variables) # Calculating Model\u0026#39;s Accuracy @tf.function def evaluate(model, images, labels): logits = model(images, training=False) correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) return accuracy # ADAM Optimizer # Learning_rate = 0.001 optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate) # Creating Checkpoint checkpoint = tf.train.Checkpoint(cnn=model) # Train Function Define @tf.function def train(model, images, labels): grads = grad(model, images, labels) optimizer.apply_gradients(zip(grads, model.trainable_variables)) # Train Model print(\u0026#39;Learning Started...\u0026#39;) # Epochë§Œí¼ for loop for epoch in range(training_epochs): # default setting avg_loss = 0. avg_train_acc = 0. avg_test_acc = 0. train_step = 0 test_step = 0 # train_datasetì—ì„œ for loop for images, labels in train_dataset: # train train(model, images, labels) #grads = grad(model, images, labels)  #optimizer.apply_gradients(zip(grads, model.variables)) # loss loss = loss_fn(model, images, labels) # evaluate acc = evaluate(model, images, labels) # loss cum avg_loss = avg_loss + loss # acc cum avg_train_acc = avg_train_acc + acc # for loop train_step += 1 # avg_loss / avg_train_acc avg_loss = avg_loss / train_step avg_train_acc = avg_train_acc / train_step # test_datasetì—ì„œ for loop for images, labels in test_dataset: # evaluate  acc = evaluate(model, images, labels) avg_test_acc = avg_test_acc + acc test_step += 1 # test acc average avg_test_acc = avg_test_acc / test_step print(\u0026#39;EPCH : \u0026#39;, \u0026#39;{}\u0026#39;.format(epoch + 1), \u0026#39;| LOSS : \u0026#39;, \u0026#39;{:.8f}\u0026#39;.format(avg_loss), \u0026#39;| TR_ACC : \u0026#39;, \u0026#39;{:.4f}\u0026#39;.format(avg_train_acc), \u0026#39;| TS_ACC : \u0026#39;, \u0026#39;{:.4f}\u0026#39;.format(avg_test_acc)) checkpoint.save(file_prefix=checkpoint_prefix) print(\u0026#39;Learning Finished...\u0026#39;)     "});index.add({'id':28,'href':'/docs/documents/business/','title':"Business",'section':"Documents",'content':"Business #     Supply Chain Management   Description   "});index.add({'id':29,'href':'/docs/documents/english/','title':"English",'section':"Documents",'content':"English #     Seulsam   English Expression from SeulSam   Picked Up English   Picked up english expressions here and there from web sites   "});index.add({'id':30,'href':'/docs/documents/english/seulsam/','title':"Seulsam",'section':"English",'content':"English Expression Diary - êµ¬ìŠ¬ìŒ¤ì˜ ì‰¬ìš´ ë‹¨ì–´ë¡œ ë„¤ì´í‹°ë¸Œì²˜ëŸ¼ ë§í•˜ê¸° #   êµ¬ìŠ¬ìŒ¤ì˜ ì‰¬ìš´ ë‹¨ì–´ë¡œ ë„¤ì´í‹°ë¸Œì²˜ëŸ¼ ë§í•˜ê¸°ë¥¼ ì •ë¦¬\nI find it doable #  í• ë§Œí•´.\nI find it challenging #  í˜ë“¤ê¸´í•˜ì§€ë§Œ í•´ë³¼ë§Œí•´.\nDid you eat at all? #  ë­ ì¢€ ë¨¹ì—ˆë‹ˆ?\nIf you need anthing at all, please let me know. #  ë­ë¼ë„ í•„ìš”í•˜ì‹œë©´ ì•Œë ¤ì£¼ì„¸ìš”.\nI\u0026rsquo;m in the middle of a meeting. Can I call you back in just a minute? #  í•œì°¸ íšŒì˜ ì¤‘ì¸ë° ì¢€ìˆë‹¤ ê±¸ì–´ë„ ë ê¹Œ?\nI\u0026rsquo;m at work. #  ë‚œ ê·¼ë¬´ì¤‘ì´ì•¼.\nYou made my day. #  ë•ë¶„ì— ê¸°ë¶„ ì¢‹ì•„.\nYou don\u0026rsquo;t appreciate good man. #  ì¢‹ì€ ë‚¨ìì˜ ì§„ê°€ë¥¼ ëª°ë¼ë³´ë‚´.\nI appreciate it. Of course #  ë„¤ ë•ë¶„ì´ì•¼. ì²œë§Œì—ìš”. (?)\nThank you. No, thank YOU. #  ê³ ë§ˆì›Œìš”. ì²œë§Œì—ìš”.\nHow old do you think she is? I think.. she\u0026rsquo;s 30-ish. #  ê·¸ ì—¬ìê°€ ëª‡ì‚´ì¸ê±° ê°™ë‹ˆ. ë‚´ ìƒê°ì—” ëŒ€ëµ 30ì‚´ì •ë„?\nThere\u0026rsquo;s always room for improvement. #  ëŠ˜ ê°œì„ ì˜ ì—¬ì§€ê°€ ìˆì–´ìš”.\nI\u0026rsquo;m so flattered. #  ê³¼ì°¬ì´ì„¸ìš”.\nThank you for your compliment. #  ì¹­ì°¬ ê°ì‚¬í•©ë‹ˆë‹¤.\nI hope you saved room for dessert. #  ë””ì €íŠ¸ ë¨¹ì„ ìë¦¬ ë‚¨ê²¨ ë†“ì•˜ê¸¸ ë°”ë˜ìš”.\nDo you have enough room? #  ìë¦¬ ê³µê°„ ì¶©ë¶„í•˜ì„¸ìš”?\nThere\u0026rsquo;s no room for mistakes. #  ì‹¤ìˆ˜í•  ì—¬ìœ  ì—†ì–´\nThere\u0026rsquo;s no room for delay. #  ì§€ì²´í•  ì—¬ìœ  ì—†ì–´.\nI coundn\u0026rsquo;t have done it without you. #  ì˜ ëœê±´ ë‹¤ ë„¤ ë•ë¶„ì´ì•¼.\nI have so many things to do. I\u0026rsquo;m overwhelmed. #  í•´ì•¼ í• ê²Œ ë„ˆë¬´ ë§ì•„. ì¢€ ë²…ì°¨.\nIt\u0026rsquo;s overwhelming. #  ê°ì •ì´ ë²…ì°¨ì˜¤ë¥¸ë‹¤.\nLet us not take this planet for granted. #  ì§€êµ¬ë¥¼ ë‹¹ì—°í•œ ê²ƒìœ¼ë¡œ ìƒê°í•˜ì§€ ë§ì.\nThis looks pretty legit. #  ì´ê±° ê½¤ ê´œì°®ì•„ ë³´ì¸ë‹¤.\nAt least it was cheap. #  ìµœì†Œí•œ ì‹¸ê¸°ë¼ë„ í–ˆë‹¤.\na variety of affordable options. #  ë‹¤ì–‘í•œ í•©ë¦¬ì ê°€ê²©ì˜ ì˜µì…˜ë“¤.\nShe is smart with her money. #  ê·¸ë…€ëŠ” í•©ë¦¬ì ì¸ ì†Œë¹„ìë‹¤.\nYou\u0026rsquo;re acting/beging weird. #  ë„ˆ ì˜¤ëŠ˜ í–‰ë™ì´ ì´ìƒí•´. (ë„ˆ ì˜¤ëŠ˜ ì¢€ ì´ìƒí•´)\nStop being such a neat freak. #  ì¢€ ê¹”ë”ì¢€ ê³ ë§Œ ë–¨ì–´.\nPlease stop being so bossy. #  ìƒì‚¬ì§ˆ ì¢€ ê³ ë§Œ í•´ë¼.\nI\u0026rsquo;m freaking out. / Don\u0026rsquo;t freak out. #  ì—„ì²­ ë‹¹í™©í–ˆì–´. / ë‹¹í™©í•˜ì§€ ë§ˆë¼.\nI almost freaked out. #  ì—„ì²­ ë‹¹í™©í• ë»” í–ˆì–´.\nPlease bear with me. #  ì¡°ê¸ˆ ì°¸ê³  ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.\nIt\u0026rsquo;s that easy. #  ê·¸ë ‡ê²Œë‚˜ ì‰¬ì›Œìš”.\nIt\u0026rsquo;s not that easy. #  ê·¸ë ‡ê²Œ ì‰½ì§€ ì•Šì•„ìš”.\nI owe you a drink. #  ìˆ í•œì” ì‚´ê»˜ìš”.\nI owe you an apology. #  ë‚´ê°€ ì‚¬ê³¼í•´ì•¼ë ê±° ê°™ì•„ìš”.\nI owe you an explanation. #  ë‚´ê°€ ì„¤ëª…ë“œë ¤ì•¼ ë ê±° ê°™ì•„ìš”.\nLet\u0026rsquo;s hit the road. #  ë¹¨ë¦¬ ê°€ì.\nShe\u0026rsquo;s expecting. #  ê·¸ë…€ëŠ” ì¶œì‚° ì˜ˆì •ì´ë‹¤.\nI expect it done by the morning. #  ë‚´ì¼ ì•„ì¹¨ê¹Œì§€ ëë‚¼ì§€ ë‘ê³  ë³¼êº¼ì•¼.\nI look forward to seeing you tomorrow. #  ë‚´ì¼ ë„ ë³¼ê±¸ ê¸°ëŒ€í•˜ê³  ìˆì–´.\nI am looking forward to seeing you tomorrow. #  ë‚´ì¼ ë„ ë³¼ê±¸ ì§€ê¸ˆ ëª¹ì‹œ ê¸°ëŒ€í•˜ê³  ìˆì–´.\nI\u0026rsquo;ve been looking forward to seeing you tomorrow. #  ë‚´ì¼ ë„ ë³¼ê±¸ ì „ë¶€í„° ì­‰ ëª¹ì‹œ ê¸°ëŒ€í•˜ê³  ìˆì–´.\nI can\u0026rsquo;t wait to see you. #  ë„ ë³´ëŠ”ê²Œ ë¬´ì²™ ê¸°ë‹¤ë ¤ ì§„ë‹¤.\nI can\u0026rsquo;t wait for tonight. #  ì˜¤ëŠ˜ì´ ë¬´ì²™ ê¸°ë‹¤ë ¤ ì§„ë‹¤.\nI know. #  ê·¸ëŸ¬ê²Œ.\nYou know. #  ìˆì–ì•„.\nI mean #  ë‚´ ë§ì€, ë‹¤ì‹œ ë§í•˜ë©´.\nI\u0026rsquo;m telling you #  ì§„ì§œì•¼.\nCan you pick me up at 9:45 sharp. #  9:45ì— ë”± ë§ì¶° íƒœì›Œ ì¤„ ìˆ˜ ìˆìŒ?\nPicked you up some coffee. #  ì»¤í”¼ ì¢€ ì‚¬ë‹¤ ì¤„ê¹Œ?\nDid you pick up on that? #  ë„ˆ ëˆˆì¹˜ ì±˜ë‹ˆ?\nHow did you not pick up on that? #  ì–´ë–»ê²Œ ê·¸ê±¸ ëˆˆì¹˜ ëª»ì±ˆê±°ì•¼?\nWhen did you pick up on that? #  ê·¸ê±° ì–¸ì œ ëˆˆì¹˜ ì±˜ì–´?\nI picked up some Japanese. #  ë‚œ ì¼ë³¸ì–´ë¥¼ ì–´ê¹¨ ë„ˆë¨¸ë¡œ ë°°ì› ì–´.\nI did pick up some Japanes during my high-school days. #  ê³ ë“±í•™êµ ì‹œì ˆì— ì¼ë³¸ì–´ë¥¼ ì–´ê¹¨ë„˜ì–´ë¡œ ë°°ì› ì–´.\nWell, it\u0026rsquo;s sort of a bad habit I picked up being a consultant. #  ì»¨ì„¤í„´íŠ¸ ì‹œì ˆì— ë°°ì› ë˜ ì•ˆì¢‹ì€ ìŠµê´€ì´ì£ .\nOkay, let\u0026rsquo;s call it a day. #  ì˜¤ëŠ˜ì€ ì—¬ê¸°ê¹Œì§€ í•˜ì.\nWrap it up. #  ì •ë¦¬ í•˜ì.\nI need to run an errand. / I need to run som errands. #  ë³¼ì¼ì´ ì¢€ ìˆì–´.\nLast but not least, #  ë§ˆì§€ë§‰ìœ¼ë¡œ, í•˜ì§€ë§Œ ì•ì„œ ë§í•œê²ƒ ì²˜ëŸ¼ ì¤‘ìš”í•œ.\nLet me make it up to you. #  ë‹¹ì‹ ì—ê²Œ ë³´ë‹µí•  ê¸°íšŒë¥¼ ì¤˜ìš”.\nCoffee is my treat. I wanna make it up to you. #  ì»¤í”¼ëŠ” ë‚´ê°€ ì‚´ê²Œ. í™” í’€ì–´ì£¼ê³  ì‹¶ì–´ì„œ ê·¸ë˜.\nA six-figure income #  ì–µëŒ€ ì—°ë´‰\nGive me a ballpark figure. How much? #  ì–´ë¦¼ì¹˜ë¡œë¼ë„ ì–˜ê¸°í•´ì¤˜. ì–¼ë§ˆì§€?\nDon\u0026rsquo;t worry about the money. I\u0026rsquo;ll figure something out. #  ëˆì€ ê±±ì •í•˜ì§€ë§ˆ. ì–´ë–»ê²Œë“  ë°©ë²•ì„ ì°¾ì•„ë³¼ê²Œ.\nI\u0026rsquo;m struck in traffc. #  ì°¨ê°€ ë§‰í˜€ ê¼¼ì§ ëª»í•´.\nI need to take a rain check. I\u0026rsquo;m stuck at work. #  ë‹¤ìŒìœ¼ë¡œ ë¯¸ë¤„ì•¼ í•  ê²ƒ ê°™ì•„. íšŒì‚¬ì— ì¼ì´ ë§ì•„ì„œ ê¼¼ì§ëª»í•´.\nI wanna say thanks for doing this. #  ì´ê±¸ í•´ì¤˜ì„œ ê³ ë§™ë‹¤ê³  ë§í•˜ê³  ì‹¶ì–´.\nLet\u0026rsquo;s grab lunch sometime. #  ì–¸ì œ ì ì‹¬ì´ë¼ë„ ê°™ì´ í•˜ì.\nLet me go grab my phone. #  í•¸ë“œí° ì¢€ ê°€ì ¸ì˜¬ê»˜.\nDo you want some company. / (I\u0026rsquo;d) love some company. #  ê°™ì´ ê°€ì¤„ê¹Œ? / ì¢‹ì§€.\nI enjoy your company. #  ê°™ì´ ìˆì–´ì„œ ì¦ê±°ì› ì–´.\nI don\u0026rsquo;t mean to be late, but I\u0026rsquo;m stuck in traffic. #  ëŠ¦ê³  ì‹¶ì§€ ì•Šì€ë° ì°¨ê°€ ë§‰íˆê³  ìˆì–´.\nI didn\u0026rsquo;t mean to be late, but I was stuck in traffic. #  ëŠ¦ê³  ì‹¶ì§€ ì•Šì•˜ëŠ”ë° ì°¨ê°€ ë§‰í˜”ì–´.\nI don\u0026rsquo;t mean to interrupt, but I have extremely important news. #  ë°©í•´í•˜ê³  ì‹¶ì§€ ì•Šì§€ë§Œ ì •ë§ ì¤‘ìš”í•œ ë‰´ìŠ¤ê°€ ìˆì–´.\nThis can wait. Let\u0026rsquo;s go get a drink. #  ì´ê±° ë‚˜ì¤‘ì— í•´ë„ ë¼. ê°€ì„œ ìˆ ì´ë‚˜ í•œì” í•˜ì.\nCan it wait? #  ê·¸ê±° ì´ë”° í•´ë„ ë¼?\nHow did it go? They both went well. Too well. #  ì–´ë–»ê²Œ ë˜ê³  ìˆì–´? ë‘ê°œë‹¤ ì˜ ë¼. ë„ˆë¬´ ì˜ ë¼.\nHow did it go? It went well. / It didn\u0026rsquo;t go well. #  ì–´ë–»ê²Œ ë¼ê³  ìˆì–´? ì˜ ì§„í–‰ ëì–´. ì˜ ì§„í–‰ë˜ì§€ ì•Šì•˜ì–´.\nDid you go through every single page? #  í•˜ë‚˜ë„ ë¹ ì§ ì—†ì´ ì˜ ì²´í¬í•´ë´¤ì–´?\nI\u0026rsquo;m sorry you had to go through that. #  ê·¸ê±¸ ê²ªê²Œ ë˜ì–´ì„œ ìœ ê°ì´ì•¼.\nSorry you had to go through that. #  ë„¤ê°€ ê·¸ëŸ° ì¼ì„ ê²ªì–´ì•¼ í–ˆë‹¤ë‹ˆ ìœ ê°ì´ì•¼.\nShe\u0026rsquo;s going through a lot. #  ê·¸ë…€ëŠ” ë§ì€ ì¼ë“¤ì„ ê²ªê³  ìˆì–´.\nThe payment didn\u0026rsquo;t go through. #  ê²°ì œê°€ ë„˜ì–´ê°€ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\nI\u0026rsquo;ll go with a latte. #  ë¼ë–¼ë¡œ í•˜ê² ìŠµë‹ˆë‹¤.\nGo on. #  ê³„ì†í•´.\nWhere does this part go? #  ì´ê±´ ë¶€í’ˆì€ ì–´ë”” ë“¤ì–´ê°€ì§€?\nCould you look over this email? #  ì´ ì´ë©”ì¼ ì¢€ ë´ì¤„ ìˆ˜ ìˆì–´?\nWhy donâ€™t you look over some of our materials? #  ìš°ë¦¬ ìë£Œ ì¢€ ë´ì£¼ì§€ ì•Šê² ì–´?\nListen, did you get a chance to look over that contract for me? #  í˜¹ì‹œ ë‚´ê°€ ë³´ë‚¸ ê³„ì•½ì„œ í›‘ì–´ ë³¼ ì‹œê°„ ìˆì—ˆì–´?\nLetâ€™s go over this quickly. #  ì´ê±¸ ë¹ ë¥´ê²Œ í›‘ì–´ ë´…ì‹œë‹¤.\nWe went over that yesterday. #  ìš°ë¦¬ ì–´ì œ ê·¸ê±° í›‘ì–´ ë´¤ì–´.\nExecuse me, we just went over this. #  ì‹¤ë¡€í•˜ì§€ë§Œ, ìš°ë¦¬ ìš°ë¦¬ ì´ê±° ë°©ê¸ˆ ë‹¤ë¤˜ì—ˆìë‚˜?\nOkay, letâ€™s go over the instructions. #  ì¢‹ì•„, ì„¤ëª…ì„œë¥¼ ì°¨ê·¼ì°¨ê·¼ ì½ì–´ ë³´ì.\nSo, letâ€™s go over the plan again. #  ê·¸ë˜ì„œ ë‹¤ì‹œ ê³„íšì„ ì¢€ ë” ì‚´í´ ë³´ì.\nAnd yes, Iâ€™m thorough. #  ê·¸ë˜ ë‚˜ ê¼¼ê¼¼í•´.\nWe went over that thoroughly. #  ìš°ë¦¬ëŠ” ê·¸ê±¸ ê¼¼ê¼¼í•˜ê²Œ ì‚´í´ë³´ê² ë‹¤.\nAnd you donâ€™t look a day over 25. #  ê·¸ë¦¬ê³  ë‹¹ì‹ ì€ 25ì„ ë„˜ì–´ ë³´ì´ì§€ ì•Šì•„.\nIt was an honest mistake. #  ì˜ë„ì¹˜ ì•Šì€ ì‹¤ìˆ˜ì˜€ì–´.\nThat\u0026rsquo;s a pretty solid idea. #  ê½¤ ê´œì°®ì€ ìƒê°ì¸ê±¸.\nThanks for the heads-up. #  ë¯¸ë¦¬ ê²½ê³  í•´ì¤˜ì„œ ê³ ë§ˆì›Œ.\nIâ€™m at working right now. #  ì§€ê¸ˆ ê·¼ë¬´ì¤‘ì´ì•¼.\nThis is a heads-up. You got visitors. #  ì´ê±´ ê²½ê³ ì•¼. ë°©ë¬¸ê°ì´ ìˆì„ê±°ì•¼.\nHappy Hump day. #  ìˆ˜ìš”ì¼ ê³ ë¹„ë¥¼ ì˜ ë„˜ê²¨ë³´ì.\nGuess what day it is. Itâ€™s hump day. #  ì˜¤ëŠ˜ ë¬´ìŠ¨ ë‚ ì¸ì§€ ë§ì¶° ë´. ì˜¤ëŠ˜ ìˆ˜ìš”ì¼ì´ì•¼.\nRSVP (please respond) #  ì°¸ì„ì—¬ë¶€ íšŒì‹ ì„ ë¶€íƒë“œë¦½ë‹ˆë‹¤.\nBut unfortunately. I donâ€™t think I can make it. #  ì € ê°€ë ¤ê³  í–ˆëŠ”ë° ëª» ê°ˆê±° ê°™ì•„ìš”.\nI canâ€™t make it. #  ëª» ê°€ìš”\nI canâ€™t go. #  ì € ëª»ê°€ìš”.\nNo RSVP required. Just be there. #  ì°¸ì„ì—¬ë¶€ ì—†ì´ ê·¸ëƒ¥ ì°¸ì„í•˜ì„¸ìš”.\nI got you something. #  ë„ ìœ„í•´ ë­ì¢€ ê°€ì ¸ì™”ì–´.\nI got you some coffee. #  ì»¤í”¼ ì¢€ ì‚¬ì™”ì–´.\nI got lattes for the entire office. #  ì‚¬ë¬´ì‹¤ ì „ì²´ë¥¼ ìœ„í•´ ë”ë° ì¢€ ì‚¬ì™”ì–´ìš”.\nYou should get some sleep. #  ì  ì¢€ ìëŠ”ê²Œ ì¢‹ê² ë‹¤.\nYou should get some rest. #  ì¢€ ì‰¬ëŠ”ê²Œ ì¢‹ê² ë‹¤.\nMy coffee is getting cold. #  ë‚´ ì»¤í”¼ê°€ ì ì  ì‹ì–´ê°€ë„¤.\nMy coffee got cold. #  ë‚´ ì»¤í”¼ê°€ ì‹ì—ˆì–´.\nYour food getting cold. What are you doing? #  ìŒì‹ì´ ì‹ì–ì•„. ë­ í•˜ëŠ” ê±°ì•¼?\nI hope you get better soon. #  ë‚˜ì•„ì§€ê¸¸ ë°”ë„ê²Œ.\nGet well soon #  ê³§ ë‚˜ì•„ì§€ê¸¸ ë°”ë„ê»˜.\nCall me when you get to work. #  íšŒì‚¬ì— ë„ì°©í•˜ë©´ ì „í™”í•´ì¤˜\nDid you get home safely? #  ì§‘ì— ì˜ ë„ì°©í–ˆì–´?\nGet home safely. #  ì¡°ì‹¬íˆ ë“¤ì–´ê°€.\nGet home safe. #  ì¡°ì‹¬íˆ ë“¤ì–´ê°€.\nI got it. #  ì´í•´í–ˆë‹¤.\nI\u0026rsquo;ll get her to send you the info. #  ê·¸ë…€í•œí…Œ ë„¤ê²Œ ë¬¸ìë¡œ ì •ë³´ ë³´ë‚´ì£¼ë¼ê³  í• ê²Œ.\nI got him to quit smoking. #  ê·¸ê°€ ê¸ˆì—°í•˜ê²Œë” ì„¤ë“í•˜ê² ì–´.\nI\u0026rsquo;ll get her to call you right away. #  ê·¸ë…€ ë³´ê³  ì§€ê¸ˆ ë‹¹ì¥ ì „í™”ë“œë¦¬ë¼ê³  í• ê»˜ìš”.\nAnd get her to call me on this. #  ê·¸ë¦¬ê³  ê·¸ë…€ê°€ ì´ê±¸ë¡œ ì „í™”í•˜ê²Œë” ì„¤ë“í•´.\nDid you get my email? #  ë‚´ ì´ë©”ì¼ ë°›ì•˜ì–´?\nDid you get my text? / I got your text. #  ë‚´ ë¬¸ì ë°›ì•˜ì–´? ë°›ì•˜ì–´.\nI just got a call from corporate. #  ë°©ê¸ˆ ë³¸ì‚¬ë¡œë¶€í„° ì „í™” ë°›ì•˜ì–´.\nI didn\u0026rsquo;t see this coming. #  ì´ë ‡ê²Œ ë ì¤„ ëª°ëì–´.\nSo what was your takeaway? #  ì˜¤ëŠ˜ ë‹ˆê°€ ë°°ìš´ì ì€ ë­ë‹ˆ?\nShall we gat a takeaway? #  í¬ì¥ ìŒì‹ ì–´ë•Œ?\nCan you come up with a plan by tomorrow? #  ë‚´ì¼ê¹Œì§€ ê³„íšŒ ì¢€ ì¤€ë¹„ í•´ì¤„ë˜?\nI came up with a strategy. #  ë‚´ê°€ ì „ëµì„ ë§ˆë ¨í–ˆì–´.\nHow did you come up with the rent? #  ì›”ì„¸ë¥¼ ì–´ë–»ê²Œ ë§ˆë ¨í–ˆë‹ˆ?\nMaybe come up with a better plan? #  ì¢€ë” ë‚˜ì€ ê³„íšì„ ì¤€ë¹„í•´ì¤„ ìˆ˜ ìˆì–´?\nI\u0026rsquo;m putting together a team. #  ë‚˜ëŠ” íŒ€ì›ì„ ëª¨ìœ¼ëŠ” ì¤‘ì´ë‹¤.\nCan you put together a presentation? #  í”„ë ˆì  í…Œì´ì…˜ ì¤€ë¹„í•´ì¤„ ìˆ˜ ìˆì–´?\nCould you put together a presentation for tomorrow? #  ë‚´ì¼ í”„ë ˆì  í…Œì´ì…˜ ì¢€ ì¤€ë¹„í•´ ì¤„ ìˆ˜ ìˆì–´?\nSo I\u0026rsquo;ve put together a marketing campaign. #  ê·¸ë˜ì„œ ë‚´ê°€ ë§ˆì¼€íŒ… ìº í˜ì¸ì„ ì¤€ë¹„í–ˆì–´.\nIt was worth it. #  ê°€ì¹˜ê°€ ìˆì—ˆë‹¤.\nDoes tomorrow work for you? #  ë‚´ì¼ ì‹œê°„ ë¼?\nDo you work tomorrow? #  ë„ˆ ë‚´ì¼ ê·¼ë¬´ í•´?\nDoes Friday work for you? #  ê¸ˆìš”ì¼ì— ì‹œê°„ ë¼?\nDo you work on Friday? #  ë„ˆ ê¸ˆìš”ì¼ì— ê·¼ë¬´í•´?\nI don\u0026rsquo;t think the wi-fi is working. #  ì™€ì´íŒŒì´ê°€ ì•ˆë˜ëŠ” ê²ƒ ê°™ì•„ìš”.\nI don\u0026rsquo;t think my lap-top is working. #  ë©íƒ‘ì´ ì‘ë™í•˜ì§€ ì•ŠëŠ”ê±° ê°™ì•„ìš”.\nI\u0026rsquo;ll keep you posted #  ì–´ë–»ê²Œ ì§„í–‰ë˜ëŠ”ì§€ ì•Œë ¤ì¤„ê»˜\nKeep me posted #  ì–´ë–»ê²Œ ì§„í–‰ ë˜ëŠ”ì§€ ì•Œë ¤ì¤˜\nCan you move up the meeting? #  ë¯¸íŒ… ì¼ì •ì¢€ ë‹¹ê²¨ì¤„ ìˆ˜ ìˆì–´?\nLet\u0026rsquo;s move on to the next topic. #  ë‹¤ìŒ ì£¼ì œë¡œ ë„˜ì–´ê°‘ì‹œë‹¤.\nLet\u0026rsquo;s move on to the next presentation. #  ë‹¤ìŒ í”„ë ˆì  í…Œì´ì…˜ìœ¼ë¡œ ë„˜ì–´ê°‘ì‹œë‹¤.\nCan we move on to the next question? #  ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ë„˜ì–´ ê°ˆê¹Œìš”?\nI need to move on. #  ë‚œ ìƒˆì¶œë°œì´ í•„ìš”í•´.\nLet\u0026rsquo;s pick this up tomorrow. #  ë‚´ì¼ ê³„ì†í•˜ì.\nLet\u0026rsquo;s pick this up after lunch. #  ì ì‹¬ ë¨¹ê³  ê³„ì†í•˜ì.\nLet\u0026rsquo;s call it a day and pick this up tomorrow. #  ì˜¤ëŠ˜ì€ ì´ì¯¤ í•˜ê³  ë‚´ì¼ ë‹¤ì‹œ í•˜ì.\nI\u0026rsquo;ll go pick up some coffee. #  ì»¤í”¼ ì¢€ ì‚¬ì˜¬ê»˜.\nPicked you up some coffe. #  ì»¤í”¼ ì¢€ ì‚¬ì™”ì–´.\nAlso, I got you this coffee. #  ë˜í•œ, ì—¬ê¸° ì»¤í”¼ ì¢€ ì–»ì–´ ì™”ì–´.\nPlease let me know at your earliest convenience. #  í¸í•˜ì‹¤ ë•Œ ë§ì”€ ë¶€íƒë“œë¦½ë‹ˆë‹¤.\nPlease let me know A.S.A.P #  ìµœëŒ€í•œ ë¹¨ë¦¬ ë§ì”€ ë¶€íƒë“œë¦½ë‹ˆë‹¤.\nPlease let me know by Friday at the latest. #  ëŠ¦ì–´ë„ ê¸ˆìš”ì¼ê¹Œì§€ ë§ì”€ ë¶€íƒë“œë¦½ë‹ˆë‹¤.\nSign off on #  ìŠ¹ì¸í•˜ë‹¤\nWrap up #  ë§ˆë¬´ë¦¬í•˜ë‹¤\nBring up #  ì–¸ê¸‰í•˜ë‹¤\nWork on #  ê³µë“¤ì´ë‹¤.\nForward #  ì „ë‹¬í•˜ë‹¤\nShe is turning 50 this year. #  ê·¸ë…€ëŠ” ì˜¬í•´ 50ì‚´ì´ ë¼.\nI\u0026rsquo;m turning 35 this year. #  ë‚˜ëŠ” ì˜¬í•´ 35ì´ ë¼.\nI turned 35 this year. #  ë‚˜ëŠ” ì˜¬í•´ ì„œë¥¸ 35ì´ ëì–´.\nMy oldest just turned 30. #  í°ì• ê°€ ë§‰ 30ì´ ëì–´.\nNo, it\u0026rsquo;s my turn. #  ì•„ë‹™ë‹ˆë‹¤. ì œ ì°¨ë¡€ì…ë‹ˆë‹¤.\nThings will turn around. #  ìƒí™©ì´ ë‚˜ì•„ì§ˆê±°ì•¼.\nThings will work out. #  ë‹¤ ì˜ í’€ë¦´ ê±°ì•¼.\nHow did it go? It went well. #  ì–´ë–»ê²Œ ëì–´? ì˜ ëì–´.\nSo how did the big meeting turn out? #  ê·¸ ë¯¸íŒ… ê²°ê³¼ëŠ” ì–´ë• ì–´?\nTurns out it was for me. #  ê·¸ê±° ë‚´ ì·¨í–¥ì´ë”ë¼\nTurns out it was perfect for me. #  ê·¸ê±° ì™„ì „ ë‚´ ì·¨í–¥ì´ë”ë¼.\nTurns out it wasn\u0026rsquo;t for me. #  ê·¸ê±´ ë‚´ ì·¨í–¥ì´ ì•„ë‹ˆë”ë¼.\nIt might turn into a great opportunity. #  ì¢‹ì€ ê¸°íšŒê°€ ë ì§€ë„ ëª°ë¼.\nIt turned into a nightmare. #  ìµœì•…ì˜ ìƒí™©ì´ ëì–´.\nI\u0026rsquo;ll turn it into something new. #  ë­”ê°€ ìƒˆë¡œìš´ê±¸ë¡œ ë°”ê¿”ì•¼ ê² ë‹¤.\nThis is turning into a nightmare. #  ì´ê±´ ì§€ê¸ˆ ì•…ëª½ì´ ë˜ê³  ìˆëŠ” ì¤‘ì´ì•¼.\nCan you help me turn on the air? #  ì—ì–´ì»¨ ì¢€ ì¼œëŠ”ê²ƒì¢€ ë„ì™€ì¤„ë˜?\nI can turn off the air. #  ì—ì–´ì»¨ êº¼ì¤„ ìˆ˜ ìˆì–´.\nIt\u0026rsquo;s cold. Turn up the heat. #  ì¶”ì›Œ. íˆí„° ì¢€ ì¼œ.\nI\u0026rsquo;d like to say thank you, but I\u0026rsquo;m gonna have to turn your offer down. #  ê³ ë§™ë‹¤ê³  ë§ì”€ë“œë¦¬ê³  ì‹¶ì§€ë§Œ ì œì•ˆì€ ê±°ì ˆ ë“œë ¤ì•¼í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤.\nI\u0026rsquo;m gonna have to take a rain check. #  ì—°ê¸°ë¥¼ í•´ì•¼í•  ê²ƒ ê°™ì•„ìš”.\nI\u0026rsquo;m gonna have to say no. #  ì•ˆ ëœë‹¤ê³  ë§ì”€ ë“œë ¤ì•¼ í•  ê²ƒ ê°™ì•„ìš”.\nDon\u0026rsquo;t forget to take your medicine. #  ì•½ ë¨¹ëŠ”ê±° ìŠì§€ ë§ˆë¼.\nTake your vitamin. It\u0026rsquo;s the last one. #  ë¹„íƒ€ë¯¼ ë¨¹ì–´ë¼. ë§ˆì§€ë§‰ì´ì•¼.\nI\u0026rsquo;ll take a cab. #  íƒì‹œ íƒˆê±°ì•¼.\nExecuse me, Could you take a picture of us? #  ì‹¤ë¡€ì§€ë§Œ, ìš°ë¦¬ ì‚¬ì§„ ì¢€ ì°ì–´ ì¤„ë˜?\nTake a coffee break. #  ì»¤í”¼ íƒ€ì„ ê°–ì.\nWe should take a coffee break. Let\u0026rsquo;s all get coffee, okay? #  ìš°ë¦¬ ì»¤í”¼ë§ˆì‹œë©´ì„œ ì‰¬ì. ëª¨ë‘ ì»¤í”¼ ê°€ì ¸ì˜¤ëŠ”ê±°ì•¼. OK?\nAll you guys, take a lunch break. #  ë„ˆí¬ë“¤ ëª¨ë‘ ì ì‹¬ íœ´ì‹ì´ì•¼.\nLet\u0026rsquo;s pick this up after lunch. #  ì ì‹¬ ë¨¹ê³  ê³„ì†í•˜ì.\nGood things take time. #  ì¢‹ì€ ê±´ ì‹œê°„ì´ ê±¸ë¦¬ê¸° ë§ˆë ¨ì´ë‹¤.\nHow long would it take? #  ì–¼ë§ˆë‚˜ ì˜¤ë˜ ê±¸ë¦´ê¹Œìš”?\nI\u0026rsquo;ll take it from here. #  ì—¬ê¸°ì„œë¶€í„´ ì œê°€ ë§¡ì„ê»˜ìš”.\nWhy don\u0026rsquo;t I take you out to lunch? My treat. #  ë‚´ê°€ ì ì‹¬ì‚´ê¹Œ? ë‚´ê°€ ì‚´ê»˜.\nI\u0026rsquo;d like to take you out for coffee sometime. #  ì–¸ì œ ì‹œê°„ë ë•Œ ì»¤í”¼í•œì” ì‚¬ê³  ì‹¶ì–´ìš”.\nCould I take you out for coffee sometime? #  ì–¸ì œ ì œê°€ ì»¤í”¼í•œì” ì‚´ ìˆ˜ ìˆì„ê¹Œìš”?\nI\u0026rsquo;ll get the takeout menus. #  í¬ì¥ìŒì‹ ë©”ë‰´ ê°€ì ¸ì˜¬ê»˜.\nI take full responsibility. #  ë‚´ê°€ ì „ì ìœ¼ë¡œ ì±…ì„ì„ ì§€ê² ì–´.\nI can\u0026rsquo;t help but feel partially responsible. #  ì±…ì„ì´ ì¼ë¶€ë¶„ ìˆë‹¤ëŠ” ëŠë‚Œì„ ì§€ìš¸ ìˆ˜ ì—†ë„¤.\nTake responsibility for your actions. #  í–‰ë™ì— ì±…ì„ì„ ì ¸ì•¼í•œë‹¤.\nTake responsibility for your mistakes. #  ë„ˆì˜ ì‹¤ìˆ˜ì— ì±…ì„ì„ ì ¸ì•¼í•œë‹¤.\nI just wanna take advantage of this opportunity. #  ê·¸ëƒ¥ ì´ ê¸°íšŒë¥¼ ì˜ í™œìš©í•´ë³´ê³  ì‹¶ì–´.\nDon\u0026rsquo;t take advantage of me. #  ë‚˜ë¥¼ ì•…ìš©í•˜ì§€ ë§ˆ.\nWe don\u0026rsquo;t wann take advantage of your hospitality. #  ë‹¹ì‹ ì˜ í˜¸ì˜ë¥¼ ë„ˆë¬´ ì´ìš©í•˜ê³  ì‹¶ì§€ ì•ŠìŠµë‹ˆë‹¤.\nCan you take off your shoes, please? #  ì‹ ë°œ ì¢€ ë²—ì–´ ì£¼ì‹œê² ì–´ìš”?\nDo you want me to take off my shoes? #  ì œê°€ ì‹ ë°œì„ ë²—ê¸¸ ì›í•˜ì‹œë‚˜ìš”?\nSorry guys, but I gotta take off. #  ë‚˜ ì§€ê¸ˆ ê°€ë´ì•¼ ë¼.\nI gotta go. #  ê°€ë´ì•¼ë¼.\nI gotta run. #  ê°€ë´ì•¼ë¼.\nI am just frustrated. #  ë‚˜ ë‹µë‹µí•´.\nThis is so frustrating. #  ì´ê±° ì •ë§ ì§œì¦ë‚˜ëŠ”ê±¸.\nI am so embarrassed. #  ì´ê±° ì •ë§ ë¯¼ë§í•´\nThis is so embarrassing. #  ì´ê±° ë¯¼ë§í•œê±¸.\nHe is embarrassed #  ê·¸ëŠ” ë¯¼ë§í•´í–ˆë‹¤.\nHe is embarrassing. #  ê·¸ëŠ” ë¯¼ë§í•œ ì‚¬ëŒì„.\nDevastated, simply devastated. #  ì¶©ê²©ë°›ì•˜ì–´. ë‹¨ìˆœíˆ ì¶©ê²©ë°›ì•˜ì–´.\nDad was devastated. #  ì•„ë¹ ëŠ” ì¶©ê²©ë°›ì•˜ë‹¤.\nI am upset #  ë‚˜ ì†ìƒí•´\nThat\u0026rsquo;s upsetting #  ê·¸ê±° ì†ìƒí•˜ë„¤.\nI\u0026rsquo;m humiliated. #  êµ´ìš•ì ì´ì•¼.\nGod. this is so humiliating. #  ì˜¤ ì‹ ì´ì—¬. ì´ê±´ ì •ë§ ì¹˜ìš•ì ì´ì•¼.\nWhat kind of work to do. #  ì–´ë–¤ ì¢…ë¥˜ì˜ ì¼ì„ í•˜ë‹ˆ?\nWhat kind of music do you like? #  ì–´ë–¤ ì¢…ë¥˜ì˜ ìŒì•…ì„ ì¢‹ì•„í•˜ë‹ˆ?\nIâ€™m kind of hungry. #  ë‚˜ ì«Œ ë°°ê³ íŒŒ\nIâ€™m kinda hungry. #  ë‚˜ ì«Œ ë°°ê³ í”„ë„¤.\nIâ€™m kinda tired. #  ë‚œ ì¢€ íŒŒê³¤í•´\nI kinda like her. #  ë‚œ ì¢€ ê·¸ë…€ê°€ ë§˜ì— ë“¤ì–´.\nItâ€™s kinda getting warm. #  ë‚ ì”¨ê°€ ì¢€ ë”°ëœ» í•´ì§€ë„¤.\nItâ€™s kind of getting hot. #  ì«Œ ì ì  ë”ì›Œì§€ëŠ”ë°.\nIâ€™m sorta busy right now. #  ì§€ê¸ˆ ì«Œ ë°”ë¹ .\nSorta like this weather. #  ì§€ê¸ˆ ì´ ë‚ ì”¨ê°€ ì«Œ ì¢‹ì•„.\nSorry about the mess. #  ì •ëˆì´ ì•ˆë˜ì„œ ë¯¸ì•ˆ.\nSorry, itâ€™s a bit messy. #  ë¯¸ì•ˆ ì¢€ ì–´ìˆ˜ì„ í•˜ë„¤.\nItâ€™s a little messy. #  ì¢€ ì–´ìˆ˜ì„ í•˜ë„¤ìš”.\nAlso, do not forget that he has just gone through a messy divorce. #  ë˜í•œ ê·¸ëŠ” ë§‰ ê³¨ì¹˜ì•„í”ˆ ì´í˜¼ì„ ê²ªì—ˆë‹¤ëŠ”ê±¸ ìŠì§€ ë§ˆì„¸ìš”.\nDonâ€™t mess with me. #  ì¥ë‚œì¹˜ì§€ë§ˆ.\nYoueâ€™re messing with the wrong person. #  ë„ˆ ì§€ê¸ˆ ì‚¬ëŒ ì˜ ëª» ê±´ë“œë ¸ì–´.\nI messed up everything. #  ë‚´ê°€ ë‹¤ ë§ì³¤ì–´.\nDonâ€™t mess it up. #  ë§ì¹˜ì§€ì§€ë§ˆ.\nI donâ€™t wanna mess it up, you know? #  ë‚œ ë§ì¹˜ê³  ì‹¶ì§€ì•Šì•„. ì•Œì•„?\nIâ€™m sorry I messed up your flowers and everything. #  ë„¤ ê½ƒê³¼ ëª¨ë“ ê±¸ ë§ì³ì„œ ë¯¸ì•ˆí•´.\nI saw this coming #  ì´ë ‡ê²Œ ë  ì¤„ ì•Œì•˜ì–´.\nI saw that coming. #  ê·¸ë ‡ê²Œ ë  ì¤„ ì•Œì•˜ì–´.\nI didnâ€™t see this coming. #  ë‚œ ì´ëŸ´ ì¤„ ëª°ëì–´.\nYou saw this coming? / Yeah, everyone saw this coming. #  ì´ë ‡ê²Œ ë ì¤„ ì•Œì•˜ë‹ˆ? / ëª¨ë‘ê°€ ì•Œì•˜ì–´.\nDidnâ€™t see that comig. #  ê·¸ë ‡ê²Œ ë¨ì¤„ ëª°ëëŠ”ë°.\nSorry. Somethingâ€™s come up, and I gotta run. #  ë¯¸ì•ˆ ë¬´ìŠ¨ì¼ì´ ìƒê²¨ì„œ ë¹¨ë¦¬ ê°€ë´ì•¼ë¼\nI need to run an errand. #  ë‚˜ ë³¼ì¼ì´ ìˆì–´.\nI need to run some errands. #  ë‚˜ ë³¼ì¼ì´ ì¢€ ìˆì–´.\nIâ€™ve been running around all day. #  í•˜ë£¨ì¢…ì¼ ì—¬ê¸°ì €ê¸° ë‹¤ë‹ˆëŠë¼ ì •ì‹ ì´ í•˜ë‚˜ë„ ì—†ì–´.\nHe runs a convenience store. #  ê·¸ëŠ” í¸ì˜ì ì„ ìš´ì˜í•˜ê³  ìˆì–´.\nWho runs this place? #  ëˆ„ê°€ ì—¬ê¸° ì±…ì„ìì§€?\nI need to speak to the person in charge? #  ë‹´ë‹¹ìì™€ ì–˜ê¸° í•˜ê³  ì‹¶ë‹¤.\nMay I speak to the person in charge? #  ë‹´ë‹¹ìì™€ ì´ì•¼ê¸° í•  ìˆ˜ ìˆë‚˜ìš”?\nSorry Iâ€™m late. Class ran long. #  ëŠ¦ì–´ì„œ ë¯¸ì•ˆí•´. ìˆ˜ì—…ì´ ëŠ¦ê²Œ ëë‚¬ì–´.\nI ran into her the other day. #  ì €ë²ˆì— ì§€ë‚˜ê°€ë‹¤ ìš°ì—°íˆ ê·¸ë…€ë¥¼ ë§Œë‚¬ì–´.\nGlad I ran into you. #  ì´ë ‡ê²Œ ìš°ì—°íˆ ë³´ë‹ˆ ë°˜ê°€ìš´ê±¸.\nWe ran into some problems. #  ë¬¸ì œ ë“¤ì— ë¶€ë”ªí˜”ì–´.\nI ran into some trouble. #  ê³¤ë€ì— ë¶€ë”ªí˜”ì–´.\nI ran across an interesting article the other day. #  ì €ë²ˆì— í¥ë¯¸ë¡œìš´ ê¸°ì‚¬ë¥¼ ìš°ì—°íˆ ë°œê²¬í–ˆì–´.\nI ran across an interesting quote the other day. #  ì €ë²ˆì— í¥ë¯¸ë¡œìš´ ë¬¸êµ¬ë¥¼ ìš°ì—°íˆ ë°œê²¬í–ˆì–´.\nThese days, itâ€™s hard to run across someone without mask. #  ìš”ì¦˜ì€ ë§ˆìŠ¤í¬ë¥¼ ì“°ì§€ ì•Šì€ ì‚¬ëŒì„ ì ‘í•˜ê¸°ê°€ ì–´ë ¤ì›Œ.\nYou are not exactly the smartest guy I ever ran across. #  ë‚´ê°€ ì ‘í•´ë³¸ ì‚¬ëŒ ì¤‘ ê°€ì¥ ë˜‘ë˜‘í•œ ì‚¬ëŒì€ ì•„ë‹ˆì•¼.\nAnd Iâ€™ve never run across anyone quite like you. #  ì§€ê¸ˆê¹Œì§€ ë„ˆê°™ì€ ì‚¬ëŒì€ ë§Œë‚˜ ë³¸ ì ì´ ì—†ì–´.\nLetâ€™s run this by him first. #  ë¨¼ì € ê·¸ì—ê²Œ ì¡°ì–¸ì„ ë°›ìì‹œë‹¤.\nWell, Iâ€™ll call him and run it by him. #  ê·¸ì—ê²Œ ì „í™”í•´ì„œ ì¡°ì–¸ì„ ë°›ì„ê»˜.\nDonâ€™t worry. Iâ€™ll run it by you before I press any buttons. #  ê±±ì •í•˜ì§€ë§ˆ, ë­˜ ê²°ì •í•˜ê¸°ì •ì— ë‹¹ì‹ ê³¼ ìƒë‹´í•˜ê² ì–´.\nWeâ€™re running out of time. #  ì‹œê°„ì´ ì–¼ë§ˆ ì•ˆë‚¨ì•˜ì–´.\nWeâ€™re running out of money. #  ëˆì´ ë‹¤ ë–¨ì–´ì ¸ê°€.\nWeâ€™re running our of gas. #  ê¸°ë¦„ì´ ë‹¤ ë–¨ì–´ì ¸ê°€.\nIâ€™m running out of patience. #  ì¸ë‚´ì‹¬ì´ ë°”ë‹¥ ë‚˜ê³  ìˆì–´.\nLetâ€™s quickly run through this list. #  ì´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¬ë¹¨ë¦¬ í›‘ì–´ ë³´ì.\nLetâ€™s do a quick run-through. #  ìš°ë¦¬ ì¬ë¹¨ë¦¬ ì—°ìŠµí•´ë³´ì.\nYouâ€™re so generous. #  ë„ˆ ë„ˆë¬´ ê³ ë§™ë‹¤.\nThis is so generous. #  ì´ê±° ë„ˆë¬´ ê³ ë§ˆìš´ê±¸.\nThis is too generous. #  ë„ˆë¬´ ê³ ë§™ë‹¤.\nThis is too much. #  ë„ˆë¬´ ê³¼í•˜ë‹¤.\nI canâ€™t accept this. Itâ€™s too much. #  ì´ê±¸ ë°›ì„ ìˆ˜ ì—†ì–´. ë„ˆë¬´ ê³¼í•´.\nI insist. #  ê±°ì ˆ ë§ì–´.\nOn us / Oh, no thank you. I insist. #  ì €í¬ê°€ ì‚´ê»˜ìš”. / ì•„ë‹ˆì—ìš”. ê´œì°®ìŠµë‹ˆë‹¤. ê±°ì ˆë§ˆì„¸ìš”.\nThatâ€™s too generous. / No, I insist. #  ë„ˆë¬´ ê³¼í•˜ë‹¤. / ì•„ë‹ˆ ê±°ì ˆí•˜ì§€ë§ˆ.\nItâ€™s the least I can do. #  ê·¸ ì •ë„ëŠ” ë‚´ ìµœì†Œí•œì˜ ë„ë¦¬.\nI have a simple question. #  ì´ê±´ ê°„ë‹¨í•œ ì§ˆë¬¸ì´ì•¼.\nWell, it\u0026rsquo;s a simple question. Do you love the guy or not? #  ì´ê±° ê°„ë‹¨í•œ ì§ˆë¬¸ì´ì–ì•„. ê·¸ë¥¼ ì‚¬ë‘í•´ ì•„ë‹ˆì•¼?\nI have a quick question. #  ë‚˜ëŠ” ê°„ë‹¨í•œ ì§ˆë¬¸ì´ ìˆì–´.\nCan I ask you a quick question? #  ê°„ë‹¨í•œê±° ë¬¼ì–´ë´ë„ ë ê¹Œ?\nI\u0026rsquo;d like to ask you a quick question. #  ê°„ë‹¨í•œ ì§ˆë¬¸ì„ ë¬»ê³  ì‹¶ì–´.\nJust a quick question. #  ê°„ë‹¨í•œê±° í•˜ë‚˜ë§Œ.\nWhat are you doing? #  ë­í•˜ëŠ”ê±°ëƒ?\nWhat are you working on? #  ë­ ê·¸ë¦¬ ì—´ì‹¬íˆ í•˜ë‹ˆ?\nI\u0026rsquo;m working on it. #  ì§€ê¸ˆ ì—´ì‹¬íˆ í•˜ê³  ìˆì–´.\nAre you okay? #  í™•ì‹¤íˆ ë¬¸ì œìˆëŠ” ìƒí™©ì—ì„œ ë„ˆ ê´œì°®ì•„?\nIs everything okay? #  ì•„ë¬´ì¼ ì—†ì§€? ê´œì°®ë‹ˆ?\nI made you some coffee. #  ë‚´ê°€ ë„ˆë¥¼ ìœ„í•´ì„œ ì»¤í”¼ë¥¼ ë§Œë“¤ì—ˆì–´.\nI picked you up some coffee. #  ë‚´ê°€ ë„ˆë¥¼ ìœ„í•´ ì»¤í”¼ë¥¼ ì‚¬ì™”ì–´.\nI got you some coffee. #  ë‚´ê°€ ë„ˆë¥¼ ìœ„í•´ ì»¤í”¼ë¥¼ ì–»ì–´ì™”ì–´.\nI went to the Asian market, got the ingredients, and made it from scratch. #  ì•„ì‹œì•„ ì‹œì¥ì— ê°€ì„œ ì¬ë£Œë¥¼ ì‚¬ë‹¤ê°€ ì²˜ìŒë¶€í„° ëê¹Œì§€ ë‹¤ ë§Œë“¤ì—ˆì–´.\nIt\u0026rsquo;s all good, I\u0026rsquo;ll make it work. #  ê´œì°®ì•„, ë˜ê²Œ í• ê»˜.\nMake it work. #  ë˜ê²Œ í•´.\nYou made it. / We made it. #  ë„ˆ í•´ëƒˆêµ¬ë‚˜. ë„ˆ ì™”êµ¬ë‚˜. / ìš°ë¦¬ê°€ í–ˆëƒˆë‹¤.\nI hope you can make it. #  ì˜¬ ìˆ˜ ìˆìœ¼ë©´ ì¢‹ê² ì–´.\nI can\u0026rsquo;t go. #  ë‚œ ëª»ê°„ë‹¤.\nI can\u0026rsquo;t make it #  ê°€ë ¤ê³  í–ˆëŠ”ë° ëª»ê°€ìš”.\nI don\u0026rsquo;t think I can make it. #  ê°€ë ¤ê³  í–ˆëŠ”ë° ëª» ê°ˆ ê²ƒ ê°™ì•„ìš”.\nI really don\u0026rsquo;t feel very comfortable making this decision. #  ë‚˜ëŠ” ì •ë§ ì´ ê²°ì •ì„ ë‚´ë¦¬ëŠ”ê²Œ ë¶ˆí¸í•˜ë„¤.\nI don\u0026rsquo;t feel comfortable going there. #  ê±°ê¸° ê°€ëŠ”ê²Œ ë¶ˆí¸í•´.\nI don\u0026rsquo;t feel comfortable asking her. #  ê·¸ë…€ì—ê²Œ ë¬»ëŠ”ê²Œ í¸ì¹˜ ì•Šì•„.\nI\u0026rsquo;m not comfortable with this. #  ì´ê±´ ì •ë§ ë§ˆìŒì´ í¸ì¹˜ì•Šì•„.\nThat\u0026rsquo;s not your decision to make. #  ê·¸ê±´ ë‹ˆê°€ ë‚´ë¦´ ê²°ì •ì´ ì•„ëƒ.\nI\u0026rsquo;ll make a note of it. #  ë©”ëª¨í•´ë‘˜ê²Œ. ê¸°ì–µí• ê²Œ.\nPlease, make a note of this. #  ì´ê±° ê¼­ ê¸°ì–µí•´ë‘ì„¸ìš”.\nWe\u0026rsquo;re making every effort department wide to fix this problem. #  ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì „ ë¶€ì„œì— ê±¸ì³ ì •ë§ ì• ì“°ê³  ìˆìŠµë‹ˆë‹¤.\nWe\u0026rsquo;re making every effort department wide to continue to find ways to assist you in this challenge. #  ì´ ì–´ë ¤ìš´ ìƒí™©ì—ì„œ ë‹¹ì‹ ì„ ë•ìš¸ ë°©ë²•ì„ ì°¾ê¸° ìœ„í•´ ì „ ë¶€ì„œê°€ ëª¨ë“  ë…¸ë ¥ì„ ì•„ë¼ì§€ ì•Šê³  ìˆë‹¤.\nWe\u0026rsquo;re doing everything we can. #  ìš°ë¦¬ê°€ í•  ìˆ˜ ìˆëŠ”ê±° ë‹¤í•˜ê³  ìˆì–´.\nI\u0026rsquo;ll make it up to you. #  ë‚˜ì¤‘ì— ë³´ë‹µí• ê»˜.\nHow can I make it up to you? I\u0026rsquo;ll take you to dinner tonight. #  ì–´ë–»ê²Œ í•´ì•¼ í™”ê°€ í’€ë¦¬ê² ì–´? ì €ë…ì‹ì‚¬ë¥¼ ëŒ€ì ‘í•˜ê³  ì‹¶ì–´.\nMake the most of your day. #  ë‹¹ì‹ ì˜ í•˜ë£¨ë¥¼ ì˜ ì´ìš©í•´ë¼.\nHave a good day. #  ì¢‹ì€ í•˜ë£¨ ë˜ë¼.\nEnjoy the rest of your day #  ë‚¨ì€ í•˜ë£¨ ì¦ê²ê²Œ ë³´ë‚´ë¼.\nSo make the most of this opportunity. #  ê·¸ë˜ì„œ ì´ ê¸°íšŒë¥¼ ì˜ í™œìš©í•´ë´.\nI hope we\u0026rsquo;ll meet again. / I\u0026rsquo;ll make sure we do. #  ë‹¹ì‹ ì„ ë˜ ë´¤ìœ¼ë©´ ì¢‹ê² ì–´. ë¶„ëª…íˆ ê·¸ë ‡ê²Œ ë˜ë„ë¡ í• êº¼ì•¼.\nI\u0026rsquo;ll make sure this doesn\u0026rsquo;t happen again. #  ë‹¤ì‹œëŠ” ì´ëŸ°ì¼ì´ ìƒê¸°ì§€ ì•Šë„ë¡ í•˜ê² ë‹¤.\nI\u0026rsquo;m supposed to meet friends downtown. #  ì¹œêµ¬ë“¤ì´ë‘ ë‹¤ìš´íƒ€ìš´ì—ì„œ ë§Œë‚˜ê¸°ë¡œ í–ˆì–´.\nI\u0026rsquo;m going to meet friends downtown. #  ì¹œêµ¬ë“¤ì´ë‘ ë‹¤ìš´íƒ€ìš´ì—ì„œ ë§Œë‚ ê±°ì•¼.\nIt\u0026rsquo;s supposed to rain today. #  ì˜¤ëŠ˜ ë¹„ì˜¨ë‹¤ê³  í•˜ë”ë¼.\nIt\u0026rsquo;s supposed to rain tomorrow, right? #  ë‚´ì¼ ë¹„ì˜¨ë‹¤ê³  í–ˆì§€, ë§ì•„?\nIt\u0026rsquo;s supposed to snow today. #  ì˜¤ëŠ˜ ëˆˆ ì˜¨ë‹¤ê³  í•˜ë”ë¼.\nIt\u0026rsquo;s supposed to be hot today. #  ì˜¤ëŠ˜ ë¥ë‹¤ê³  í•˜ë”ë¼.\nIt\u0026rsquo;s supposed to be cold early next week. #  ë‹¤ìŒì£¼ ì´ˆëŠ” ì¶¥ë‹¤ê³  í•˜ë”ë¼.\nCome on, You\u0026rsquo;re not supposed to do that. #  ì˜¤ìš°, ë„ˆ ì›ë˜ ê·¸ëŸ¬ë©´ ì•ˆë˜ëŠ” ê±°ì–ì•„.\nNot supposed to smoke in here. #  ì—¬ê¸°ì„œ ë‹´ë°°í”¼ì‹œë©´ ì•ˆë¼ìš”.\nI don\u0026rsquo;t think you\u0026rsquo;re supposed to do that. #  ì›ë˜ ê·¸ëŸ¬ë©´ ì•ˆë˜ëŠ” ê²ƒ ê°™ì€ë°.\nI was supposed to hang out with my friends, but I was too tired. #  ì–´ì œ ì¹œêµ¬ë‘ ë§Œë‚˜ê¸°ë¡œ í–ˆëŠ”ë° í”¼ê³¤í•´ì„œ ì•ˆë§Œë‚¬ì–´.\nI was supposed to study English, but I was too busy. #  ì˜ì–´ ê³µë¶€í•˜ê¸°ë¡œ í–ˆì—ˆëŠ”ë° ë„ˆë¬´ ë°”ë¹´ì–´.\nSee, this was supposed to be easy and fun. #  ë´¤ì§€, ì´ê±´ ì›ë˜ ì‰½ê³  ì¬ë°Œì–´ì•¼ í–ˆì–´.\nWe were supposed to meet for lunch, but he didn\u0026rsquo;t show. #  ì›ë˜ ìš°ë¦° ì ì‹¬ ë¨¹ê¸°ë¡œ ë˜ì–´ ìˆì—ˆëŠ”ë° ê·¸ê°€ ì•ˆë‚˜íƒ€ë‚¬ì–´.\nWhere am I supposed to go? #  ì–´ë””ë¡œ ê°€ë¼ëŠ” ê±°ëƒ?\nWhat am I supposed to do? #  ë‚˜ë³´ê³  ì–´ì©Œë¼ëŠ” ê±°ëƒ?\nWhat\u0026rsquo;s that supposed to mean? #  ë¬´ìŠ¨ ì˜ë¯¸ ì¸ê±°ëƒ?\nIt\u0026rsquo;s supposed to be hard. If it wasn\u0026rsquo;t hard. Everyone would do it. The hard is what makes it great. #  ì›ë˜ ì–´ë ¤ì›Œì•¼ í•˜ëŠ”ë° ì–´ë µì§€ ì•Šë‹¤ë©´ ëª¨ë‘ë“¤ ê·¸ëŸ´ê±°ì•¼. ì–´ë ¤ì›€ ìì²´ê°€ ìœ„ëŒ€í•˜ê²Œ ë§Œë“œëŠ” ê±°ì•¼.\nIt\u0026rsquo;s supposed to be hard. #  ì›ë˜ ì–´ë ¤ìš´ê±°ì•¼.\nThere are a lot of job seeker tools. #  ì €ê¸°ì—ëŠ” ë§ëŠ” êµ¬ì§ ë„êµ¬ë“¤ì´ ìˆìŠµë‹ˆë‹¤.\nI\u0026rsquo;m looking for a job. #  ì €ëŠ” êµ¬ì§ì¤‘ì…ë‹ˆë‹¤.\nI\u0026rsquo;m in between jobs. #  ì €ëŠ” êµ¬ì§ì¤‘ì…ë‹ˆë‹¤.\nWhat do you do for a living? I\u0026rsquo;m in between jobs. #  ë¬´ìŠ¨ì¼ì„ í•˜ë‹ˆ? êµ¬ì§ì¤‘ì´ì•¼.\nI\u0026rsquo;m in between jobs at the moment. #  ì§€ê¸ˆ êµ¬ì§ì¤‘ì´ì•¼.\nHere\u0026rsquo;s my card. #  ë‚´ ëª…í•¨ì´ì•¼.\nCan I have your card? Sure, here\u0026rsquo;s my card. #  ëª…í•¨ì¢€ ì¤„ë˜? ì—¬ê¸° ë‚´ëª…í•¨.\nHere\u0026rsquo;s my card. If you \u0026rsquo;re ever free, give me a call. #  ì—¬ê¸° ë‚´ ëª…í•¨. í¸í•´ì§€ë©´ ì „í™”ì¤˜.\nIt\u0026rsquo;s on the house. / It\u0026rsquo;s on us. #  ì´ê±° ê³µì§œì…ë‹ˆë‹¤.\nOn us. / Oh, no, thank you. #  ê³µì§œì…ë‹ˆë‹¤. ì˜¤ ì•„ë‹ˆìš” ê°ì‚¬í•©ë‹ˆë‹¤.\nCoffee is on me. #  ì»¤í”¼ëŠ” ë‚´ê°€ ì ê»˜.\nThanks for all the hard work this week, guys! And as a thank you, lunch is on me. My treat. #  ì´ë²ˆì£¼ ì—´ì‹¬íˆ í•´ì¤˜ì„œ ê³ ë§ˆì›Œ. ê°ì‚¬ì˜ í‘œì‹œë¡œ ì ì‹¬ ì‚´ê»˜. ë‚´ê°€ ëŒ€ì ‘í•˜ëŠ”ê±°ì•¼.\nTime flies. I miss college. #  ì‹œê°„ ë¹¨ë¦¬ ê°„ë‹¤. ëŒ€í•™ì‹œì ˆì´ ê·¸ë¦½ë„¤.\nHow did you first meet? We meet in college. #  ì–´ë–»ê²Œ ì²˜ìŒ ë§Œë‚¬ì–´? ìš°ë¦¬ ëŒ€í•™ì—ì„œ ë§Œë‚¬ì–´.\nWere do you go to school? #  í•™êµ ì–´ë”” ë‹¤ë‹ˆë‹ˆ?\nWhere did you go to school? #  í•™êµ ì–´ë”” ë‚˜ì™”ë‹ˆ?\nHey! Where are you? Are you still at work? #  í—¤ì´! ì–´ë””ë‹ˆ? ì•„ì§ë„ ê·¼ë¬´ì•¼?\nI like dog. #  ë‚˜ ê°œê³ ê¸° ì¢‹ì•„í•´.\nI like dogs. #  ë‚œ ê°œë¥¼ ì¢‹ì•„í•´.\nI\u0026rsquo;m a dog person. #  ë‚œ ê°•ì•„ì§€ë¥¼ ì¢‹ì•„í•˜ëŠ” ì‚¬ëŒì´ì•¼.\nI\u0026rsquo;m a cat person. #  ë‚œ ê³ ì–‘ì´ë¥¼ ì¢‹ì•„í•´.\nI\u0026rsquo;m a people person. #  ë‚˜ëŠ” ì‚¬êµì ì¸ ì‚¬ëŒì´ì•¼.\nI\u0026rsquo;m a morning person. #  ë‚œ ì•„ì¹¨í˜• ì¸ê°„ì´ì•¼.\nI\u0026rsquo;m a coffee person. #  ë‚œ ì»¤í”¼ë¥¼ ì¢‹ì•„í•´.\nAre you a dog or a cat person? #  ë„Œ ê°•ì•„ì§€ë‘ ê³ ì–‘ì´ì¤‘ ë­ê°€ ì¢‹ë‹ˆ?\nAre you a dog person? No, Actually, I\u0026rsquo;m more of a cat person. #  ê°œ ì¢‹ì•„í•˜ë‹ˆ? ì•„ë‹ˆ, ì‚¬ì‹¤ ê³ ì–‘ì´ë¥¼ ë” ì¢‹ì•„í•´.\nI\u0026rsquo;m more of a night person. #  ë‚˜ëŠ” ì €ë…í˜• ì¸ê°„ì— ë” ê°€ê¹Œì›Œ.\nDo you like cats? I\u0026rsquo;m more of a dog person myself. #  ê³ ì–‘ì´ ì¢‹ì•„í•˜ë‹ˆ? ë‚˜ëŠ” ì¢€ë” ê°œë¥¼ ì¢‹ì•„í•´.\nI\u0026rsquo;m not much of a dog person. #  ë‚œ ê°œë¥¼ ì¢‹ì•„í•˜ëŠ” ì‚¬ëŒì´ ì•„ë‹ˆì•¼.\nI\u0026rsquo;m not much of a drinker. #  ì „ ìˆ  ë³„ë¡œ ì•ˆ ì¢‹ì•„í•´ìš”.\nI took a semester off. #  ë‚˜ í•œí•œê¸° íœ´í•™í–ˆì–´.\nI\u0026rsquo;m taking a semester off from school. #  ë‚˜ ì§€ê¸ˆ í•œí•™ê¸° ì‰¬ê³  ìˆëŠ” ì¤‘ì´ì•¼.\nI\u0026rsquo;m actually not feeling so hot. Yeah, you should take the day off and feel better. okay? #  ë‚˜ ì§€ê¸ˆ ë­”ê°€ ì—´ì •ì ì´ì§€ ì•Šì•„. ê·¸ë˜ ê·¸ëŸ¼ í•˜ë£¨ ì‰¬ê³  ì¢€ ë‚˜ì•„ì ¸. OK?\nI\u0026rsquo;m not feeling well. #  ë‚˜ ì»¨ë””ì…˜ì´ ë³„ë¡œì•¼.\nTake the rest of the day off. #  ë‚¨ì€ í•˜ë£¨ëŠ” ì§‘ì— ê°€ì„œ ì‰¬ì–´.\nGet some rest. #  ì¢€ ì‰¬ì–´ë¼.\nTake the rest of the afternoon off. Take a sick day. #  ì˜¤í›„ëŠ” í‡´ê·¼í•˜ê³  ë“¤ì–´ê°€ì„œ ì‰¬ì–´. í•˜ë£¨ëŠ” ì‰¬ì–´ë¼.\nTake this Friday off. #  ì´ë²ˆì£¼ ê¸ˆìš”ì¼ì€ ì‰¬ì–´ë¼.\nTake a year off. #  ì¼ë…„ì„ ì‰¬ì–´ë¼.\nShe\u0026rsquo;s on marternity leave right now. #  ê·¸ë…€ëŠ” ì§€ê¸ˆ ì¶œì‚° íœ´ê°€ ì¤‘ì´ì•¼.\nSpeak English. (jargon) #  ì•Œì•„ ë“£ê²Œ ì–˜ê¸°í•´ë´.\nFinally, someone who speaks English. It\u0026rsquo;s good to meet you, Dr. Banner. #  ì´ì œì•¼ ë§ì´ í†µí•˜ëŠ” ì‚¬ëŒì„ ë§Œë‚¬ë„¤. ë§Œë‚˜ì„œ ë°˜ê°‘ë‹¤. ë‹¥í„° ë°°ë„ˆ.\nI lied. I said that that was the last item. I actually have one more artifact still to come. #  ì‹¤ìˆ˜í–ˆë„¤ìš”. ì €ê²Œ ë§ˆì§€ë§‰ ì•„ì´í…œì´ë¼ê³  ì–˜ê¸°í–ˆëŠ”ë° ì‚¬ì‹¤ í•˜ë‚˜ë” ìˆìŠµë‹ˆë‹¤.\nI lied, I do have a favorite line. It\u0026rsquo;s when Burr says at the end of the show. #  ì‹¤ìˆ˜ í–ˆë„¤. ì •ë§ ì¢‹ì•„í•˜ëŠ” ë¬¸ì¥ì´ ìˆì–´. ë²„ë¥´ê°€ ì‡¼ ë§ˆì§€ë§‰ì— í•˜ëŠ”ê±°ì•¼.\nOh. Sorry. My bad. #  ì˜¤ ë¯¸ì•ˆ, ë‚´ê°€ ì‹¤ìˆ˜ í–ˆì–´.\nThis has got the three categories on the bottom, oh actually four, I lied, four categories. #  ë°”ë‹¥ì— ì„¸ê°œ ì¹´í…Œê³ ë¦¬ì…ë‹ˆë‹¤. ì˜¤ ë„¤ê°œë„¤ìš”. ì œê°€ ì‹¤ìˆ˜í–ˆë„¤ìš”. ë„¤ê°œ ì¹´í…Œê³ ë¦¬ì…ë‹ˆë‹¤.\nYou\u0026rsquo;re silly. / Yep, I am silly. #  ë„ˆ ì°¸ ì—‰ëš±í•´. ê·¸ë˜ ë‚œ ì—‰ëš±í•´.\nDon\u0026rsquo;t be silly. #  ì¥ë‚œ ê·¸ë§Œí•´.\nYou\u0026rsquo;re being silly. #  ë„ˆ ì—‰ëš±í•˜ê²Œ í–‰ë™í•˜ë„¤.\nYou\u0026rsquo;re silly. Well a little. #  ë„ˆ ì—‰ëš±í•´. ê¸€ì„ ì¡°ê¸ˆ.\nYou\u0026rsquo;re all stupid. #  ë„ˆí¬ë“¤ ë‹¤ ë©ì²­í•´.\nStupid pen! #  ì§œì¦ë‚˜ëŠ” íœ.\nHe was up all night working on his stupid project. #  ê·¸ëŠ” ê·¸ ì§œì¦ë‚˜ëŠ” í”„ë¡œì íŠ¸í•˜ë‚˜ë¼ê³  ë°¤ì„ ìƒœì–´.\nAre you sure? / I am sure. #  ì •ë§ ê·¸ë˜ë„ ë¼? ê·¸ë˜ë„ ë¼.\nDeep down. #  ë‚´ì‹¬.\nAre you sure? That would be nice. You\u0026rsquo;re an angel. You\u0026rsquo;re the best. #  ì •ë§? ê³ ë§ˆì›Œ. ë„Œ ì²œì‚¬ì•¼. ë‹ˆê°€ ìµœê³ ì•¼.\nAre you sure? / I\u0026rsquo;m positive. Take the pen. #  ì •ë§ìš”? ê·¸ë˜. íœê°€ì ¸.\nOh, I\u0026rsquo;ll get it. Are you sure? #  ê°€ì ¸ì˜¬ê»˜. ì •ë§?\nLet\u0026rsquo;s talk about this over lunch. #  ì ì‹¬ ë¨¹ìœ¼ë©´ì„œ ì–˜ê¸°í•˜ì.\nLet\u0026rsquo;s discuss this over lunch. #  ì ì‹¬ ë¨¹ìœ¼ë©´ì„œ ì–˜ê¸°í•˜ì.\nThat\u0026rsquo;s a good question. Let\u0026rsquo;s talk more over lunch. #  ì¢‹ì€ ì§ˆë¬´ë‹ˆì•¼. ì ì‹¬ ë¨¹ìœ¼ë©´ì„œ ì–˜ê¸°í•˜ì.\nI hope you got something out of it. #  ìœ ìµ í–ˆê¸¸ ë°”ë˜ìš”.\nWhy don\u0026rsquo;t we talk about this over lunch? #  ì ì‹¬ ë¨¹ìœ¼ë©´ì„œ ì´ê±° ì–˜ê¸°í•´ë³´ëŠ”ê²Œ ì–´ë•Œ?\nWhy don\u0026rsquo;t we talk about this over dinner? #  ì €ë… ë¨¹ìœ¼ë©´ì„œ ì–˜ê¸°í•´ë³´ëŠ”ê²Œ ì–´ë•Œ?\nLet\u0026rsquo;s talk about this over coffee. #  ì»¤í”¼ë§ˆì‹œë©´ì„œ ì–˜ê¸°í•´ë³´ë‹¤.\nHe\u0026rsquo;s on the phone #  ê·¸ëŠ” í†µí™” ì¤‘ì´ì•¼.\nAre you on the phone? #  ì§€ê¸ˆ í†µí™” ì¤‘ì´ì•¼?\nI\u0026rsquo;m on the phone. #  ë‚˜ ì§€ê¸ˆ í†µí™”ì¤‘ì´ì•¼.\nWe spoke on the phone. #  ìš°ë¦¬ ì „í™” í†µí™” í–ˆì—ˆì£ .\nCan you put Mike on the phone? #  Mike ì¢€ ë°”ê¿” ì¤„ë˜?\nCould you put Carrie on the phone, please? #  ìºë¦¬ì¢€ ë°”ê¿”ì¤„ë˜?\nYou sounded upset on the phone. #  í†µí™”í• ë•Œ ì†ìƒí•´ í•˜ëŠ” ê²ƒ ê°™ë”ë¼.\nHey. Is everything okay? You sounded weird on the phone. #  ì „í™” í†µí™”í• ë•Œ í‰ì†Œì™€ ë‹¤ë¥´ë˜ë° ì•„ë¬´ì¼ ì—†ì§€?\nCan I make a reservation over the phone? #  ì „í™”ë¡œ ì˜ˆì•½ ê°€ëŠ¥ í•œê°€ìš”?\nDo I have to do it in person or can I do it over the phone? #  ì§ì ‘ ê°€ì„œ í•´ì•¼ í•˜ë‚˜ìš”? ì•„ë‹ˆë©´ ì „í™”ë¡œ í•´ë„ ë˜ë‚˜ìš”?\nThanks again for letting me pay over the phone. #  ì „í™”ë¡œ ì§€ë¶ˆí•  ìˆ˜ ìˆê²Œ í•´ì¤˜ì„œ ê³ ë§ˆì›Œ.\nIt\u0026rsquo;s too complicated to explain over the phone. #  ì „í™”ë¡œ í•˜ê¸°ì—ëŠ” ë„ˆë¬´ ë³µì¡í•´.\nI\u0026rsquo;m not comfortable giving you that information over the phone. #  ì „í™”ìƒìœ¼ë¡œ ê·¸ëŸ° ì •ë³´ë¥¼ ë“œë¦¬ëŠ”ê²Œ ë¶ˆí¸í•˜ë„¤ìš”.\nBecause he didn\u0026rsquo;t wanna relay that kind of information over the phone. #  ì™œëƒí•˜ë©´ ê·¸ëŠ” ì „í™”ë¡œ ê·¸ëŸ° ì¢…ë¥˜ë¥¼ ì „í•´ì£¼ê³  ì‹¶ì§€ ì•Šì•˜ì„ê±°ì•¼.\nNo, I\u0026rsquo;d rather not do it over the phone. #  ì•„ë‹ˆìš”. ì €ëŠ” ì „í™”ìƒìœ¼ë¡œ ê·¸ê²ƒì„ í•˜ê³  ì‹¶ì§€ ì•ŠìŠµë‹ˆë‹¤.\nWhat did you do over the weekend? #  ì£¼ë§ì— ë­í–ˆë‹ˆ?\nLet\u0026rsquo;s talk about it over lunch. #  ì ì‹¬ ë¨¹ìœ¼ë©´ì„œ ì–˜ê¸°í•˜ì.\nCan I just do it over the phone? #  ì „í™”ë¡œ í• ìˆ˜ ìˆë‚˜ìš”?\nWe can\u0026rsquo;t discuss them over the phone? #  ì „í™”ìƒìœ¼ë¡œ ì´ì•¼ê¸° í•  ìˆ˜ ì—†ë‚˜ìš”?\nI need to work late tonight. #  ëŠ¦ê²Œ ê¹Œì§€ ì•¼ê·¼ í•´ì•¼ í•´.\nYou are working late. #  ì•¼ê·¼í•˜ëŠ” êµ¬ë‚˜\nI had to work late. #  ì•¼ê·¼ í•´ì•¼ë¼.\nFingers crossed for overtime pay. #  ì•¼ê·¼ìˆ˜ë‹¹ ë‚˜ì˜¤ë©´ ì¢‹ê² ë‹¤.\nHow do you know each other? We work together. #  ì–´ë–»ê²Œ ì•„ëŠ” ì‚¬ì´ì¸ê°€ìš”? ìš°ë¦¬ ê°™ì´ ì¼í•´ìš”.\nWe used to work together. #  ìš°ë¦¬ ì˜ˆì „ì— ê°™ì´ ì¼í–ˆì–´ìš”.\nWhat time do you get off work? #  ëª‡ ì‹œì— í‡´ê·¼í•˜ë‹ˆ?\nWhat time do you get off tonight? #  ì˜¤ëŠ˜ ì–¸ì œ í‡´ê·¼í•˜ë‹ˆ?\nI get off work at six. #  ë‚˜ ì—¬ì„¯ì‹œì— í‡´ê·¼í•´.\nYour hard work just paid off. #  ì—´ì‹¬íˆ ì¼í•œ ë³´ëŒì´ ìˆë„¤.\nI mean Iâ€™m a real hard worker. #  ì €ëŠ” ì •ë§ ì—´ì‹¬íˆ ì¼í•˜ëŠ” ì‚¬ëŒì…ë‹ˆë‹¤.\nI mean, itâ€™s not lick. Youâ€™re a very hard worker. #  ê·¸ê±´ ë‹¨ì§€ ìš´ì´ ì•„ë‹ˆì•¼. ë‹¹ì‹ ì€ ê·¼ë©´ ì„±ì‹¤í•œ ì‚¬ëŒì´ì•¼.\nKeep up the good work, everyone. #  ë‹¤ë“¤ ìˆ˜ê³ í•˜ê²Œ.\nThis is very impressive work. #  ë§¤ìš° ëŒ€ë‹¨í•˜ë„¤.\nImpressive work. #  ëŒ€ë‹¨í•˜ë„¤.\nIâ€™m impressived. #  ëŒ€ë‹¨í•˜ë„¤.\nI don\u0026rsquo;t think the wi-fi is working. #  ì™€ì´ íŒŒì´ê°€ ì•ˆë˜ëŠ” ê±° ê°™ì•„.\nI donâ€™t think this working. #  ì´ê±° ì•ˆë˜ëŠ”ê±° ê°™ì•„.\nDoes Monday work for you? #  ì›”ìš”ì¼ ì‹œê°„ ë¼?\nDo you work on monday? #  ë„ˆ ì›”ìš”ì¼ì— ê·¼ë¬´í•´?\nDoes the first week of February work for you? #  2ì›” ì²«ì£¼ ì‹œê°„ ë¼?\nDoes tomorrow work fo you at all? #  ë‚´ì¼ ì ê¹ì´ë¼ë„ ì‹œê°„ ë¼?\nDoes today work for you at all? #  ì˜¤ëŠ˜ ì ê¹ ì‹œê°„ ë¼?\nYou look good. Have you been working out? #  ë„ˆ ê´œì°®ì•„ ë³´ì¸ë‹¤. ìš”ì¦˜ ìš´ë™í•˜ë‹ˆ?\nI hope everything works outÂ for you. #  ë„¤ê²Œ ëª¨ë“  ì¼ì´ ì˜ í’€ë ¸ìœ¼ë©´ ì¢‹ê² ë‹¤.\nI hope everything works out well. #  ëª¨ë“ ê²Œ ì˜ í’€ë ¸ìœ¼ë©´ ì¢‹ê² ë‹¤.\nI\u0026rsquo;m gonna make sure everything works out. #  ëª¨ë“ ê²Œ ì˜ë¼ë„ë¡ í™•ì‹¤íˆ í•˜ê»˜.\nWhat are you working on? #  ë­˜ ê·¸ë ‡ê²Œ ì—´ì‹¬íˆ í•˜ê³  ìˆë‹ˆ?\nWeâ€™ve been working on this side project. #  ìš°ë¦¬ëŠ” ì´ ë¶€ìˆ˜ ê³¼ì œë¥¼ ì—´ì‹¬íˆ í–ˆë‹¤.\nYou need to work on your people skills. #  ë„ˆëŠ” ëŒ€ì¸ ê´€ê³„ ê¸°ìˆ  ì¢€ ê°œì„ í•´ì•¼ë¼.\nYou really need to work on your listening skills. #  ë„ˆëŠ” ì •ë§ ë“£ê¸° ì—°ìŠµì¢€ í•´ì•¼ê² ë‹¤.\nIâ€™m working on it. #  ì§€ê¸ˆ ê·¸ê±° ì—´ì‹¬íˆ í•˜ê³  ìˆì–´ìš”.\nMaybe you could start thinking about an exit strategy. / Iâ€™m working on it. #  ì•„ë§ˆ ë„ˆëŠ” ì¶œêµ¬ ì „ëµ ê³ ë¯¼ì„ ì‹œì‘ í•  ìˆ˜ë„ ìˆë‹¤.Â ê·¸ëŸ¬ë ¤ê³  ë…¸ë ¥ ì¤‘ì´ë‹¤.\nDonâ€™t worry. Iâ€™m working on it. #  ê±±ì •ë§ˆ. ì§€ê¸ˆ ì—´ì‹¬íˆ í•˜ëŠ” ì¤‘ì´ì•¼.\nWeâ€™ll work around your schedule. #  ìš°ë¦¬ê°€ ë‚´ ìŠ¤ì¼€ì¥´ì— ë§ì¶œê»˜.\nIâ€™m free. #  ë‚˜ í•œê°€í•´.\nIâ€™m available. #  ë‚˜ ê°€ëŠ¥í•´.\nAbsolutely, Weâ€™ll work around your schedule. #  ë‹¹ì—°í•˜ì£ . ì €í¬ê°€ ë‹¹ì‹ ì˜ ì¼ì •ì— ë§ì¶œê»˜ìš”.\nWe can work through this. #  ìš°ë¦° ì´ê±¸ í—¤ì³ë‚˜ê°ˆ ìˆ˜ ìˆì–´.\nWhat we have is too important. We can work through this. #  ìš°ë¦¬ê°€ ê°€ì§€ê³  ìˆëŠ”ê±´ ë„ˆë¬´ ì¤‘ìš”í•œê±°ì•¼. ìš°ë¦° ì´ê±¸ í—¤ì³ë‚˜ê°ˆ ìˆ˜ ìˆì–´.\nI work through lunch. #  ì ì‹¬ ì•ˆë¨¹ê³  ì¼í–ˆì–´. (ì ì‹¬ ë–¼ìš°ê³  ì¼í–ˆì–´)\nIâ€™m gonna work through lunch. #  ì ì‹¬ ì•ˆë¨¹ê³  ì¼í• ê±°ì•¼.\nI work Monday through Friday. #  ì›”ìš”ì¼ ë¶€í„° ê¸ˆìš”ì¼ ê¹Œì§€ ê·¼ë¬´í•œë‹¤.\nI hope you get better. #  ê³§ ë‚«ê¸¸ ë°”ëë‹ˆë‹¤.\nI wish today were Friday. #  ì˜¤ëŠ˜ì´ ê¸ˆìš”ì¼ì´ë©´ ì¢‹ê² ì–´.\nI wish it wete Friday. #  ê¸ˆìš”ì¼ì´ì˜€ìœ¼ë©´ ì¢‹ê² ì–´.\nI wish every day was Sunday. #  ëª¨ë“ ë‚ ì´ ì¼ìš”ì¼ì´ì˜€ìœ¼ë©´ ì¢‹ê² ì–´.\nCould you help me (out) with something? #  ë‚˜ ì¢€ ë„ì™€ ì¤„ ìˆ˜ ìˆë‹ˆ?\nI wish I could but I canâ€™t. #  ê·¸ëŸ¬ê³  ì‹¶ì€ë° ëª»í•´.\nSo, help us out. Wish I could, but I canâ€™t. #  ê·¸ë˜ì„œ ìš°ë¦¬ì¢€ ë„ì™€ì¤˜. ê·¸ëŸ¬ê³  ì‹¶ì§€ë§Œ ëª»í•´.\nCould you help me out with something? #  ë­ ì¢€ ì˜¤ì™€ì¤„ ìˆ˜ ìˆì–´?\nCould you help me out with the presentation? #  í”„ë ˆì  í…Œì´ì…˜ ì¢€ ë„ì™€ ì¤„ ìˆ˜ ìˆë‹ˆ?\nCould you help me out with my resume? #  ë‚´ ì´ë ¥ì„œì¢€ ë„ì™€ì¤„ë˜?\nCan you help me out with that? #  ê·¸ê²ƒ ì¢€ ë„ì™€ì¤„ ìˆ˜ ìŒã……ë‹ˆ?\nSure, Iâ€™m happy to help you out with this. #  ë¬¼ë¡  ë‚œ ì´ê±¸ë¡œ ë„ ë„ì™€ì£¼ë©´ ê¸°ì˜ë‹¤.\nI wish you were here. #  ë„¤ê°€ ì—¬ê¸° ìˆë‹¤ë©´ ì¢‹ì„í…ë°.\nI miss you and I wish you were here. #  ë„¤ê°€ ê·¸ë¦½ê³  ì—¬ê¸° ìˆì—ˆìœ¼ë©´ ì¢‹ê² ì–´.\nI wish things were different. #  ìƒí™©ì´ ë‹¬ëìœ¼ë©´ ì¢‹ê² ë‹¤.\nI wish things were different. I really do. #  ìƒí™©ì´ ë‹¬ëìœ¼ë©´ ì¢‹ì•˜ì„í…ë°. ì •ë§ ê·¸ë˜.\nWe wish you a merry Christmas. #  We hope you have a merry Christmas.\nWish me luck. #  Please hope that I have good luck.\nI called to wish you luck at the conference. #  ì»¨í¼ëŸ°ìŠ¤ ì˜í•˜ë¼ê³  ì „í™”í–ˆë‹¤.\nGood luck with her not that you need it. #  ê·¸ë…€ë‘ ì˜ë¼ê¸¸ ë°”ë˜. ë„ˆí•œí… í•„ìš”ì—†ê² ì§€ë§Œ.\nAnway, not that you need it, but good luck. #  ì–´ì¨Œê±´ í•„ìš”ì—†ê² ì§€ë§Œ í–‰ìš´ì„ ë¹ˆë‹¤.\nNonetheless, I wish you all the best in your efforts. #  ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ , ë…¸ë ¥ì— ë³´ë‹µë°›ê¸°ë¥¼ ë°”ë€ë‹¤.\nI wish you the best. #  ì¢‹ì€ ì¼ë§Œ ì¼ì–´ë‚˜ê¸¸ ë°”ë€ë‹¤.\nI wish you the very best. #  ì¢‹ì€ ì¼ë§Œ ì¼ì–´ë‚˜ê¸¸ ë°”ë˜.\nAnd I wish you the very best of luck. Thank you. #  ì—¬ëŸ¬ë¶„ ëª¨ë‘ì—ê²Œ ì¢‹ì€ ì¼ë§Œ ì¼ì–´ë‚˜ê¸¸ ë°”ë¼ê² ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤.\nThere are things I wish to say. #  ë§í•˜ê³  ì‹¶ì€ê²Œ ìˆì–´.\nI donâ€™t wish to discuss it further. #  ì´ê±´ ë”ì´ìƒ ì–˜ê¸°í•˜ê³  ì‹¶ì§€ ì•Šì•„.\nI simply donâ€™t wish to discuss it further. #  ë‚œ ë‹¨ìˆœíˆ ë”ì´ìƒ ê·¸ê±¸ ì–˜ê¸°í•˜ê³  ì‹¶ì§€ ì•Šì•„.\nExactly, what happened? Itâ€™s complicated, And I donâ€™t wish to discuss it. #  ì •í™•íˆ ë­ê°€ ì¼ì–´ë‚¬ë‹ˆ. ì¢€ ë³µì¡í•œë° ë” ì´ìƒ ì–˜ê¸°í•˜ê³  ì‹¶ì§€ ì•Šì•„.\nI wish to soeak to the manager. #  ë§¤ë‹ˆì €ì™€ ì–˜ê¸°í•˜ê³  ì‹¶ë‹¤.\nTake me to the one in charge. I wish to speak to him. #  ë‚˜ë¥¼ ì±…ì„ìì—ê²Œ ë°ë ¤ë‹¤ ì£¼ì‹œì˜¤. ê·¸ì™€ ì–˜ê¸°í•˜ê³  ì‹¶ì†Œ.\nI wish to see her. #  ê·¸ë…€ë¥¼ ëµ™ê³  ì‹¶ìŠµë‹ˆë‹¤.\nI hope to see her. #  ê·¸ë…€ë¥¼ ëµìˆ˜ ìˆê¸¸ ë°”ë˜ìš”.\nI hope youâ€™re having a great day. #  ì¢‹ì€ ë‚  ë³´ë‚´ì‹œê¸¸ ë°”ë˜ìš”.\nHi, hope youâ€™re having a good day. #  ì•ˆë…• ì¢‹ì€ë‚  ë³´ë‚´ê¸¸ ë°”ë˜.\nI hope you enjoyed your lunch. #  ì ì‹¬ì‹ì‚¬ ë§›ìˆê²Œ í•˜ì‹œê¸¸ ë°”ë˜ìš”.\nHope you had a pleasant flight. #  ì¦ê±°ìš´ ë¹„í–‰ ë˜ì‹œê¸¸ ë°”ë˜ìš”.\nWhen they are literally on their deathbeds. Number one. I wish I hadnâ€™t worked so hard. #  ì •ë§ ì„ì¢…ì„ ì•ë‘ì—ˆì„ë•Œ. ì²«ë²ˆì§¸. ê·¸ë ‡ê²Œ ì—´ì‹¬íˆ ì¼í•˜ì§€ ë§ê±¸.\nYou must be Ms.Lee. Please call me Sunja. #  ì´ì–‘ì´ì‹œêµ°ìš”. ìˆœìë¼ ë¶ˆëŸ¬ì£¼ì„¸ìš”.\nMrs\u0026hellip;(Should) at least be Ms.? #  ë¯¸ì„ìŠ¤\u0026hellip; ìµœì†Œí•œ ë¯¸ì“°ë„¤.\nPleasure to meet you Ms. Bennett. Thatâ€™s Mrs. Bennet, sweetheart. #  ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ë¯¸ìŠ¤ ë²„ë„·ì–‘. ë¯¸ì„ìŠ¤ ë²„ë„·ì´ë„¤. ì´ì–‘ë°˜ì•„.\nNow Mr. Fischer was my fatherâ€™s name. Call me Bert, please. #  ë¯¸ìŠ¤í„° í”¼ì…”ëŠ” ì•„ë²„ì§€ ì´ë¦„ì…ë‹ˆë‹¤. ë²„íŠ¸ë¼ê³  ë¶ˆëŸ¬ì£¼ì„¸ìš”.\nYou look tired today? #  ë„ˆ ì˜¤ëŠ˜ í”¼ê³¤í•˜ë‹ˆ?\nYou look tired. Did you not sleep? #  í”¼ê³¤í•´ ë³´ì´ë„¤. ì ì€ ì¤ì–´?\nYou look tire. Why don\u0026rsquo;t you go home, get some rest. #  í”¼ê³¤í•´ ë³´ì´ëŠ”ë° í‡´ê·¼í•˜ê³  ì§‘ì—ê°€ì„œ ì‰¬ëŠ”ê²Œ ì¢€ ì–´ë•Œ?\nGot you a little pick-me-up. #  ì´ê±° ë¨¹ê³  í˜ë‚´.\nLet me know if there\u0026rsquo;s anything I can do to help. #  ë‚´ê°€ ë„ì™€ ì¤„ ìˆ˜ ìˆëŠ”ê²Œ ë­ë¼ë„ ìˆìœ¼ë©´ ë§í•´ì¤˜.\nI don\u0026rsquo;t care. #  ê´€ì‹¬ ì—†ì–´.\nI don\u0026rsquo;t care how long it takes, I will keep you here all night. #  ì–¼ë§ˆë‚˜ ì˜¤ë˜ê±¸ë¦¬ëŠ”ì§€ ìƒê´€ì—†ì–´. ë‚˜ëŠ” ë„ˆí¬ë“¤ì„ ë°¤ìƒˆë„ë¡ ë¶™ì¡ê³  ìˆì„ê±°ì•¼.\nI don\u0026rsquo;t care how long it takes. Just get it done. #  ì–¼ë§ˆë‚˜ ì˜¤ë˜ ê±¸ë¦¬ëŠ”ì§€ ìƒê´€ì—†ì–´. ê·¸ëƒ¥ í•´.\nI don\u0026rsquo;t care about what you think. #  ë„¤ê°€ ë­˜ ìƒê°í•˜ëŠ” ë‚œ ì‹ ê²½ì•ˆì¨.\nYou know, we don\u0026rsquo;t really care about your opinion. #  ìš°ë¦¬ê°€ ë„¤ ì˜ê²¬ì„ ì‹ ê²½ì•ˆì“°ëŠ”ì§€ ì•Œì–ì•„.\nI couldn\u0026rsquo;t care less. #  ì§„ì§œ 1ë„ ê´€ì‹¬ ì—†ì–´.\nDon\u0026rsquo;t care. Counldn\u0026rsquo;t care less. #  ì‹ ê²½ì•ˆì¨. ì§„ì§œ ê´€ì‹¬ì—†ì–´.\nI really couldn\u0026rsquo;t care less. #  ì§„ì§œ 1ë„ ê´€ì‹¬ì—†ì–´.\nAnything. It doesn\u0026rsquo;t matter to me. #  ë­ë“  ìƒê´€ì—†ì–´.\nI\u0026rsquo;m good/fine with whatever. #  ë­ë“  ì¢‹ì•„.\nIt doesn\u0026rsquo;t matter to me, your choice. #  ìƒê´€ì—†ìœ¼ë‹ˆ ë‹ˆê°€ ê³¨ë¼.\nLet\u0026rsquo;s eat. I\u0026rsquo;m good with whatever. #  ë¨¹ìê³ . ë‚˜ëŠ” ë­ë“  ìƒê´€ì—†ì–´.\nAnywhere. It doesn\u0026rsquo;t matter to me. #  ì–´ë””ë“  ìƒê´€ì—†ì–´.\nI\u0026rsquo;m good/fine with wherever. #  ì–´ë””ë“  ì¢‹ì•„.\nAre you with me? #  ë‚´ ë§ ì´í•´í–ˆì–´?\nAre you/we on the same page? #  ì˜ ë”°ë¼ ì˜¤ëŠ” ê±°ì§€? ì´í•´í•˜ëŠ” ê±°ì§€?\nLet me make sure we\u0026rsquo;re on the same page. #  ì œê°€ ì˜ ì´í•´í•œê±´ì§€ í™•ì¸ì°¨ ì—¬ì­¤ë´…ë‹ˆë‹¤.\nWe\u0026rsquo;re 100% on the same page. #  100% ê°™ì€ ìƒê°ì…ë‹ˆë‹¤.\nWe\u0026rsquo;re not on the same page. #  ê°™ì€ ìƒê°ì´ ì•„ë‹ˆì•¼.\nI don\u0026rsquo;t think we\u0026rsquo;re on the same page. #  ê°™ì€ ìƒê°ì´ ì•„ë‹Œê±° ê°™ì•„.\nWe\u0026rsquo;re getting there. #  ì´í•´ê°€ ë˜ì–´ê°€ê³  ìˆë‹¤.\nNot quite. But we\u0026rsquo;re getting there. #  ì•„ì§ì€ ì•„ë‹ˆì§€ë§Œ ì ì  í•©ì˜ì ì— ë„ë‹¬í•˜ê³  ìˆë‹¤.\nAre you on the same page with this matter? We\u0026rsquo;re not so far away. #  ì´ ë¬¸ì œì— ëŒ€í•´ ìƒê°ì´ ê°™ì€ê°€ìš”? ê·¸ë ‡ê²Œ ë‹¤ë¥´ì§„ ì•Šë‹¤.\nYou\u0026rsquo;ll get there. #  ëª©í‘œì— ë„ë‹¬í• ê±°ë‹¤.\nThere\u0026rsquo;s still room for improvement. #  ì—¬ì „íˆ ê°œì„ ì˜ ì—¬ì§€ê°€ ìˆë‹¤.\nI\u0026rsquo;ll get there. #  ëª©í‘œì— ë„ë‹¬í• ê±°ì•¼.\nI feel confident that we\u0026rsquo;ll get there. #  ë‚˜ëŠ” ëª©í‘œì— ë„ë‹¬í•  ìì‹ ê°ì€ ëŠë‚€ë‹¤.\nCould you bring me up to speed? #  ì§€ê¸ˆ ì–´ë–»ê²Œ ì§„í–‰ë˜ê³  ìˆëŠ”ì§€ ì•Œë ¤ì¤„ ìˆ˜ ìˆì–´?\nI can bring you up to speed. #  í˜„ìƒí™©ì„ ë‚´ê°€ ì•Œë ¤ì¤„ê»˜.\nCan I bring you up to speed over dinner? #  ì €ë… ë¨¹ìœ¼ë©´ì„œ ì–´ë–»ê²Œ ëœì§€ ì–˜ê¸° í•´ì¤˜ë„ ë ê¹Œ?\nThen why don\u0026rsquo;t you sit down, and I\u0026rsquo;ll bring you up to speed? #  ê·¸ëŸ¼ ì•‰ëŠ”ê²Œ ì–´ë•Œ? ë‚´ê°€ ì–´ë–¤ ìƒí™©ì¸ì§€ ì´ì•¼ê¸°í• ê²Œ.\nJust to bring everybody up to speed, I\u0026rsquo;ve put together a brief report. #  ëª¨ë‘ì—ê²Œ í˜„ìƒí™©ì„ ì•Œë ¤ë“œë¦¬ê¸° ìœ„í•´ ê°„ë‹¨í•œ ë³´ê³ ì„œë¥¼ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤.\nSo just to bring everybody up to speed, let\u0026rsquo;s define what we\u0026rsquo;re actually talking about. #  ëª¨ë‘ê°€ ì´í•´í•  ìˆ˜ ìˆê²Œ ìš°ë¦¬ê°€ ì‹¤ì œë¡œ ë¬´ì–¸ì— ê´€í•´ ë…¼ì˜ í–ˆëŠ”ì§€ë¥¼ ì •ì˜í•´ë´…ì‹œë‹¤.\nWhere were we? #  ìš°ë¦¬ê°€ ì–´ë””ê¹Œì§€ í–ˆì§€?\nWhere are we? #  ìš°ë¦¬ê°€ ì–´ë–¤ ìƒí™©ì´ì§€?\nWhere are we on the deal? Waiting on documents from overseas. #  ê·¸ ê±°ë˜ëŠ” ì–´ë–¤ìƒí™©ì´ì§€? í•´ì™¸ ì„œë¥˜ë¥¼ ê¸°ë‹¤ë¦¬ê³  ìˆë‹¤.\nWhere are we ont eh process of it? #  í˜„ì¬ ì–´ë””ê¹Œì§€ ì§„í–‰ëœê±°ì£ ?\nWhere are you going with this? #  ë¬´ìŠ¨ ë§ì„ í•˜ë ¤ëŠ”ê±°ì•¼?\nWhat\u0026rsquo;s your point? #  ìš”ì ì´ ë­ë‹ˆ?\nWhere are you going with this? I want a divorce. This isn\u0026rsquo;t working anymore. #  ë¬´ìŠ¨ ë§ì„ í•˜ë ¤ëŠ”ê±°ì•¼? ì´í˜¼ì„ ì›í•´. ì´ê±´ í†µí•˜ì§€ ì•Šì„ê±°ì•¼.\nWhat do you say? #  ì–´ë–»ê²Œ ìƒê°í•´?\nLet\u0026rsquo;s grab a beer. What do you say? #  ë§¥ì£¼ í•œì” í•˜ì. ì–´ë•Œ?\nI think it would be better to go with option A. What do you say? #  Option Aë¡œ í•˜ëŠ”ê²Œ ë” ë‚˜ì„ ê±° ê°™ì€ë° ì–´ë–»ê²Œ ìƒê°í•´?\nWhat do you say we give it a shot? #  í•œë²ˆ ì‹œë„í•´ ë³´ëŠ”ê²Œ ì–´ë•Œ?\nSo, what do you say we give it a try? #  ê·¸ëŸ¬ë‹ˆ í•œë²ˆ ì‹œë„í•´ë³´ëŠ”ê²Œ ì–´ë•Œ?\nWhat do you say we get a drink after work? #  í‡´ê·¼ í›„ ìˆ  í•œì” ì–´ë•Œ?\nWhat do you say we get a drink? #  ìˆ í•œì” ì–´ë•Œ?\nDo you have a minute? Of course. Always. #  ì ê¹ì‹œê°„ ë¼? ê·¸ëŸ¼. ì–¸ì œë“ ì§€.\nYou wanted to see me? #  ë¶ˆë €ë‹ˆ?\nMay I have a word (with you)? #  ì ì‹œ ëŒ€í™” ì¢€ í•  ìˆ˜ ìˆì„ê¹Œ?\nMay I have a work with you in private? #  ì ê¹ ë”°ë¡œ ì–˜ê¸° ì¢€ í•  ìˆ˜ ìˆì„ê¹Œ?\nMay I go to the bathroom? #  í™”ì¥ì‹¤ì¢€ ê°ˆ ìˆ˜ ìˆë‚˜ìš”?\nKeep your word. #  ë„¤ê°€ í•œ ì•½ì† ì§€ì¼œ.\nI\u0026rsquo;ll keep my word. #  ë‚´ê°€ í•œ ì•½ì†ì„ ì§€í‚¤ê² ë‹¤.\nYou gave me your word! #  ì•½ì† í–ˆì–ì•„!\nIt\u0026rsquo;s your call #  ê·¸ê±´ ë„¤ê°€ ê²°ì •í•´\nIt\u0026rsquo;s my call #  ê·¸ê±´ ë‚´ê°€ ê²°ì •í• ê±°ì•¼.\nI\u0026rsquo;m sorry, but it\u0026rsquo;s not my call. #  ë¯¸ì•ˆí•œë° ë‚´ê°€ ê²°ì •í•  ë¶€ë¶„ì´ ì•„ë‹ˆë‹¤.\nWho would you fire? Not my call. #  ëˆ„êµ¬ë¥¼ í•´ê³ í•˜ê² ë‚˜? ë‚´ê°€ ê²°ì •í• ê²Œ ì•„ë‹ˆì•¼.\nI\u0026rsquo;m not in a position to make that decision. #  ê·¸ëŸ° ê²°ì •ì„ ë‚´ë¦´ ì…ì¥ì´ ì•„ë‹ˆì•¼.\nFor here or to go? #  ë“œì‹œê³  ê°€ì‹¤ê±´ê°€ìš”? ê°€ì ¸ê°ˆê±´ê°€ìš”?\nDid you make a reservation? #  ì˜ˆì•½ í•˜ì…¨ë‚˜ìš”?\nTable or booth? #  í…Œì´ë¸” ë˜ëŠ” ë¶€ìŠ¤ ì–´ë””ë¡œ?\nYour group\u0026rsquo;s on the patio. I\u0026rsquo;ll show you the way. #  ì¼í–‰ì´ Patioì— ìˆì–´ìš”. ì œê°€ ì•ˆë‚´ í•´ë“œë¦´ê»˜ìš”.\nHow many in your party? #  ì¼í–‰ì´ ì´ ëª‡ ë¶„ì´ì„¸ìš”?\nTable for 2 under Youn. #  Younìœ¼ë¡œ 2ëª… í…Œì´ë¸” ì˜ˆì•½í–ˆì–´ìš”.\nDid you make a reservation? Yes, I did. It should be under You. #  ì˜ˆì•½ í•˜ì…¨ë‚˜ìš”? ì˜ˆ. ìœ¤ìœ¼ë¡œ ì˜ˆì•½ë˜ì–´ ìˆì„ ê±°ì˜ˆìš”.\nTwo rooms under Youn. #  ìœ¤ìœ¼ë¡œ ë°© ë‘ê°œ ì˜ˆì•½í–ˆìŠµë‹ˆë‹¤.\nI made a reservation. #  ì˜ˆì•½ í–ˆìŠµë‹ˆë‹¤. (ë‹¨ë…)\nI reserved a table. I reserved a room. #  í…Œì´ë¸”ì„ ì˜ˆì•½í–ˆìŠµë‹ˆë‹¤. (ëŒ€ìƒ)\nI thought we reserved the conference room. #  ìš°ë¦¬ê°€ íšŒì˜ì‹¤ì„ ì˜ˆì•½í•˜ì§€ ì•Šì•˜ë‚˜?\nTap or bottled (water)? #  ìˆ˜ë—ë¬¼ ì•„ë‹ˆë©´ ë³‘ë¬¼ ë“œë ¤ìš”?\nTap water\u0026rsquo;s fine. #  ìˆ˜ë—ë¬¼ ì£¼ì…”ë„ ë˜ìš”.\nDraft or bottled? #  ìƒë§¥ ë˜ëŠ” ë³‘ë§¥?\nDraft beer here. #  ì—¬ê¸° ìƒë§¥ ì¤˜ìš”.\nWould you like to start a tab? #  (ì” ë‹¹ ê³„ì‚°ì´ ì•„ë‹Œ) í•œë²ˆì— ê³„ì‚°í•˜ëŠ”ê±° í•´ì¤„ê¹Œ?\nDo you wanna pay cash or start a tab? #  ì”ë‹¹ ë§ˆì‹¤ë˜ ì•„ë‹ˆë©´ í•œêº¼ë²ˆì— ê³„ì‚°í•´ì¤„ê¹Œ?\nI\u0026rsquo;d like to close my tab. #  (ë‹¤ ë§ˆì…¨ìœ¼ë‹ˆ) ì´ì œ ê³„ì‚°í• ê²Œìš”.\nI haven\u0026rsquo;t bee able to find your happy hour menu. #  í•´í”¼ì•„ì›Œ ë©”ë‰´ê°€ ì•ˆë³´ì—¬ìš”.\nIt\u0026rsquo;s on the house. #  ì´ê±° ì„œë¹„ìŠ¤ì•¼.\nIt\u0026rsquo;s on us. #  ì´ê±° ì„œë¹„ìŠ¤ì•¼.\nIt\u0026rsquo;s on me. #  ë‚´ê°€ ì ê»˜.\nMy treat. #  ë‚´ê°€ ëŒ€ì ‘í• ê»˜.\n(The) sky\u0026rsquo;s the limit. #  (í•œê³„ ì—†ì´) ì‹œí‚¤ê³  ì‹¶ì€ê±° ë‹¤ ì‹œì¼œ.\nI\u0026rsquo;m getting married!. Here we go. First round\u0026rsquo;s on the house. #  ì € ê²°í˜¼í•´ìš”. ì—¬ê¸°ìš”. ì²« ì”ì€ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.\nDoes it come with drink? #  ìŒë£Œë„ ê°™ì´ ë”¸ë ¤ ë‚˜ì˜¤ë‚˜ìš”?\nDoes that come with cheese? #  ì¹˜ì¦ˆë„ ê°™ì´ ë‚˜ì˜¤ë‚˜ìš”?\nIt comes with two sides. #  ì‚¬ì´ë“œ ë‘ê°œ ê°™ì´ ë”¸ë ¤ ë‚˜ì™€ìš”.\nCan you give us a minute? #  (ì¼ë¶„ì •ë„) ì ì‹œ ì‹œê°„ ì¢€ ì£¼ì‹¤ë˜ìš”?\nCan you give us another minute? #  ìƒê°í•  ì‹œê°„ ì¢€ ë” ì£¼ì„¸ìš”.\nCan I have the sauce on the side? #  ì†ŒìŠ¤ëŠ” ì‚¬ì´ë“œì— ë”°ë¡œ ì£¼ì‹¤ë˜ìš”?\nCan I have the sauce without mayo? #  ë§ˆìš”ë„¤ì¦ˆëŠ” ë¹¼ì¤„ë˜ìš”?\nCan I have the extra sauce? #  ì†ŒìŠ¤ ì¢€ ë” ì¤„ë˜ìš”?\nCan I have the dressing on the side? #  ë“œë ˆì‹±ì€ ë”°ë¡œ ì£¼ì‹¤ë˜ìš”?\nTwo green salads, dressing on the side. #  ê·¸ë¦° ìƒëŸ¬ë“œ ë‘ê°œì— ë“œë ˆìŠ¤ëŠ” ì‚¬ì´ë“œë¡œ ì¤˜ìš”.\nWould you like it toasted? #  êµ¬ì›Œë“œë¦´ê¹Œìš”?\nWhat kind of dressing would you like? (Ranch, Balamic vinaigrette, Thousand island,italian,blue cheese) #  ì–´ë–¤ ë“œë ˆì‹±ìœ¼ë¡œ í• ê¹Œìš”?\nCan I have a to-go box? #  í¬ì¥í•  ìˆ˜ ìˆëŠ” ìš©ê¸° ì¢€ ì–»ì„ ìˆ˜ ìˆì„ê¹Œìš”?\nCan I have a to-go cup? #  í¬ì¥í•  ìˆ˜ ìˆëŠ” ì»µ ì¢€ ì–»ì„ ìˆ˜ ìˆì„ê¹Œìš”?\nHi, could I get these in to-go cups? #  ì´ê²ƒ ì¢€ í¬ì¥ìš© ì»µì— ë„£ì–´ ì¤„ë˜ìš”?\nCan I have/get this in a to-go cup? #  ì´ê²ƒ ì¢€ í¬ì¥ í•´ì£¼ì‹¤ë˜ìš”?\nCan I have/get a to-go cup? #  í¬ì¥ìš© ì»µ ì¢€ ì£¼ì‹¤ë˜ìš”?\nI heated up some leftover lasagne. #  ë‚¨ì€ ë¼ìëƒ ì¢€ ë°ì›Œ ì™”ì–´.\nIs this together or separate? #  ê°™ì´ ë‚¼ê±´ê°€ìš”? ë”°ë¡œ ë‚¼ê±´ê°€ìš”?\nSeparate or together? Together. #  ë”°ë¡œë‚¼ê±´ê°€ìš”? ê°™ì´ ë‚¼ê±´ê°€ìš”? ê°™ì´ ë‚¼ê²Œìš”.\nCould we split the check, please? #  ì´ê±° Në¶„ í•´ì„œ ê³„ì‚° í•´ì¤„ë˜ìš”?\nDoes anybody wanna split the chocolate pudding? #  ë‚˜ì™€ ì´ˆì½œë › í‘¸ë”© ë‚˜ëˆ ë¨¹ê³  ì‹¶ì€ ì‚¬ëŒ ìˆì–´?\nIt\u0026rsquo;s all about family. #  ê°€ì¡±ì´ ê°€ì¥ ì¤‘ìš”í•´.\nIt\u0026rsquo;s all about making connections. #  ì¸ë§¥ì„ ìŒ“ëŠ”ê²Œ ê°€ì¥ ì¤‘ìš”í•´.\nIt\u0026rsquo;s all about teamwork. #  íŒ€ì›ì´ ê°€ì¥ ì¤‘ìš”í•´.\nIt\u0026rsquo;s all about location. #  ìœ„ì¹˜ê°€ ê°€ì¥ ì¤‘ìš”í•´.\nIt\u0026rsquo;s all about confidence. #  ìì‹ ê°ì´ ê°€ì¥ ì¤‘ìš”í•´.\nCooking\u0026rsquo;s all about confidence. #  ìš”ë¦¬ì—ì„œëŠ” ìì‹ ê°ì´ ê°€ì¥ ì¤‘ìš”í•´.\nIt\u0026rsquo;s not about money. #  ëˆì´ ì¤‘ìš”í•œê²Œ ì•„ë‹ˆì•¼.\nIt\u0026rsquo;s not about looks. #  ì™¸ëª¨ê°€ ì¤‘ìš”í•œê²Œ ì•„ë‹ˆì•¼.\nChristmas is all about giving. #  í¬ë¦¬ìŠ¤ë§ˆìŠ¤ëŠ” ë² í‘¸ëŠ”ê²Œ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤.\nBecause it\u0026rsquo;s not about being perfect, it\u0026rsquo;s about trying. #  ì™„ë²½í•˜ê²Œ í•˜ëŠ”ê²Œ ì¤‘ìš”í•œê²Œ ì•„ë‹ˆë¼ ì‹œë„í•´ë³´ëŠ”ê²Œ ì¤‘ìš”í•œê±°ì•¼.\nI\u0026rsquo;m not interested. #  ê´€ì‹¬ ì—†ì–´ìš”.\nI\u0026rsquo;ll think about it. #  ìƒê°í•´ë³¼ê²Œìš”.\nI don\u0026rsquo;t want to buy that. #  ê·¸ê±° ì‚¬ê³  ì‹¶ì§€ ì•Šì•„ìš”.\nI\u0026rsquo;m not in a position to make that purchase. #  ê·¸ëŸ° êµ¬ë§¤ë¥¼ í•  ìƒí™©ì´ ì•„ë‹Œê±¸ìš”.\nI don\u0026rsquo;t buy it. #  ëª» ë¯¿ê² ì–´.\nDo you buy that? #  ë„ˆ ì € ë§ ë¯¿ì–´?\nI don\u0026rsquo;t buy that for a second. #  ê·¸ ë§ ì¡°ê¸ˆë„ ì•ˆë¯¿ì–´.\nAnd she bought it? It was the truth. There was nothing to buy. #  ê·¸ë…€ê°€ ë¯¿ì—ˆì–´? ê·¸ê±´ ì‚¬ì‹¤ì´ì•¼. ë¯¿ê³  ìì‹œê³  í• ê²Œ ì—†ì–´.\nThat\u0026rsquo;s the last thing on my mind. #  ê·¸ê±´ ìƒê°ë„ ì•ˆí•˜ê³  ìˆì–´.\nIt\u0026rsquo;s all about ~ = ~ is my prioirty = I can\u0026rsquo;t emphasize this enough = I can\u0026rsquo;t stress this enough #  ~ê°€ ì¤‘ìš”í•˜ë‹¤.\nSuddenly my column was the last thing on my mind. #  ë”ì´ìƒ ë‚˜ì˜ ì»¬ëŸ¼ì´ ì¤‘ìš”í•˜ì§€ ì•Šê²Œ ë˜ì—ˆë‹¤.\nThe last thing you need is caffeine. #  ë„ˆ ì§€ê¸ˆ ì¹´í˜ì¸ ë§ˆì‹œë©´ ì•ˆë¼.\nThe last thing you need is more stress. #  ë„ˆ ë” ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ìœ¼ë©´ ì•ˆë¼.\n36 #  "});index.add({'id':31,'href':'/docs/documents/english/pickedup/','title':"Picked Up English",'section':"English",'content':"English Expression Diary #   A little bird told me #  ë™ë¬¸ì„œë‹µìœ¼ë¡œ \u0026lsquo;ê·¸ëƒ¥ ì£¼ì›Œë“¤ì—ˆì–´\u0026rsquo; í‘œí˜„. \u0026lsquo;Tell me. Where did you hear that?\u0026rsquo; \u0026lsquo;A little bird told me.\u0026rsquo;\nBigger fish to fry #  \u0026lsquo;ì¢€ ë” ì¤‘ìš”í•˜ê³  í° ë‹¤ë¥¸ ê²ƒ\u0026rsquo;ì´ë€ ì˜ë¯¸. \u0026lsquo;Why did you turn down their offer?\u0026rsquo; \u0026lsquo;Oh, I have bigger fish to fry.\u0026rsquo;\nCan of Worms #  \u0026lsquo;ì‹¬ê°í•œ ë¬¸ì œë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆëŠ” ìœ„í—˜\u0026rsquo;. \u0026lsquo;What do you think about the new tax law?\u0026rsquo; \u0026lsquo;I think it\u0026rsquo;s opening a can of worms.\u0026rsquo;\nCuriousity kills the cat #  \u0026lsquo;ê´œí•œ í˜¸ê¸°ì‹¬ì´ í™”ë¥¼ ë¶€ë¥¸ë‹¤\u0026rsquo;ë¼ëŠ” í‘œí˜„. \u0026lsquo;I want to know who\u0026rsquo;s behind the plot.\u0026rsquo; \u0026lsquo;Curiosity kills the cat. Keep that in mind.\u0026rsquo;\nStop and kick every dog that barks at you #  \u0026lsquo;ì¡°ê¸ˆì´ë¼ë„ íƒ€ì¸ì´ ë“£ê¸° ì‹«ì€ ì†Œë¦¬í•˜ëŠ”ê±¸ ëª»ì°¸ì•„ í•˜ëŠ” ê²ƒ\u0026rsquo;ì„ ì˜ë¯¸. \u0026lsquo;Don\u0026rsquo;t stop and kick every dog that barks at you. You are wasting your time\u0026rsquo; \u0026lsquo;I know. I know. But sometimes it really gets on my nerves.\u0026rsquo;\nTry telling me English is easy. #  \u0026lsquo;Try me\u0026rsquo;ëŠ” \u0026lsquo;How you dare\u0026rsquo;ì˜ ëœ» \u0026ldquo;ì–´ë”” í•œë²ˆ ì˜ì–´ê°€ ì‰½ë‹¤ê³  ë§í•´ë³´ì‹œì§€?\u0026rdquo; ì •ë„.\nEase into #  ì²œì²œíˆ ë“¤ë“¯ì´ ì ì‘í•˜ë‹¤ëŠ” ì˜ë¯¸.\nFeel insecure about someone\u0026rsquo;s appearance #  ì™¸ëª¨ì— ìì‹ ê°ì´ ì—†ë‹¤ëŠ” ëœ».\nElbow me in the rib #  ì˜†êµ¬ë¦¬ë¥¼ ì¿¡ì¿¡ ì°Œë¥¸ë‹¤ëŠ” ì˜ë¯¸.\nBe good at #  ~ ì˜í•˜ë‹¤. \u0026ldquo;I\u0026rsquo;m not good at English.\u0026rdquo;\nStay safe #  í•­ìƒ ì¡°ì‹¬í•´. Be Safe. ì¡°ì‹¬í•´. ì˜ì§€ë‚´. ì •ë„.\nI don\u0026rsquo;t want your help to go in vain. #  \u0026ldquo;ê´œí•œì§“ í•˜ì§€ ì•Šìœ¼ì…”ë„ ë˜ìš”\u0026rdquo; ì •ë„.\nThe mortifying ordeal of being known. #  \u0026ldquo;ì•Œë ¤ì¡Œë‹¤ëŠ” ìˆ˜ì¹˜ìŠ¤ëŸ¬ìš´ ê·¸ ê³ í†µ\u0026rdquo;.\nThat\u0026rsquo;s so cheesy! #  ì˜¤ê¸€ê±°ë¦°ë‹¤ëŠ” í‘œí˜„\nHe is so lucky his face does it for him when he has an attitude. #  \u0026ldquo;ì•½ê°„ ê±°ë“¤ë¨¹ ê±°ë ¤ë„ ê·¸ì˜ ì–¼êµ´ì´ ë°›ì³ì£¼ë‹ˆ ì°¸ í–‰ìš´ì•„ë„¤\u0026rdquo; ì •ë„.\nWhy would you do that?! #  \u0026ldquo;ì™œ ê·¸ëŸ´ë ¤ê³  í•˜ëŠ”ë°?\u0026ldquo;ì™€ \u0026ldquo;ì™œ ê·¸ë¬ë‹ˆ\u0026rdquo; ì •ë„.\nGod complex #  ì‹  ì»´í”Œë ‰ìŠ¤ì´ë„¤ìš”. ìì‹ ì´ ì‹  ë˜ëŠ” ë©”ì‹œì•„ë¼ê³  ë¯¿ëŠ” ê³¼ëŒ€ë§ìƒ.\nShe is so catch. #  íŠ•ê¸´ë‹¤ëŠ” í‘œí˜„.\nThe ramen will get mushy and soggy. #  \u0026ldquo;ë¼ë©´ ë¶ˆê² ë‹¤\u0026quot;ëŠ” í‘œí˜„.\nMirror selfie #  ì „ì‹ ì„ ê±°ìš¸ë¡œ ì°ëŠ” ì…€ì¹´.\nShowing off #  ìƒìƒ‰ë‚¸ë‹¤ëŠ” í‘œí˜„.\nBe possessed by a ghost #  ê·€ì‹ ë“¤ë ¸ë‹¤ëŠ” í‘œí˜„.\nhigh-maintenance #  ì†ì´ ë§ì´ ê°„ë‹¤ëŠ” í‘œí˜„.\nI\u0026rsquo;m practicing passive listening while I study the bible. #  ì„±ê²½ì„ ê³µë¶€í•˜ë©° í˜ë ¤ë“£ê¸°ë¡œ ë“£ê¸° ì—°ìŠµì„ í•œë‹¤.\nKeep your eyes peeled for my message coming soon. #  ë‚´ê°€ ë‹µì¥í• ë•Œê¹Œì§€ ëˆˆ ë˜‘ë°”ë¡œ ëœ¨ê³  ì˜ë³´ê³  ìˆì–´.\nCheeky #  ì•„ì£¼ ë°‰ì§€ ì•Šì€ ì–„ë¯¸ìš´ê²ƒ.\nBath dipper or pail #  ì†ì¡ì´ ë‹¬ë¦° ë°”ê°€ì§€.\nKeep your bangs #  ì•ë¨¸ë¦¬ ìë¥´ì§€ ë§ˆ\nFolded the paper star #  ì¢…ì´ë³„ ì ‘ëŠ”ë‹¤\nSpirit animal #  ëŒ€í‘œ ì˜í˜¼? ë¡¤ëª¨ë¸?\nTo call on #  ì„ ìƒë‹˜ë“¤ì´ í•™ìƒë“¤í•œí…Œ ì§ˆë¬¸ì„ ë¬»ê¸° ìœ„í•´ ë¶€ë¦„\nHis research put forth 5 hypotheses. #  ê·¸ì˜ ì—°êµ¬ëŠ” ë‹¤ì„¯ê°€ì§€ì˜ ê°€ì„¤ì„ ì œì‹œí•œë‹¤.\nWhat are some cheesy pick-up lines in English? #  ì˜ì–´ë¡œ ì˜¤ê¸€ê±°ë¦¬ëŠ” ì‘ì—…ë©˜íŠ¸ ì•Œë ¤ì£¼ì„¸ìš”.\nSo the most I will do is send a message #  ë‹µê¸€ ë³´ë‚´ëŠ”ê±° ì´ìƒì€ í•˜ì§€ ì•Šê² ë‹¤.\nPiggyback ride #  ì–´ë¶€ë°”\nTo knead rice #  ìŒ€ì„ ë¹šë‹¤\nThe power of someone\u0026rsquo;s glasses #  ì•ˆê²½ ë„ìˆ˜\nWhat\u0026rsquo;s with you #  ë„ˆ ì™œê·¸ëŸ¬ë‹ˆ?\nâ€A smile curves up his lips #  ê·¸ì˜ ì…ê°€ì— ë¯¸ì†Œê°€ ë²ˆì¡Œë‹¤.\nThis cleanser is a splurge #  ì´ í´ë¦°ì €ëŠ” ëˆë‚­ë¹„ì…ë‹ˆë‹¤.\nBullshitting #  ì§€ë„\nAbbreation Form in messanger #  TY : Thank you\nYW : You\u0026rsquo;re welcome\nTTYL : Talk to you later.\nK : Okay.\nSUP : Wat\u0026rsquo;s UP?\nIK : I know.\nDon\u0026rsquo;t tread on me #  Gadsden flag\nAn unearned win / the game by default #  ë¶€ì „ìŠ¹\nLike I used to #  ì˜ˆì „ ê°™ì€\nFor the life of me #  ì•„ë¬´ë¦¬ ìƒê°í•´ë´ë„\u0026rdquo; I can\u0026rsquo;t remember her name for the life of me.\nShoot the breeze #  ìˆ˜ë‹¤ë¥¼ ë–¤ë‹¤\nShoot one\u0026rsquo;s mouth off #  ì£¼ì ˆê±°ë¦°ë‹¤\nI\u0026rsquo;m good #  ë‚œ ëì–´\nDownward spiral #  ì•…ìˆœí™˜\nGwak, Blank expression #  Gwakingì€ ë©í•œ, ì–¼ë¹ ì§„ í‘œì •\nCome over to my place #  ìš°ë¦¬ ì§‘ì— ë†€ëŸ¬ì™€\nOne of the three dog days #  ë³µë‚ \nMaintain composure #  í‰ì •ì‹¬ ìœ ì§€\n"});index.add({'id':32,'href':'/docs/documents/gamedev/','title':"Game Dev",'section':"Documents",'content':"Game Dev #     C   Description   C\u0026#43;\u0026#43;   Description   "});index.add({'id':33,'href':'/posts/210727/','title':"Update ì¬ê°œ",'section':"Blog",'content':" ë” ì•ˆì“°ë‹¤ê°€ëŠ” ìŠì–´ë¨¹ì„ ë“¯ í•˜ì—¬ ì¡°ê¸ˆì´ë¼ë„ ì—…ë°ì´íŠ¸ë¥¼ ì¬ê°œí•¨. ë¬´ë”ìœ„ ì™€ì¤‘ RUSTë¥¼ ì¢€ë” ê³µë¶€í•´ë³´ë ¤ê³  ê³„íš ì¤‘. ì¼ë‹¨ í•œë²ˆ í›‘ì–´ë´¤ë‹¤ê³  ìƒê°í–ˆì§€ë§Œ ì•ˆì“°ë‹ˆ ë‹¤ì‹œ ìƒì†Œí•´ì ¸ ë²„ë ¸ìŒ. ì†ì— ìµì„ë•Œê¹Œì§€ ë­”ê°€ë¥¼ í•´ë³´ì•˜ìœ¼ë©´ ì¢‹ê² ëŠ”ë° ê²Œì„ëŸ¬ì„œ ìê¾¸ ëŠìŠ¨í•´ì§€ê²Œ ë¨. ì–´ì°Œë˜ì—ˆê±´ ë‹¤ì‹œ ì¡°ê¸ˆì”©ì´ë¼ë„ ê°±ì‹ í•˜ë ¤ê³  ì†ë³´ê³  ìˆìŒ.\n ë‹¤ìŒê¸€  "});index.add({'id':34,'href':'/posts/201022/','title':"Personal Log",'section':"Blog",'content':" ì „ì²´ ê¸°ë¡ ì´ˆê¸°í™” ì§„í–‰. ì „ì²´ ì–´ì¡° ë³€ê²½ ì§„í–‰. ë‹¹ ì‚¬ì´íŠ¸ëŠ” ê°œì¸ ê¸°ë¡ ìš©ë„ë¡œ ì „í™˜. ë¸”ë¡œê·¸ì—ì„œ ì¼ìê°€ ê³¼ê±°ì¼ ê²½ìš° ì¡°íšŒê°€ ë˜ì§€ ì•ŠëŠ” ë‚´ìš© í™•ì¸. ë¸”ë¡œê·¸ ìš”ì•½ í‘œì‹œëŠ” ì „ë°˜ 400ì ì •ë„ë¥¼ ì¶”ì¶œí•˜ì—¬ í‘œì‹œ. BOOK í…Œë§ˆ ì›ë³¸ ìˆ˜ì • ì¡°ì¹˜ (Summary -\u0026gt; Description ì „í™˜)\n ë‹¤ìŒê¸€  "});index.add({'id':35,'href':'/posts/200928/','title':"Hello World!",'section':"Blog",'content':" Software Programing Languageí•œ ì§€ê¸ˆê¹Œì§€ ë‹¤ë£¨ì–´ ì–¸ì–´ëŠ” ì´ 21ê°œ í•­ëª©. ASP.NET, BASIC, C, C#, C++, Coldfusion, Command Script, CSS, Go, HTML, Java, Javascript, PHP, ProC, Python3, PL/SQL, Rust, SQL, Unix Shell, VBScript. ì§ë¬´ ê´€ë ¨ ì–¸ì–´ ëª‡ê°€ì§€ë¥¼ ì œì™¸í•˜ë©´ ëª¨ë‘ë¥¼ ììœ ë¡­ê²Œ ë‹¤ë£¨ì§€ëŠ” ëª»í•¨. ìµœê·¼ ê´€ì‹¬ì–¸ì–´ëŠ” Rust,Go,Javascript,Python,Web Assembly ë“±ì´ ìˆìŒ. ì „ì²´ì ìœ¼ë¡œëŠ” ë²”ìš©ì„±ì´ ë†’ì€ ì–¸ì–´ë¥¼ ê¹Šê²Œ ê°€ì ¸ê°€ê¸°ë¥¼ í¬ë§í•˜ë‚˜ ëª©í‘œê°€ ë¶ˆë¶„ëª…í•´ ì–¸ì–´ì‚¬ì´ë¥¼ ë°©í™©í•˜ê³  ìˆëŠ” ë“¯í•œ ì†Œê²¬ì„. í•˜ê¸° ì»¬ë ‰ì…˜ ì‚¬ì´íŠ¸ë¥¼ ì°¸ì¡°í•˜ì—¬ \u0026lsquo;Hello World\u0026rsquo; ìƒì„± ì½”ë“œ ì‘ì„±í•˜ì˜€ìŒ.\nhelloworldcollection\nASP.NET #  \u0026lt;%= \u0026#34;Hello World!\u0026#34; %\u0026gt;  ì–¸ì œ : ë°€ë ˆë‹ˆì—„ ì¦ˆìŒ\u0026hellip;\nì´ìœ  : ê°œì¸ í™ˆí˜ì´ì§€ ì œì‘\u0026hellip;\n BASIC #  10 PRINT \u0026#34;Hello World!\u0026#34;  ì–¸ì œ : ì¤‘í•™êµ\u0026hellip; ì´ìœ  : í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²Œì„ ì œì‘\u0026hellip;\n C (Windows) #  #include \u0026lt;windows.h\u0026gt; int PASCAL WinMain(HINSTANCE hInstance, HINSTANCE hPrevInstance, LPSTR CmdLine, int Show) { MessageBox(GetActiveWindw(), \u0026#34;Hello World!\u0026#34;, \u0026#34;Hello Windows\u0026#34;, MB_OK); return 0; }  ì–¸ì œ : ëŒ€í•™êµ\u0026hellip; ì´ìœ  : ìœˆë„ìš° API ê³µë¶€\u0026hellip;\n C# #  class HelloWorld { static void Main() { System.Console.WriteLine(\u0026#34;Hello, World!\u0026#34;); } }  ì–¸ì œ : 2009ë…„ 5ì›”\u0026hellip;\nì´ìœ  : ëª¨ë¸ë§ íˆ´ ì œì‘\u0026hellip;\n C++ (.NET) #  using namespace System; void main() { console::WriteLine(\u0026#34;Hello World!\u0026#34;); }  ì–¸ì œ : 2003ë…„ 11ì›”\u0026hellip;\nì´ìœ  : ì´ì†Œë©”íŠ¸ë¦­ ê²Œì„ ì œì‘\u0026hellip;\n ColdFusion #  \u0026lt;cfset message = \u0026#34;Hello World!\u0026#34;\u0026gt; \u0026lt;cfoutput\u0026gt; #message# \u0026lt;/cfoutput\u0026gt;  ì–¸ì œ : ì„ì‚¬\u0026hellip;\nì´ìœ  : ê³¼ì œ\u0026hellip;\n CommandScript #  echo \u0026#34;Hello World!\u0026#34;  ì–¸ì œ : ê°€ë”\u0026hellip;\nì´ìœ  : íŒŒì¼ì •ë¦¬\u0026hellip;\n CSS #  body:before { content: \u0026#34;Hello World!\u0026#34;; }  ì–¸ì œ : ìµœê·¼\u0026hellip;\nì´ìœ  : Frontend ê³µë¶€\u0026hellip;\n Go #  package main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;Hello Wrold!\u0026#34;) }  ì–¸ì œ : ìµœê·¼\u0026hellip;\nì´ìœ  : Backend ê³µë¶€\u0026hellip;\n HTML #  \u0026lt;HTML\u0026gt; \u0026lt;HEAD\u0026gt; \u0026lt;TITLE\u0026gt;Hello World!\u0026lt;TITLE\u0026gt; \u0026lt;/HEAD\u0026gt; \u0026lt;BODY\u0026gt; \u0026lt;H1\u0026gt;Hello World!\u0026lt;/H1\u0026gt; \u0026lt;/BODY\u0026gt; \u0026lt;/HTML\u0026gt;  ì–¸ì œ : ìµœê·¼\u0026hellip;\nì´ìœ  : Frontend ê³µë¶€\u0026hellip;\n Java #  class HelloWorld { static public void main (String args[]) { System.out.println(\u0026#34;Hello World!\u0026#34;); } }  ì–¸ì œ : ëŒ€í•™êµ\u0026hellip; ì´ìœ  : í•™ë¶€ìˆ˜ì—…\u0026hellip;\n JavaScript #  console.log(\u0026#34;Hello World!\u0026#34;)  ì–¸ì œ : ìµœê·¼\u0026hellip;\nì´ìœ  : Frontend ê³µë¶€\u0026hellip;\n PHP #  \u0026lt;?php echo \u0026#39;Hello World!\u0026#39;; ?\u0026gt; ì–¸ì œ : ëŒ€í•™êµ\u0026hellip; ì´ìœ  : ê°œì¸ í™ˆí˜ì´ì§€ ì œì‘\u0026hellip;\n ProC #  #include \u0026lt;stdio.h\u0026gt;EXEC SQL INCLUDE SQLCA; int main() { char hello[15]; char *user = \u0026#34;user\u0026#34;; char *pass = \u0026#34;password\u0026#34;; char *sid = \u0026#34;the_sid\u0026#34;; EXEC SQL CONNECT :user IDENTIFIED BY :password USING :sid; EXEC SQL SELECT \u0026#39;Hello World!\u0026#34; INTO :hello FROM DUAL; printf(\u0026#34;%s\\n\u0026#34;,hello) EXEC SQL COMMIT RELEASE; return 0; }  ì–¸ì œ : 2005ë…„ë¶€í„° 2010ë…„ê¹Œì§€\u0026hellip;\nì´ìœ  : íšŒì‚¬ ì—…ë¬´\u0026hellip;\n Python3 #  print(\u0026#34;Hello World!\u0026#34;)  ì–¸ì œ : ìµœê·¼\u0026hellip;\nì´ìœ  : ë”¥ëŸ¬ë‹ ê³µë¶€\u0026hellip;\n PL/SQL (Oracle) #  begin dbms_output.enable(1000); dbms_output.put_line(\u0026#39;Hello World!\u0026#39;); end;  ì–¸ì œ : 2005ë…„ ì´í›„\u0026hellip;\nì´ìœ  : íšŒì‚¬ ì—…ë¬´\u0026hellip;\n Rust #  fn main () { println!(\u0026#34;Hello World!\u0026#34;); }  ì–¸ì œ : ìµœê·¼\u0026hellip;\nì´ìœ  : Backend ê³µë¶€\u0026hellip;\n SQL (Oracle) #  select \u0026#39;Hello World\u0026#39; from dual;  ì–¸ì œ : 2005ë…„ ì´í›„\u0026hellip;\nì´ìœ  : íšŒì‚¬ ì—…ë¬´\u0026hellip;\n Unix Shell #  echo Hello World  ì–¸ì œ : 2005ë…„ë¶€í„° 2010ë…„ê¹Œì§€\u0026hellip;\nì´ìœ  : íšŒì‚¬ ì—…ë¬´\u0026hellip;\n VBScript #  MsgBox \u0026#34;Hello world!\u0026#34;  ì–¸ì œ : 2010ë…„ë¶€í„° 2017ë…„ê¹Œì§€\u0026hellip;\nì´ìœ  : íšŒì‚¬ ì—…ë¬´\u0026hellip;\n  ë‹¤ìŒê¸€  "});})();