<!DOCTYPE html>
<html lang="ko" dir=>

<head>
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Description">
<meta name="theme-color" content="#FFFFFF"><meta property="og:title" content="Tensorflow" />
<meta property="og:description" content="Description" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://prokoptasis.github.io/docs/documents/deeplearning/tensorflow/" />

<title>Tensorflow | Prokoptasis</title>
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="/book.min.001ba32d1ef4d2469d055e28d66fd96b16bfeddee698f1e2729af3a54ed86b8b.css" integrity="sha256-ABujLR700kadBV4o1m/Zaxa/7d7mmPHicprzpU7Ya4s=">
<script defer src="/ko.search.min.d4d788e81c0bdda964c3fea379c7883684cb650d2303b8131f03763f33f4dd70.js" integrity="sha256-1NeI6BwL3alkw/6jeceINoTLZQ0jA7gTHwN2PzP03XA="></script>

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-176412981-2', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
<link rel="alternate" type="application/rss+xml" href="http://prokoptasis.github.io/docs/documents/deeplearning/tensorflow/index.xml" title="Prokoptasis" />
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
 
</head>

<body dir=>
    <input type="checkbox" class="hidden toggle" id="menu-control" />
    <input type="checkbox" class="hidden toggle" id="toc-control" />
    <main class="container flex">
        <aside class="book-menu">
             <nav>
<h2 class="book-brand" style="text-align: center;">
    <a href="/"><span>Prokoptasis</span>
  </a>
</h2>
<hr>

<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>





 


  



  
  <ul>
    
      
        <li class="book-section-flat" >
          
  
  

  
    <span>Documents</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-bc65bd57d9bab5039e9a79b5ae591c72" class="toggle"  />
    <label for="section-bc65bd57d9bab5039e9a79b5ae591c72" class="flex justify-between">
      <a href="http://prokoptasis.github.io/docs/documents/projects/" class="">Projects</a>
      <span>▾</span>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/projects/hugogirl/" class="">hU-Go-Girl</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/projects/rust_sdl2/" class="">Rust_SDL2</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/projects/defenders/" class="">Defenders</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/projects/wordfighter/" class="">Wordfighter</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/projects/qmodeler/" class="">Qmodeler</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/projects/cube/" class="">Cube</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/projects/scmtemplate/" class="">SCM Template</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-53481227639f164d3f7ead2411c43315" class="toggle"  />
    <label for="section-53481227639f164d3f7ead2411c43315" class="flex justify-between">
      <a href="http://prokoptasis.github.io/docs/documents/frontend/" class="">Front End</a>
      <span>▾</span>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/frontend/markdown/" class="">Markdown</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-7cff03fc158d9929dd882d45e5e83f84" class="toggle"  />
    <label for="section-7cff03fc158d9929dd882d45e5e83f84" class="flex justify-between">
      <a href="http://prokoptasis.github.io/docs/documents/backend/" class="">Back End</a>
      <span>▾</span>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-0480d4c0a3a48973d66dda9e8e9d50da" class="toggle"  />
    <label for="section-0480d4c0a3a48973d66dda9e8e9d50da" class="flex justify-between">
      <a href="http://prokoptasis.github.io/docs/documents/backend/rust/" class="">Rust</a>
      <span>▾</span>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/backend/rust/rust02/" class="">Rust Advanced</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/backend/rust/rust01/" class="">Rust Basic</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-0d89b8a13399f7331f6db819807b653c" class="toggle"  />
    <label for="section-0d89b8a13399f7331f6db819807b653c" class="flex justify-between">
      <a href="http://prokoptasis.github.io/docs/documents/backend/go/" class="">Go</a>
      <span>▾</span>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/backend/go/go01/" class="">GO Basic</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/backend/go/go02/" class="">GO Advanced</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/backend/go/go03/" class="">GO DB 연결</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-47bd3d4bf94d414a097efcbbc9ad3f79" class="toggle"  />
    <label for="section-47bd3d4bf94d414a097efcbbc9ad3f79" class="flex justify-between">
      <a href="http://prokoptasis.github.io/docs/documents/backend/sql/" class="">SQL</a>
      <span>▾</span>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/backend/sql/sql01/" class="">SQL Basic</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/backend/sql/sql02/" class="">SQL Advanced</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-08315e14cb91d15111d683212f692de5" class="toggle" checked />
    <label for="section-08315e14cb91d15111d683212f692de5" class="flex justify-between">
      <a href="http://prokoptasis.github.io/docs/documents/deeplearning/" class="">Deep Learning</a>
      <span>▾</span>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/deeplearning/machinelearning/" class="">Machine Learning</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/deeplearning/tensorflow/" class=" active">Tensorflow</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-ae112940aeb03e8cb2e77b93d2c01134" class="toggle"  />
    <label for="section-ae112940aeb03e8cb2e77b93d2c01134" class="flex justify-between">
      <a href="http://prokoptasis.github.io/docs/documents/english/" class="">English</a>
      <span>▾</span>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/english/seulsam/" class="">Seulsam</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/english/pickedup/" class="">Picked Up English</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>










 
<ul>
    
    <li>
        <a href="/posts/" >
        Blog
      </a>
    </li>
    
    <li>
        <a href="mailto:c.exigua@gmail.com" target="_blank" rel="noopener" >
        Gmail
      </a>
    </li>
    
    <li>
        <a href="https://github.com/prokoptasis/prokoptasis" target="_blank" rel="noopener" >
        Github
      </a>
    </li>
    
    <li>
        <a href="https://twitter.com/prokoptasis" target="_blank" rel="noopener" >
        Twitter
      </a>
    </li>
    
    <li>
        <a href="https://www.facebook.com/c.exigua/" target="_blank" rel="noopener" >
        Facebook
      </a>
    </li>
    
</ul>
<br>
<hr>
<div style="display: block; text-align: center;">
    <script type="text/javascript" src="https://cdnjs.buymeacoffee.com/1.0.0/button.prod.min.js" data-name="bmc-button" data-slug="prokoptasis" data-color="#e9e9e9" data-emoji="" data-font="Arial" data-text="" data-outline-color="#000" data-font-color="#000"
        data-coffee-color="#6f4e37"></script>
</div>
  

</nav>




  <script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script>

 
            
        </aside>

        <div class="book-page">
            <header class="book-header">
                 <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>Tensorflow</strong>

  <label for="toc-control">
    
  </label>
</div>
  
                
            </header>

            
<article class="markdown"><h2 id="tensor-flow">
  Tensor Flow
  <a class="anchor" href="#tensor-flow">#</a>
</h2>
<hr>
<p>텐저 플로우는 2015년 구글에서 공개한 머신러닝 라이브러리. 케라스는 딥러닝 라이브러리를 Backend로 하는 신경망 모델 구성 라이브러리. 구글은 쥬피터 노트북이라는 오픈 소스 웹 어플리케이션을 코랩이라는 서비스를 통해 제공하고 있음.</p>
<p><a href="https://www.tensorflow.org/?hl=ko" target="_blank" rel="noopener">Tensor Flow</a><br>
<a href="https://keras.io/" target="_blank" rel="noopener">Keras</a><br>
<a href="https://www.python.org/" target="_blank" rel="noopener">Python</a><br>
<a href="https://jupyter.org/" target="_blank" rel="noopener">Jupyter Notebook</a><br>
<a href="https://colab.research.google.com/notebooks/intro.ipynb" target="_blank" rel="noopener">Colab</a></p>
<p>이때 Tensor는 흘러다니는 데이터를 의미함.</p>


<script src="/mermaid.min.js"></script>

  <script>mermaid.initialize({
  "flowchart": {
    "useMaxWidth":true
  },
  "theme": "default"
}
)</script>




<p class="mermaid text-center">
graph LR
A((X:Tensor))-->|Edge|C((+:Node))-->|Edge|D((X+Y:Tensor))
B((Y:Tensor))-->|Edge|C

style A fill:#ffffff,stroke:#000000,stroke-width:1px
style B fill:#ffffff,stroke:#000000,stroke-width:1px
style C fill:#ffffff,stroke:#000000,stroke-width:1px
style D fill:#ffffff,stroke:#000000,stroke-width:1px
</p>

<h3 id="모두를-위한-딥러닝-시즌2">
  모두를 위한 딥러닝 시즌2
  <a class="anchor" href="#%eb%aa%a8%eb%91%90%eb%a5%bc-%ec%9c%84%ed%95%9c-%eb%94%a5%eb%9f%ac%eb%8b%9d-%ec%8b%9c%ec%a6%8c2">#</a>
</h3>
<p>아래는 <a href="https://www.youtube.com/playlist?list=PLQ28Nx3M4Jrguyuwg4xe9d9t2XE639e5C" target="_blank" rel="noopener">모두를 위한 딥러닝 시즌2</a>의 Tensorflow 2 Code를 분석한 내용임.</p>

<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-02-1</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np

<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)

<span style="color:#75715e"># x_data, y_data 정의</span>
x_data <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>]
y_data <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>]

<span style="color:#75715e"># matplot import</span>
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
plt<span style="color:#f92672">.</span>plot(x_data, y_data, <span style="color:#e6db74">&#39;o&#39;</span>)
plt<span style="color:#f92672">.</span>ylim(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">8</span>)

<span style="color:#75715e"># reduce_mean의 차원감소 평균 사용</span>
v <span style="color:#f92672">=</span>[<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">2.</span>, <span style="color:#ae81ff">3.</span>, <span style="color:#ae81ff">4.</span>]
tf<span style="color:#f92672">.</span>reduce_mean(v) <span style="color:#75715e"># 2.5</span>

<span style="color:#75715e"># 3의 제곱근</span>
tf<span style="color:#f92672">.</span>square(<span style="color:#ae81ff">3</span>) <span style="color:#75715e"># 9</span>

<span style="color:#75715e"># x_data, y_data 정의</span>
x_data <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>]
y_data <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>]

<span style="color:#75715e"># Weight Bias 정의</span>
W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(<span style="color:#ae81ff">2.0</span>)
b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(<span style="color:#ae81ff">0.5</span>)

<span style="color:#75715e"># 가설 정의</span>
hypothesis <span style="color:#f92672">=</span> W <span style="color:#f92672">*</span> x_data <span style="color:#f92672">+</span> b

<span style="color:#75715e"># Weight Bias Numpy 출력</span>
W<span style="color:#f92672">.</span>numpy(), b<span style="color:#f92672">.</span>numpy()

<span style="color:#75715e"># Hypothesis Numpy 출력</span>
hypothesis<span style="color:#f92672">.</span>numpy()

<span style="color:#75715e"># x_data, hypothesis -&gt; R</span>
<span style="color:#75715e"># x_data, y_data -&gt; O</span>
plt<span style="color:#f92672">.</span>plot(x_data, hypothesis<span style="color:#f92672">.</span>numpy(), <span style="color:#e6db74">&#39;r-&#39;</span>)
plt<span style="color:#f92672">.</span>plot(x_data, y_data, <span style="color:#e6db74">&#39;o&#39;</span>)
plt<span style="color:#f92672">.</span>ylim(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">12</span>)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># Cost 함수 정의 Average((H-Y)^2)</span>
cost <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>square(hypothesis <span style="color:#f92672">-</span> y_data))

<span style="color:#75715e"># 경사 하강법 Tape 정의</span>
<span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
    hypothesis <span style="color:#f92672">=</span> W <span style="color:#f92672">*</span> x_data <span style="color:#f92672">+</span> b
    cost <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>square(hypothesis <span style="color:#f92672">-</span> y_data))

<span style="color:#75715e"># Weight, Bias</span>
W_grad, b_grad <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(cost, [W, b])
W_grad<span style="color:#f92672">.</span>numpy(), b_grad<span style="color:#f92672">.</span>numpy()

<span style="color:#75715e"># 학습율</span>
learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>

<span style="color:#75715e"># Weight - (Weight * Learning Rate)</span>
W<span style="color:#f92672">.</span>assign_sub(learning_rate <span style="color:#f92672">*</span> W_grad)

<span style="color:#75715e"># Bias - (Bias * Learning Rate)</span>
b<span style="color:#f92672">.</span>assign_sub(learning_rate <span style="color:#f92672">*</span> b_grad)

W<span style="color:#f92672">.</span>numpy(), b<span style="color:#f92672">.</span>numpy()

plt<span style="color:#f92672">.</span>plot(x_data, hypothesis<span style="color:#f92672">.</span>numpy(), <span style="color:#e6db74">&#39;r-&#39;</span>)
plt<span style="color:#f92672">.</span>plot(x_data, y_data, <span style="color:#e6db74">&#39;o&#39;</span>)
plt<span style="color:#f92672">.</span>ylim(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">12</span>)

<span style="color:#75715e"># Weight / Bias 정의</span>
W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(<span style="color:#ae81ff">2.9</span>)
b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(<span style="color:#ae81ff">0.5</span>)

<span style="color:#75715e"># for loop 수행</span>
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">100</span>):
    <span style="color:#75715e"># Gradient Tap e정의</span>
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
        hypothesis <span style="color:#f92672">=</span> W <span style="color:#f92672">*</span> x_data <span style="color:#f92672">+</span> b
        cost <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>square(hypothesis <span style="color:#f92672">-</span> y_data))

    <span style="color:#75715e"># Weight Bias 정의</span>
    W_grad, b_grad <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(cost, [W, b])

    <span style="color:#75715e"># Weight - (Learning Rate * Weight)</span>
    W<span style="color:#f92672">.</span>assign_sub(learning_rate <span style="color:#f92672">*</span> W_grad)
    <span style="color:#75715e"># Bias - (Learning Rate * Bias)</span>
    b<span style="color:#f92672">.</span>assign_sub(learning_rate <span style="color:#f92672">*</span> b_grad)

    <span style="color:#75715e"># Step 10에서 출력</span>
    <span style="color:#66d9ef">if</span> i <span style="color:#f92672">%</span> <span style="color:#ae81ff">10</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
      <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;{:5}|{:10.4f}|{:10.4f}|{:10.6f}&#34;</span><span style="color:#f92672">.</span>format(i, W<span style="color:#f92672">.</span>numpy(), b<span style="color:#f92672">.</span>numpy(), cost))
      
    <span style="color:#75715e"># x_data, y_data 원본</span>
    plt<span style="color:#f92672">.</span>plot(x_data, y_data, <span style="color:#e6db74">&#39;o&#39;</span>)
    <span style="color:#75715e"># x_data, hypothesis 가설</span>
    plt<span style="color:#f92672">.</span>plot(x_data, hypothesis<span style="color:#f92672">.</span>numpy(), <span style="color:#e6db74">&#39;r-&#39;</span>)
    plt<span style="color:#f92672">.</span>ylim(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">18</span>)

<span style="color:#66d9ef">print</span>(W <span style="color:#f92672">*</span> <span style="color:#ae81ff">5</span> <span style="color:#f92672">+</span> b)
<span style="color:#66d9ef">print</span>(W <span style="color:#f92672">*</span> <span style="color:#ae81ff">2.5</span> <span style="color:#f92672">+</span> b)
</code></pre></div></div>
    </div>
  </label>
</div>


<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-03-1</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># import library</span>
<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np

<span style="color:#75715e"># tensor flow version 확인</span>
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)

<span style="color:#75715e"># X,Y 1,2,3 np array 선언 </span>
X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>])
Y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>])

<span style="color:#75715e"># cost_func 정의 (Python형태)</span>
<span style="color:#75715e"># Weight, X, Y를 인자로 전달</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">cost_func_py</span>(W, X, Y):
    <span style="color:#75715e"># c 를 0으로 초기화</span>
    c <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    <span style="color:#75715e"># X 만큼 Loop를 돌며 (W*X - Y) ^ 2 -&gt; C로 대입</span>
    <span style="color:#75715e"># C (오차 제곱근)을 X로 평균낸 값을 return</span>
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(X)):
        c <span style="color:#f92672">+=</span> (W <span style="color:#f92672">*</span> X[i] <span style="color:#f92672">-</span> Y[i]) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>
    <span style="color:#66d9ef">return</span> c <span style="color:#f92672">/</span> len(X)

<span style="color:#75715e"># cost_func 정의 (tensorflow형태)</span>
<span style="color:#75715e"># Weight, X, Y를 인자로 전달</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">cost_func_tf</span>(W, X, Y):
  hypothesis <span style="color:#f92672">=</span> X <span style="color:#f92672">*</span> W
  <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>square(hypothesis <span style="color:#f92672">-</span> Y))

<span style="color:#75715e"># Weight를 -3부터 5까지 9개의 요소로 제공 </span>
W_values <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linspace(<span style="color:#f92672">-</span><span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">12</span>, num<span style="color:#f92672">=</span><span style="color:#ae81ff">23</span>)
cost_values <span style="color:#f92672">=</span> []

<span style="color:#75715e"># numpy.linspace (start,end,number)</span>
<span style="color:#75715e"># 제공된 Weight, X, Y로 Cost를 계산</span>
<span style="color:#66d9ef">for</span> feed_W <span style="color:#f92672">in</span> W_values:
    curr_cost_py <span style="color:#f92672">=</span> cost_func_py(feed_W, X, Y)    
    curr_cost_tf <span style="color:#f92672">=</span> cost_func_tf(feed_W, X, Y)
    cost_values<span style="color:#f92672">.</span>append(curr_cost_tf)
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;{:6.3f} | {:10.5f} | {:10.5f}&#34;</span><span style="color:#f92672">.</span>format(feed_W, curr_cost_py, curr_cost_tf))

<span style="color:#75715e"># matplot import</span>
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt

<span style="color:#75715e"># figure size 지정</span>
plt<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#34;figure.figsize&#34;</span>] <span style="color:#f92672">=</span> (<span style="color:#ae81ff">8</span>,<span style="color:#ae81ff">6</span>)
plt<span style="color:#f92672">.</span>plot(W_values, cost_values, <span style="color:#e6db74">&#34;b&#34;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Cost(W)&#39;</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;W&#39;</span>)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e">##################################################################</span>
<span style="color:#75715e"># random 사용시 Seed 설정</span>
tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>set_seed(<span style="color:#ae81ff">0</span>)  <span style="color:#75715e"># for reproducibility</span>

x_data <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">2.</span>, <span style="color:#ae81ff">3.</span>, <span style="color:#ae81ff">4.</span>]
y_data <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">3.</span>, <span style="color:#ae81ff">5.</span>, <span style="color:#ae81ff">7.</span>]

<span style="color:#75715e"># random.normal((배열),mean,stddev)</span>
W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">1</span>,), <span style="color:#f92672">-</span><span style="color:#ae81ff">100.</span>, <span style="color:#ae81ff">100.</span>))

<span style="color:#75715e"># 300까지 for </span>
<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">300</span>):
    <span style="color:#75715e"># 가설을 W*X로 정의</span>
    hypothesis <span style="color:#f92672">=</span> W <span style="color:#f92672">*</span> X
    <span style="color:#75715e"># TF의 Cost 함수 사용</span>
    cost <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>square(hypothesis <span style="color:#f92672">-</span> Y))

    <span style="color:#75715e"># Learning Rate 0.01</span>
    alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>

    <span style="color:#75715e"># 경사 하강법 : 1/m * (W*X - Y) * X</span>
    gradient <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>multiply(tf<span style="color:#f92672">.</span>multiply(W, X) <span style="color:#f92672">-</span> Y, X))

    <span style="color:#75715e"># Weight - Learning Rate * Weight</span>
    descent <span style="color:#f92672">=</span> W <span style="color:#f92672">-</span> tf<span style="color:#f92672">.</span>multiply(alpha, gradient)

    <span style="color:#75715e"># Assing New Weight</span>
    W<span style="color:#f92672">.</span>assign(descent)
    
    <span style="color:#75715e"># 10 step 마다 로그</span>
    <span style="color:#66d9ef">if</span> step <span style="color:#f92672">%</span> <span style="color:#ae81ff">10</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;{:5} | {:15.6f} | {:10.6f}&#39;</span><span style="color:#f92672">.</span>format( step, cost<span style="color:#f92672">.</span>numpy(), W<span style="color:#f92672">.</span>numpy()[<span style="color:#ae81ff">0</span>]) )

<span style="color:#66d9ef">print</span>(<span style="color:#ae81ff">5.0</span> <span style="color:#f92672">*</span> W)
<span style="color:#66d9ef">print</span>(<span style="color:#ae81ff">2.5</span> <span style="color:#f92672">*</span> W)

<span style="color:#75715e">##################################################################</span>
x_data <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">2.</span>, <span style="color:#ae81ff">3.</span>, <span style="color:#ae81ff">4.</span>]
y_data <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">3.</span>, <span style="color:#ae81ff">5.</span>, <span style="color:#ae81ff">7.</span>]

<span style="color:#75715e"># Weight 5부터 접근</span>
W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable([<span style="color:#ae81ff">5.0</span>])

<span style="color:#75715e"># 300까지 for </span>
<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">300</span>):
    <span style="color:#75715e"># 가설을 W*X로 정의</span>
    hypothesis <span style="color:#f92672">=</span> W <span style="color:#f92672">*</span> X
    <span style="color:#75715e"># TF의 Cost 함수 사용</span>
    cost <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>square(hypothesis <span style="color:#f92672">-</span> Y))

    <span style="color:#75715e"># Learning Rate 0.01</span>
    alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>

    <span style="color:#75715e"># 경사 하강법 : 1/m * (W*X - Y) * X</span>
    gradient <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>multiply(tf<span style="color:#f92672">.</span>multiply(W, X) <span style="color:#f92672">-</span> Y, X))

    <span style="color:#75715e"># Weight - Learning Rate * Weight</span>
    descent <span style="color:#f92672">=</span> W <span style="color:#f92672">-</span> tf<span style="color:#f92672">.</span>multiply(alpha, gradient)

    <span style="color:#75715e"># Assing New Weight</span>
    W<span style="color:#f92672">.</span>assign(descent)
    
    <span style="color:#75715e"># 10 step 마다 로그</span>
    <span style="color:#66d9ef">if</span> step <span style="color:#f92672">%</span> <span style="color:#ae81ff">10</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;{:5} | {:10.4f} | {:10.6f}&#39;</span><span style="color:#f92672">.</span>format(step, cost<span style="color:#f92672">.</span>numpy(), W<span style="color:#f92672">.</span>numpy()[<span style="color:#ae81ff">0</span>]))
</code></pre></div></div>
    </div>
  </label>
</div>


<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-05-1</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># LIbrary 선언</span>
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">%</span>matplotlib inline
<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf

tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>set_seed(<span style="color:#ae81ff">777</span>)  <span style="color:#75715e"># for reproducibility</span>
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)

<span style="color:#75715e"># x_train data, y_train data 정의</span>
x_train <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">2.</span>],
           [<span style="color:#ae81ff">2.</span>, <span style="color:#ae81ff">3.</span>],
           [<span style="color:#ae81ff">3.</span>, <span style="color:#ae81ff">1.</span>],
           [<span style="color:#ae81ff">4.</span>, <span style="color:#ae81ff">3.</span>],
           [<span style="color:#ae81ff">5.</span>, <span style="color:#ae81ff">3.</span>],
           [<span style="color:#ae81ff">6.</span>, <span style="color:#ae81ff">2.</span>]]
y_train <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">0.</span>],
           [<span style="color:#ae81ff">0.</span>],
           [<span style="color:#ae81ff">1.</span>],
           [<span style="color:#ae81ff">1.</span>],
           [<span style="color:#ae81ff">1.</span>],
           [<span style="color:#ae81ff">1.</span>]]

<span style="color:#75715e"># x_test data, y_test data</span>
x_test <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">2.</span>,<span style="color:#ae81ff">2.</span>]]
y_test <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">0.</span>]]


<span style="color:#75715e"># x_train data의 x의 1열이 x1</span>
<span style="color:#75715e"># x_train data의 x의 2열이 x2</span>
x1 <span style="color:#f92672">=</span> [x[<span style="color:#ae81ff">0</span>] <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> x_train]
x2 <span style="color:#f92672">=</span> [x[<span style="color:#ae81ff">1</span>] <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> x_train]

<span style="color:#75715e"># y_train data의 y의 0열의 3나누기 나머지</span>
colors <span style="color:#f92672">=</span> [int(y[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">%</span> <span style="color:#ae81ff">3</span>) <span style="color:#66d9ef">for</span> y <span style="color:#f92672">in</span> y_train]

<span style="color:#75715e"># x_train data scatter plot</span>
plt<span style="color:#f92672">.</span>scatter(x1,x2, c<span style="color:#f92672">=</span>colors , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;x&#39;</span>)

<span style="color:#75715e"># x_test data scatter plot</span>
<span style="color:#75715e"># Test 데이터는 붉은색의 위치와 같이 추론시 1의 값을 가지게 됩니다.</span>
plt<span style="color:#f92672">.</span>scatter(x_test[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>],x_test[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;red&#34;</span>)

plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;x1&#34;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;x2&#34;</span>)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># 학습 dataset 정의 </span>
dataset <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices((x_train, y_train))<span style="color:#f92672">.</span>batch(len(x_train))<span style="color:#75715e">#.repeat()</span>

<span style="color:#75715e"># Weight 2*1 행렬의 0값으로 초기화</span>
<span style="color:#75715e"># Bias 1 벡터의 0값으로 초기화</span>
W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>zeros([<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">1</span>]), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight&#39;</span>)
b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>zeros([<span style="color:#ae81ff">1</span>]), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias&#39;</span>)

<span style="color:#75715e"># hypothesis = ( 1 / (1 + exponential ((fetures * W) + b)) ) &lt;- Sigmoid</span>
<span style="color:#75715e"># https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">logistic_regression</span>(features):
    hypothesis  <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>divide(<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">1.</span> <span style="color:#f92672">+</span> tf<span style="color:#f92672">.</span>exp(tf<span style="color:#f92672">.</span>matmul(features, W) <span style="color:#f92672">+</span> b))
    <span style="color:#66d9ef">return</span> hypothesis

<span style="color:#75715e"># 손실 함수 정의   - ( y * log( hypothesis(x) ) + (1- y) log(1-hypothesis) )</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss_fn</span>(hypothesis, features, labels):
    cost <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>tf<span style="color:#f92672">.</span>reduce_mean(labels <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(logistic_regression(features)) <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> labels) <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> hypothesis))
    <span style="color:#66d9ef">return</span> cost

<span style="color:#75715e"># Activation SGD로 설정 </span>
<span style="color:#75715e"># Learning Rate 0.01로 설정</span>
optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>)

<span style="color:#75715e"># Accuracy Function은 hypothesis, labes(Y)를 인자로 받음</span>
<span style="color:#75715e"># predicted = Type Cast (Hypothesis &gt; 0.5 일 경우 Float 32)</span>
<span style="color:#75715e"># accuracy = predicted, labels를 비교하여 boolean을 반납후 int 32로 cast한 값의 평균을 구함</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">accuracy_fn</span>(hypothesis, labels):
    predicted <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(hypothesis <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0.5</span>, dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32)
    accuracy <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>cast(tf<span style="color:#f92672">.</span>equal(predicted, labels), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>int32))
    <span style="color:#66d9ef">return</span> accuracy

<span style="color:#75715e"># Gradient Tape을 통해 경사값을 계산</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad</span>(features, labels):
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
        <span style="color:#75715e"># 손실 값은 loss_fn(logistic_regression(X), X, Y) 으로 정의</span>
        <span style="color:#75715e"># loss_fn은 Sigmoid</span>
        loss_value <span style="color:#f92672">=</span> loss_fn(logistic_regression(features),features,labels)
    <span style="color:#66d9ef">return</span> tape<span style="color:#f92672">.</span>gradient(loss_value, [W,b])

<span style="color:#75715e"># 1000회 반복 수행</span>
EPOCHS <span style="color:#f92672">=</span> <span style="color:#ae81ff">1001</span>

<span style="color:#75715e"># for loop 1000회</span>
<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(EPOCHS):
    <span style="color:#66d9ef">for</span> features, labels  <span style="color:#f92672">in</span> iter(dataset):
        <span style="color:#75715e"># Grad (X, Y)</span>
        grads <span style="color:#f92672">=</span> grad(features, labels)
        <span style="color:#75715e"># SGD Optimizer Vriable 설정</span>
        optimizer<span style="color:#f92672">.</span>apply_gradients(grads_and_vars<span style="color:#f92672">=</span>zip(grads,[W,b]))
        <span style="color:#75715e"># 100 Step 마다 loss log</span>
        <span style="color:#66d9ef">if</span> step <span style="color:#f92672">%</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Iter: {}, Loss: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(step, loss_fn(logistic_regression(features),features,labels)))

<span style="color:#75715e"># Accuracy Function에 Y&#39;, Y를 대입하고 Test Accuracy 산출후 출력</span>
test_acc <span style="color:#f92672">=</span> accuracy_fn(logistic_regression(x_test),y_test)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Testset Accuracy: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(test_acc))  

x_test <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">5.</span>,<span style="color:#ae81ff">2.</span>]]
y_test <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">1.</span>]]
plt<span style="color:#f92672">.</span>scatter(x_test[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>],x_test[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;red&#34;</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;x1&#34;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;x2&#34;</span>)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># Accuracy Function에 Y&#39;, Y를 대입하고 Test Accuracy 산출후 출력</span>
test_acc <span style="color:#f92672">=</span> accuracy_fn(logistic_regression(x_test),y_test)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Testset Accuracy: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(test_acc))
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Y: &#34;</span>,format(y_test),format(logistic_regression(x_test)) )
</code></pre></div></div>
    </div>
  </label>
</div>


<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-05-2</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">%</span>matplotlib inline
<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf

tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>set_seed(<span style="color:#ae81ff">777</span>)  <span style="color:#75715e"># for reproducibility</span>
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)

<span style="color:#75715e"># Google Driver 사용을 위한 Mount</span>
<span style="color:#f92672">from</span> google.colab <span style="color:#f92672">import</span> drive
drive<span style="color:#f92672">.</span>mount(<span style="color:#e6db74">&#39;/content/drive&#39;</span>)

<span style="color:#75715e"># csv data load</span>
xy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>loadtxt(<span style="color:#e6db74">&#39;/content/drive/My Drive/Colab Notebooks/dl4all_2/data-03-diabetes.csv&#39;</span>, delimiter<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;,&#39;</span>, dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32)
<span style="color:#75715e"># x_train 전체행 1열부터 마지막 전열</span>
x_train <span style="color:#f92672">=</span> xy[:, <span style="color:#ae81ff">0</span>:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
<span style="color:#75715e"># y_train 전체행 마지막 열</span>
y_train <span style="color:#f92672">=</span> xy[:, [<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]]

<span style="color:#75715e"># shape 출력</span>
<span style="color:#66d9ef">print</span>(x_train<span style="color:#f92672">.</span>shape, y_train<span style="color:#f92672">.</span>shape)

<span style="color:#75715e"># xy 출력</span>
<span style="color:#66d9ef">print</span>(xy)

dataset <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices((x_train, y_train))<span style="color:#f92672">.</span>batch(len(x_train))

<span style="color:#75715e"># Weight와 Bias를 Random으로 설정</span>
W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">1</span>)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight&#39;</span>)
b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">1</span>,)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias&#39;</span>)

<span style="color:#75715e"># Activation : Hypothesis = 1 / ( 1 + exponential ( X matmul W ) + b )</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">logistic_regression</span>(features):
    hypothesis  <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>divide(<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">1.</span> <span style="color:#f92672">+</span> tf<span style="color:#f92672">.</span>exp(tf<span style="color:#f92672">.</span>matmul(features, W) <span style="color:#f92672">+</span> b))
    <span style="color:#66d9ef">return</span> hypothesis

<span style="color:#75715e"># 손실 함수 정의   - ( y * log( hypothesis(x) ) + (1- y) log(1-hypothesis) )</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss_fn</span>(hypothesis, features, labels):
    cost <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>tf<span style="color:#f92672">.</span>reduce_mean(labels <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(logistic_regression(features)) <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> labels) <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> hypothesis))
    <span style="color:#66d9ef">return</span> cost

<span style="color:#75715e"># Activation SGD </span>
<span style="color:#75715e"># Learning Rate 0.01</span>
optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>)

<span style="color:#75715e"># Sigmoid 함수를 통해 예측값이 0.5보다 크면 1을 반환하고 0.5보다 작으면 0으로 반환</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">accuracy_fn</span>(hypothesis, labels):
    predicted <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(hypothesis <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0.5</span>, dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32)
    accuracy <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>cast(tf<span style="color:#f92672">.</span>equal(predicted, labels), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>int32))
    <span style="color:#66d9ef">return</span> accuracy

<span style="color:#75715e"># GradientTape을 통해 경사값을 계산</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad</span>(hypothesis, features, labels):
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
        loss_value <span style="color:#f92672">=</span> loss_fn(logistic_regression(features),features,labels)
    <span style="color:#66d9ef">return</span> tape<span style="color:#f92672">.</span>gradient(loss_value, [W,b])

<span style="color:#75715e"># 반복횟수 1000회</span>
EPOCHS <span style="color:#f92672">=</span> <span style="color:#ae81ff">1001</span>

<span style="color:#75715e"># Epochs 만큼 for loop</span>
<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(EPOCHS):
    <span style="color:#75715e"># dataset에 fetures와 labels 만큼 for loop</span>
    <span style="color:#66d9ef">for</span> features, labels  <span style="color:#f92672">in</span> iter(dataset):
        <span style="color:#75715e"># gradients 정의</span>
        grads <span style="color:#f92672">=</span> grad(logistic_regression(features), features, labels)

        <span style="color:#75715e"># SGD Optimizer Variable Setting</span>
        optimizer<span style="color:#f92672">.</span>apply_gradients(grads_and_vars<span style="color:#f92672">=</span>zip(grads,[W,b]))

        <span style="color:#75715e"># 100 Step 마다 로그</span>
        <span style="color:#66d9ef">if</span> step <span style="color:#f92672">%</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Iter: {}, Loss: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(step, loss_fn(logistic_regression(features),features,labels)))
</code></pre></div></div>
    </div>
  </label>
</div>


<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-06-1</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)
tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>set_seed(<span style="color:#ae81ff">777</span>)  <span style="color:#75715e"># for reproducibility</span>

<span style="color:#75715e"># x_data, y_data 정의</span>
x_data <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>],
          [<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">2</span>],
          [<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>],
          [<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">6</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">6</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">7</span>]]
y_data <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>],
          [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>],
          [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>],
          [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>],
          [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>],
          [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>]]

<span style="color:#75715e">#convert into numpy and float format</span>
<span style="color:#75715e"># numpy array float32 data로 converting</span>
x_data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>asarray(x_data, dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32)
y_data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>asarray(y_data, dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32)

<span style="color:#75715e"># dataset을 선언합니다.</span>
dataset <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices((x_data, y_data))
dataset <span style="color:#f92672">=</span> dataset<span style="color:#f92672">.</span>repeat()<span style="color:#f92672">.</span>batch(<span style="color:#ae81ff">2</span>)
nb_classes <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span> <span style="color:#75715e">#class의 개수입니다.</span>

<span style="color:#66d9ef">print</span>(x_data<span style="color:#f92672">.</span>shape)
<span style="color:#66d9ef">print</span>(y_data<span style="color:#f92672">.</span>shape)

<span style="color:#75715e">#Weight and bias setting</span>
<span style="color:#75715e"># Weight random.normal (4,3)</span>
<span style="color:#75715e"># Bias random.normal (3)</span>
W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">4</span>, nb_classes)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight&#39;</span>)
b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((nb_classes,)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias&#39;</span>)
variables <span style="color:#f92672">=</span> [W, b]

<span style="color:#66d9ef">print</span>(W,b)

<span style="color:#75715e"># tf.nn.softmax computes softmax activations</span>
<span style="color:#75715e"># softmax = exp(logits) / reduce_sum(exp(logits), dim)</span>
<span style="color:#75715e"># softmax softmax ( X * W + B )</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">hypothesis</span>(X):
    <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>softmax(tf<span style="color:#f92672">.</span>matmul(X, W) <span style="color:#f92672">+</span> b)

<span style="color:#75715e"># h(x) print</span>
<span style="color:#66d9ef">print</span>(hypothesis(x_data))

<span style="color:#75715e"># Softmax onehot test</span>
<span style="color:#75715e"># Softmax 테스트</span>
sample_db <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">8</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">4</span>]]
sample_db <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>asarray(sample_db, dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32)

<span style="color:#66d9ef">print</span>(hypothesis(sample_db))

<span style="color:#75715e"># Cost Function</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">cost_fn</span>(X, Y):
    <span style="color:#75715e"># H(X) = Logit</span>
    logits <span style="color:#f92672">=</span> hypothesis(X)

    <span style="color:#75715e"># Cost =  SUM ( Y * log(H(X)) ) </span>
    cost <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>tf<span style="color:#f92672">.</span>reduce_sum(Y <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(logits), axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)

    <span style="color:#75715e"># cost_mean =  mean( cost )</span>
    cost_mean <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(cost)
    
    <span style="color:#66d9ef">return</span> cost_mean

<span style="color:#66d9ef">print</span>(cost_fn(x_data, y_data))

<span style="color:#75715e"># x = 3</span>
x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>constant(<span style="color:#ae81ff">3.0</span>)

<span style="color:#75715e"># GradientTape 정의</span>
<span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> g:
    <span style="color:#75715e"># g.watch : Ensures that tensor is being traced by this tape.</span>
    g<span style="color:#f92672">.</span>watch(x)
    y <span style="color:#f92672">=</span> x <span style="color:#f92672">*</span> x <span style="color:#75715e"># x^2</span>

<span style="color:#75715e"># delta y , delta x</span>
dy_dx <span style="color:#f92672">=</span> g<span style="color:#f92672">.</span>gradient(y, x) <span style="color:#75715e"># Will compute to 6.0</span>
<span style="color:#66d9ef">print</span>(dy_dx)

<span style="color:#75715e"># Gradient Funtion</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad_fn</span>(X, Y):
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
        <span style="color:#75715e"># loss = cost_mean</span>
        loss <span style="color:#f92672">=</span> cost_fn(X, Y)

        <span style="color:#75715e"># gradient ( loss, )</span>
        grads <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(loss, variables)

        <span style="color:#66d9ef">return</span> grads

<span style="color:#66d9ef">print</span>(grad_fn(x_data, y_data))

<span style="color:#75715e"># fitting : X,Y, 2000 epochs, verbose 100</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fit</span>(X, Y, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">2000</span>, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>):
    <span style="color:#75715e"># learning rate setting</span>
    optimizer <span style="color:#f92672">=</span>  tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>)

    <span style="color:#75715e"># for loop </span>
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(epochs):
        <span style="color:#75715e"># gradient function X, Y</span>
        grads <span style="color:#f92672">=</span> grad_fn(X, Y)

        <span style="color:#75715e"># Optimizer Variable Seting</span>
        optimizer<span style="color:#f92672">.</span>apply_gradients(zip(grads, variables))

        <span style="color:#75715e"># verbose 100 step log</span>
        <span style="color:#66d9ef">if</span> (i<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>) <span style="color:#f92672">|</span> ((i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">%</span>verbose<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>):
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Loss at epoch </span><span style="color:#e6db74">%d</span><span style="color:#e6db74">: </span><span style="color:#e6db74">%f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span>(i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>, cost_fn(X, Y)<span style="color:#f92672">.</span>numpy()))

<span style="color:#75715e"># start fitting loop</span>
fit(x_data, y_data)

<span style="color:#75715e"># Sample 2, 1, 3, 2 -&gt; 0,0,1</span>
sample_data <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">2</span>]] <span style="color:#75715e"># answer_label [[0,0,1]]</span>
sample_data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>asarray(sample_data, dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32)

a <span style="color:#f92672">=</span> hypothesis(sample_data)

<span style="color:#66d9ef">print</span>(a)
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>argmax(a, <span style="color:#ae81ff">1</span>)) <span style="color:#75715e">#index: 2</span>

sample_data <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">6</span>,<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">4</span>]] 
sample_data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>asarray(sample_data, dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32)

a <span style="color:#f92672">=</span> hypothesis(sample_data)

<span style="color:#66d9ef">print</span>(a)
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>argmax(a, <span style="color:#ae81ff">1</span>)) <span style="color:#75715e">#index: 2</span>

b <span style="color:#f92672">=</span> hypothesis(x_data)
<span style="color:#66d9ef">print</span>(b)
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>argmax(b, <span style="color:#ae81ff">1</span>))
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>argmax(y_data, <span style="color:#ae81ff">1</span>)) <span style="color:#75715e"># matches with y_data</span>

<span style="color:#75715e"># softmax classifer</span>
<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">softmax_classifer</span>(tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Model):
    <span style="color:#75715e"># class initilization</span>
    <span style="color:#66d9ef">def</span> __init__(self, nb_classes):
        super(softmax_classifer, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">4</span>, nb_classes)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight&#39;</span>)
        self<span style="color:#f92672">.</span>b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((nb_classes,)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias&#39;</span>)
        
    <span style="color:#75715e"># softmax regression</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">softmax_regression</span>(self, X):
        <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>softmax(tf<span style="color:#f92672">.</span>matmul(X, self<span style="color:#f92672">.</span>W) <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>b)
    
    <span style="color:#75715e"># cost_function</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">cost_fn</span>(self, X, Y):
        logits <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>softmax_regression(X)
        cost <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(<span style="color:#f92672">-</span>tf<span style="color:#f92672">.</span>reduce_sum(Y <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(logits), axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>))        
        <span style="color:#66d9ef">return</span> cost
    
    <span style="color:#75715e"># gradient function</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad_fn</span>(self, X, Y):
        <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
            cost <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>cost_fn(x_data, y_data)
            grads <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(cost, self<span style="color:#f92672">.</span>variables)            
            <span style="color:#66d9ef">return</span> grads
    
    <span style="color:#75715e"># fitting</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fit</span>(self, X, Y, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">2000</span>, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">500</span>):
        optimizer <span style="color:#f92672">=</span>  tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>)

        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(epochs):
            grads <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>grad_fn(X, Y)
            optimizer<span style="color:#f92672">.</span>apply_gradients(zip(grads, self<span style="color:#f92672">.</span>variables))
            <span style="color:#66d9ef">if</span> (i<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>) <span style="color:#f92672">|</span> ((i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">%</span>verbose<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>):
                <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Loss at epoch </span><span style="color:#e6db74">%d</span><span style="color:#e6db74">: </span><span style="color:#e6db74">%f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span>(i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>cost_fn(X, Y)<span style="color:#f92672">.</span>numpy()))
            
model <span style="color:#f92672">=</span> softmax_classifer(nb_classes)
model<span style="color:#f92672">.</span>fit(x_data, y_data)
</code></pre></div></div>
    </div>
  </label>
</div>


<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-06-2</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)
tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>set_seed(<span style="color:#ae81ff">777</span>)  <span style="color:#75715e"># for reproducibility</span>

<span style="color:#75715e"># Google Driver 사용을 위한 Mount</span>
<span style="color:#f92672">from</span> google.colab <span style="color:#f92672">import</span> drive
drive<span style="color:#f92672">.</span>mount(<span style="color:#e6db74">&#39;/content/drive&#39;</span>)

<span style="color:#75715e"># csv data load</span>
xy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>loadtxt(<span style="color:#e6db74">&#39;/content/drive/My Drive/Colab Notebooks/dl4all_2/data-04-zoo.csv&#39;</span>, delimiter<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;,&#39;</span>, dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32)
<span style="color:#75715e"># x_data 전체행 1열부터 마지막 전열</span>
x_data <span style="color:#f92672">=</span> xy[:, <span style="color:#ae81ff">0</span>:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
<span style="color:#75715e"># y_data 전체행 마지막열</span>
y_data <span style="color:#f92672">=</span> xy[:, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]

<span style="color:#75715e"># nb_classes 7 </span>
nb_classes <span style="color:#f92672">=</span> <span style="color:#ae81ff">7</span>  <span style="color:#75715e"># 0 ~ 6</span>

<span style="color:#75715e"># Make Y data as onehot shape</span>
<span style="color:#75715e"># Y Data One-Hot Encoding</span>
Y_one_hot <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>one_hot(y_data<span style="color:#f92672">.</span>astype(np<span style="color:#f92672">.</span>int32), nb_classes)

<span style="color:#66d9ef">print</span>(x_data<span style="color:#f92672">.</span>shape, Y_one_hot<span style="color:#f92672">.</span>shape, y_data<span style="color:#f92672">.</span>shape, Y_one_hot, y_data)

<span style="color:#75715e"># Weight and bias setting</span>
<span style="color:#75715e"># Weight = 16, 7</span>
<span style="color:#75715e"># Bias = 16</span>
W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">16</span>, nb_classes)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight&#39;</span>)
b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((nb_classes,)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias&#39;</span>)
variables <span style="color:#f92672">=</span> [W, b]

<span style="color:#75715e"># tf.nn.softmax computes softmax activations</span>
<span style="color:#75715e"># softmax = exp(logits) / reduce_sum(exp(logits), dim)</span>
<span style="color:#75715e"># Logit Function = X * W + B</span>
<span style="color:#75715e"># cross_entropy_with_logist에서 logit_fn을 받음</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">logit_fn</span>(X):
    <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>matmul(X, W) <span style="color:#f92672">+</span> b

<span style="color:#75715e"># Hypothesis = Softmax ( Logit Function )</span>
<span style="color:#75715e"># prediction에서 hypothesis 받음</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">hypothesis</span>(X):
    <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>softmax(logit_fn(X))

<span style="color:#75715e"># Cost Function </span>
<span style="color:#75715e"># Logits = Logit Function</span>
<span style="color:#75715e"># Cost_i = Cross Entropy ( Y , Logit )</span>
<span style="color:#75715e"># Cost Reduce Mean ( Cost_i )</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">cost_fn</span>(X, Y):
    logits <span style="color:#f92672">=</span> logit_fn(X)
    cost_i <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>losses<span style="color:#f92672">.</span>categorical_crossentropy(y_true<span style="color:#f92672">=</span>Y, y_pred<span style="color:#f92672">=</span>logits, from_logits<span style="color:#f92672">=</span>True)    
    cost <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(cost_i)    
    <span style="color:#66d9ef">return</span> cost

<span style="color:#75715e"># Gradient Tape</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad_fn</span>(X, Y):
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
        <span style="color:#75715e"># Cost Function ( X, Y )</span>
        loss <span style="color:#f92672">=</span> cost_fn(X, Y)
        grads <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(loss, variables)
        <span style="color:#66d9ef">return</span> grads

<span style="color:#75715e"># Prediction : ArgMax (Hypothesis(X), 1)</span>
<span style="color:#75715e"># Correct Prediction</span>
<span style="color:#75715e"># Accuracy = Average ( Prediction Float 32 Type Casting )</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">prediction</span>(X, Y):
    <span style="color:#75715e"># Argument 중 Max값을 Prediction Value로 선택</span>
    pred <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>argmax(hypothesis(X), <span style="color:#ae81ff">1</span>)
    <span style="color:#75715e"># Prediction과 Argument Max의 동일 여부를 선택</span>
    correct_prediction <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>equal(pred, tf<span style="color:#f92672">.</span>argmax(Y, <span style="color:#ae81ff">1</span>))
    accuracy <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>cast(correct_prediction, tf<span style="color:#f92672">.</span>float32))

    <span style="color:#66d9ef">return</span> accuracy

<span style="color:#75715e"># fitting function</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fit</span>(X, Y, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>):
    <span style="color:#75715e"># SGD Optimizer : Learning Rate 0.1</span>
    optimizer <span style="color:#f92672">=</span>  tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>)
e
    <span style="color:#75715e"># Epochs for loop</span>
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(epochs):
        grads <span style="color:#f92672">=</span> grad_fn(X, Y)
        optimizer<span style="color:#f92672">.</span>apply_gradients(zip(grads, variables))

        <span style="color:#66d9ef">if</span> (i<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>) <span style="color:#f92672">|</span> ((i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">%</span>verbose<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>):
<span style="color:#75715e">#             print(&#39;Loss at epoch %d: %f&#39; %(i+1, cost_fn(X, Y).numpy()))</span>
            acc <span style="color:#f92672">=</span> prediction(X, Y)<span style="color:#f92672">.</span>numpy()
            loss <span style="color:#f92672">=</span> cost_fn(X, Y)<span style="color:#f92672">.</span>numpy() 
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Steps: {} Loss: {}, Acc: {}&#39;</span><span style="color:#f92672">.</span>format(i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>, loss, acc))

fit(x_data, Y_one_hot)
</code></pre></div></div>
    </div>
  </label>
</div>


<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-07-1</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># import</span>
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">%</span>matplotlib inline
<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf
<span style="color:#f92672">from</span> mpl_toolkits.mplot3d <span style="color:#f92672">import</span> Axes3D

tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>set_seed(<span style="color:#ae81ff">777</span>)  <span style="color:#75715e"># for reproducibility</span>

<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)

<span style="color:#75715e"># x_train data</span>
x_train <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">2</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">5</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">5</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">6</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">7</span>]]

<span style="color:#75715e"># y_train data</span>
y_train <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>],
          [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>],
          [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>],
          [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>],
          [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>],
          [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>]]

<span style="color:#75715e"># Evaluation our model using this test dataset</span>
<span style="color:#75715e"># x_test data</span>
x_test <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>],
          [<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>],
          [<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>]]
<span style="color:#75715e"># y_test data</span>
y_test <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>],
          [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>],
          [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>]]

<span style="color:#75715e"># x_train data</span>
x1 <span style="color:#f92672">=</span> [x[<span style="color:#ae81ff">0</span>] <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> x_train]
x2 <span style="color:#f92672">=</span> [x[<span style="color:#ae81ff">1</span>] <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> x_train]
x3 <span style="color:#f92672">=</span> [x[<span style="color:#ae81ff">2</span>] <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> x_train]

<span style="color:#75715e"># plot figure 3d</span>
fig <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>figure()
ax <span style="color:#f92672">=</span> fig<span style="color:#f92672">.</span>add_subplot(<span style="color:#ae81ff">111</span>, projection<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;3d&#39;</span>)
ax<span style="color:#f92672">.</span>scatter(x1, x2, x3, c<span style="color:#f92672">=</span>y_train, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)

<span style="color:#75715e"># test data </span>
ax<span style="color:#f92672">.</span>scatter(x_test[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>], x_test[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">1</span>], x_test[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">2</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;red&#34;</span>, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;x&#39;</span>)
ax<span style="color:#f92672">.</span>scatter(x_test[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">0</span>], x_test[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">1</span>], x_test[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">2</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;red&#34;</span>, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;x&#39;</span>)
ax<span style="color:#f92672">.</span>scatter(x_test[<span style="color:#ae81ff">2</span>][<span style="color:#ae81ff">0</span>], x_test[<span style="color:#ae81ff">2</span>][<span style="color:#ae81ff">1</span>], x_test[<span style="color:#ae81ff">2</span>][<span style="color:#ae81ff">2</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;red&#34;</span>, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;x&#39;</span>)

ax<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">&#39;X Label&#39;</span>)
ax<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">&#39;Y Label&#39;</span>)
ax<span style="color:#f92672">.</span>set_zlabel(<span style="color:#e6db74">&#39;Z Label&#39;</span>)

plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># Tensorflow data API를 통해 학습시킬 값을 담는다 (Batch Size는 한번에 학습시킬 Size로 정함)</span>
<span style="color:#75715e"># dataset 정의</span>
dataset <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices((x_train, y_train))<span style="color:#f92672">.</span>batch(len(x_train))<span style="color:#75715e">#.repeat()</span>

<span style="color:#75715e"># Weight Bias Random 정의</span>
W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>)))
b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">3</span>,)))

<span style="color:#75715e"># Softmax를 가설로 선언 : Softmax를 통해 가장 높은 값을 구함</span>
<span style="color:#75715e"># softmax_fn 선언 (Hypothesis = Softmax( F(x)를 Return))</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">softmax_fn</span>(features):
    <span style="color:#75715e"># hypothesis = softmax ( fetures (x) * Weight + Bias )</span>
    hypothesis <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>softmax(tf<span style="color:#f92672">.</span>matmul(features, W) <span style="color:#f92672">+</span> b)
    <span style="color:#66d9ef">return</span> hypothesis

<span style="color:#75715e"># 가설을 검증할 Cost 함수를 정의</span>
<span style="color:#75715e"># Average ( - SUM ( Labels (Y) * Log (Softmax(H(X))) ) )</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss_fn</span>(hypothesis, features, labels):
    cost <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(<span style="color:#f92672">-</span>tf<span style="color:#f92672">.</span>reduce_sum(labels <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(hypothesis), axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>))
    <span style="color:#66d9ef">return</span> cost

<span style="color:#75715e"># Boolean is_decay 설정</span>
is_decay <span style="color:#f92672">=</span> True

<span style="color:#75715e"># Start Learning Rate 0.1 설정</span>
starter_learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>

<span style="color:#75715e"># Decaying the learning rate</span>
<span style="color:#75715e"># tf.train.exponential_decay : This function applies an exponential decay function to a provided initial learning rate</span>
<span style="color:#75715e"># tf.train.inverse_time_decay : It is often recommended to lower the learning rate as the training progresses</span>
<span style="color:#75715e"># tf.train.natural_exp_decay : This function applies an exponential decay function to a provided initial learning rate</span>
<span style="color:#75715e"># tf.train.piecewise_constant : Piecewise constant from boundaries and interval values.</span>
<span style="color:#75715e"># tf.train.polynomial_decay : It is commonly observed that a monotonically decreasing learning rate, whose degree of change is carefully chosen, results in a better performing model.</span>
<span style="color:#75715e"># tf.train.cosine_decay : When training a model, it is often recommended to lower the learning rate as the training progresses.</span>
<span style="color:#75715e"># tf.train.linear_cosine_decay : Note that linear cosine decay is more aggressive than cosine decay and larger initial learning rates can typically be used.</span>
<span style="color:#75715e"># tf.train.noisy_linear_cosine_decay : Note that linear cosine decay is more aggressive than cosine decay and larger initial learning rates can typically be used.</span>

<span style="color:#75715e"># is_decay true 일경우</span>
<span style="color:#66d9ef">if</span>(is_decay):    
    <span style="color:#75715e"># Learning Rate = Exponential Deacy ( 시작 LR, Step, Rate, Staricase 여부 )</span>
    <span style="color:#75715e"># exp_rate = starter_rate * exp ( decay_rate * t )</span>
    <span style="color:#75715e"># initial_learning_rate : The inital Learning Rate</span>
    <span style="color:#75715e"># global_step : 현재 학습 횟수, Global step to use for the decay computation. Must not be negative.</span>
    <span style="color:#75715e"># decay_steps : 총학습 횟수, Must be positive</span>
    <span style="color:#75715e"># decay_rate : 감소 비율, * decay_rate</span>
    <span style="color:#75715e"># staricase : 이산적 학습 속도 감속 유무 ( global_step / decay_steps ) 의 제곱 적용 여부</span>
    <span style="color:#75715e"># decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)</span>
    learning_rate <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>schedules<span style="color:#f92672">.</span>ExponentialDecay(initial_learning_rate<span style="color:#f92672">=</span>starter_learning_rate,
                                                                 decay_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>,
                                                                 decay_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.96</span>,
                                                                 staircase<span style="color:#f92672">=</span>True)
    <span style="color:#75715e"># SGD Optimizer</span>
    optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate)
<span style="color:#66d9ef">else</span>:
    <span style="color:#75715e"># is_decay가 false면 초기 Learning Rate 적용</span>
    optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate<span style="color:#f92672">=</span>starter_learning_rate)

<span style="color:#75715e"># Gradient Tape</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad</span>(hypothesis, features, labels):
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
        <span style="color:#75715e"># Loss Value = Loss Function ( softmax (X) , x , y )</span>
        loss_value <span style="color:#f92672">=</span> loss_fn(softmax_fn(features),features,labels)
    <span style="color:#75715e"># Gradient Tape ( cost , [w, b] )</span>
    <span style="color:#66d9ef">return</span> tape<span style="color:#f92672">.</span>gradient(loss_value, [W,b])

<span style="color:#75715e"># Accuracy Function</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">accuracy_fn</span>(hypothesis, labels):
    <span style="color:#75715e"># Prediction = Hypothesis중 Max</span>
    prediction <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>argmax(hypothesis, <span style="color:#ae81ff">1</span>)
    <span style="color:#75715e"># Equal 판정</span>
    is_correct <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>equal(prediction, tf<span style="color:#f92672">.</span>argmax(labels, <span style="color:#ae81ff">1</span>))
    <span style="color:#75715e"># 정확도 측정</span>
    accuracy <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>cast(is_correct, tf<span style="color:#f92672">.</span>float32))
    <span style="color:#75715e"># Accuracy Return</span>
    <span style="color:#66d9ef">return</span> accuracy

<span style="color:#75715e"># epochs 1000</span>
EPOCHS <span style="color:#f92672">=</span> <span style="color:#ae81ff">1001</span>

<span style="color:#75715e"># epoch 만큼 for loop</span>
<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(EPOCHS):
    <span style="color:#75715e"># dataset 의 x, y 만큼 for loop</span>
    <span style="color:#66d9ef">for</span> features, labels  <span style="color:#f92672">in</span> iter(dataset):
        <span style="color:#75715e"># features (x)  type cast</span>
        features <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(features, tf<span style="color:#f92672">.</span>float32)
        <span style="color:#75715e"># labes (y) type cast</span>
        labels <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(labels, tf<span style="color:#f92672">.</span>float32)
        <span style="color:#75715e"># Gradient Tape</span>
        grads <span style="color:#f92672">=</span> grad(softmax_fn(features), features, labels)
        <span style="color:#75715e"># Optimizer</span>
        optimizer<span style="color:#f92672">.</span>apply_gradients(grads_and_vars<span style="color:#f92672">=</span>zip(grads,[W,b]))
        <span style="color:#75715e"># verbose 100 </span>
        <span style="color:#66d9ef">if</span> step <span style="color:#f92672">%</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Iter: {}, Loss: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(step, loss_fn(softmax_fn(features),features,labels)))

<span style="color:#75715e"># Test Set 수행</span>
x_test <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(x_test, tf<span style="color:#f92672">.</span>float32)
y_test <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(y_test, tf<span style="color:#f92672">.</span>float32)
test_acc <span style="color:#f92672">=</span> accuracy_fn(softmax_fn(x_test),y_test)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Testset Accuracy: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(test_acc))
</code></pre></div></div>
    </div>
  </label>
</div>


<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-07-2</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
<span style="color:#75715e"># import</span>
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">%</span>matplotlib inline
<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf

tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>set_seed(<span style="color:#ae81ff">777</span>)  <span style="color:#75715e"># for reproducibility</span>

<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)

<span style="color:#75715e"># xy 정의</span>
xy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[<span style="color:#ae81ff">828.659973</span>, <span style="color:#ae81ff">833.450012</span>, <span style="color:#ae81ff">908100</span>, <span style="color:#ae81ff">828.349976</span>, <span style="color:#ae81ff">831.659973</span>],
               [<span style="color:#ae81ff">823.02002</span>, <span style="color:#ae81ff">828.070007</span>, <span style="color:#ae81ff">1828100</span>, <span style="color:#ae81ff">821.655029</span>, <span style="color:#ae81ff">828.070007</span>],
               [<span style="color:#ae81ff">819.929993</span>, <span style="color:#ae81ff">824.400024</span>, <span style="color:#ae81ff">1438100</span>, <span style="color:#ae81ff">818.97998</span>, <span style="color:#ae81ff">824.159973</span>],
               [<span style="color:#ae81ff">816</span>, <span style="color:#ae81ff">820.958984</span>, <span style="color:#ae81ff">1008100</span>, <span style="color:#ae81ff">815.48999</span>, <span style="color:#ae81ff">819.23999</span>],
               [<span style="color:#ae81ff">819.359985</span>, <span style="color:#ae81ff">823</span>, <span style="color:#ae81ff">1188100</span>, <span style="color:#ae81ff">818.469971</span>, <span style="color:#ae81ff">818.97998</span>],
               [<span style="color:#ae81ff">819</span>, <span style="color:#ae81ff">823</span>, <span style="color:#ae81ff">1198100</span>, <span style="color:#ae81ff">816</span>, <span style="color:#ae81ff">820.450012</span>],
               [<span style="color:#ae81ff">811.700012</span>, <span style="color:#ae81ff">815.25</span>, <span style="color:#ae81ff">1098100</span>, <span style="color:#ae81ff">809.780029</span>, <span style="color:#ae81ff">813.669983</span>],
               [<span style="color:#ae81ff">809.51001</span>, <span style="color:#ae81ff">816.659973</span>, <span style="color:#ae81ff">1398100</span>, <span style="color:#ae81ff">804.539978</span>, <span style="color:#ae81ff">809.559998</span>]])

<span style="color:#75715e"># x_train 전행, 1열부터 마지막 전열</span>
<span style="color:#75715e"># y_train 전행, 마지막열</span>
x_train <span style="color:#f92672">=</span> xy[:, <span style="color:#ae81ff">0</span>:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
y_train <span style="color:#f92672">=</span> xy[:, [<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]]

<span style="color:#75715e"># 이상치에 의해 값이 왜곡됨</span>
plt<span style="color:#f92672">.</span>plot(x_train, <span style="color:#e6db74">&#39;ro&#39;</span>)
plt<span style="color:#f92672">.</span>plot(y_train)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># Tensorflow data API를 통해 학습시킬 값을 담음 (Batch Size는 한번에 학습시킬 Size로 정함)</span>
<span style="color:#75715e"># X(features),Y(labels)는 실재 학습에 쓰일 Data (연산을 위해 Type를 맞춰줌)</span>
dataset <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices((x_train, y_train))<span style="color:#f92672">.</span>batch(len(x_train))

<span style="color:#75715e"># Weight, Bias Random 초기화</span>
W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">1</span>)), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32)
b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">1</span>,)), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32)

<span style="color:#75715e"># Linear Regression Hypothesis 정의 : X * Weight + Bias</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">linearReg_fn</span>(features):
    hypothesis <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>matmul(features, W) <span style="color:#f92672">+</span> b
    <span style="color:#66d9ef">return</span> hypothesis

<span style="color:#75715e"># Loss Function 정의 : Average ( ( F(x) - Y ) ^ 2 )</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss_fn</span>(hypothesis, features, labels):
    cost <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>square(hypothesis <span style="color:#f92672">-</span> labels))
    <span style="color:#66d9ef">return</span> cost

<span style="color:#75715e"># SGD Optimizer 선택 : Learning Rate 0.00005</span>
optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-5</span>)

<span style="color:#75715e"># Gradient Tape </span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad</span>(hypothesis, features, labels):
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
        <span style="color:#75715e"># Loss Value = Loss Function ( LinearRegression ( x ) , x, y )</span>
        loss_value <span style="color:#f92672">=</span> loss_fn(linearReg_fn(features),features,labels)
    <span style="color:#66d9ef">return</span> tape<span style="color:#f92672">.</span>gradient(loss_value, [W,b]), loss_value

<span style="color:#75715e"># Epochs 100회</span>
EPOCHS <span style="color:#f92672">=</span> <span style="color:#ae81ff">101</span>

<span style="color:#75715e"># For loop </span>
<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(EPOCHS):
    <span style="color:#75715e"># Dataset X, Y</span>
    <span style="color:#66d9ef">for</span> features, labels  <span style="color:#f92672">in</span> dataset:
        <span style="color:#75715e"># X 의 Type Cast</span>
        features <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(features, tf<span style="color:#f92672">.</span>float32)

        <span style="color:#75715e"># Y 의 Type Case</span>
        labels <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(labels, tf<span style="color:#f92672">.</span>float32)

        <span style="color:#75715e"># Hypothesis Value = Linear Regression Function ( X )</span>
        hypo_value <span style="color:#f92672">=</span> linearReg_fn(features)

        <span style="color:#75715e"># GradientDecenst Value</span>
        grads, loss_value <span style="color:#f92672">=</span> grad(linearReg_fn(features), features, labels)        
        optimizer<span style="color:#f92672">.</span>apply_gradients(grads_and_vars<span style="color:#f92672">=</span>zip(grads,[W,b]))    
    <span style="color:#75715e"># Log without Verbose</span>
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Iter: {}, Loss: {:.4f}, Prediction: {}&#34;</span><span style="color:#f92672">.</span>format(step, loss_value, hypo_value))

<span style="color:#75715e"># xy 정의</span>
xy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[<span style="color:#ae81ff">828.659973</span>, <span style="color:#ae81ff">833.450012</span>,  <span style="color:#ae81ff">908100</span>, <span style="color:#ae81ff">828.349976</span>, <span style="color:#ae81ff">831.659973</span>],
               [<span style="color:#ae81ff">823.02002</span>,  <span style="color:#ae81ff">828.070007</span>, <span style="color:#ae81ff">1828100</span>, <span style="color:#ae81ff">821.655029</span>, <span style="color:#ae81ff">828.070007</span>],
               [<span style="color:#ae81ff">819.929993</span>, <span style="color:#ae81ff">824.400024</span>, <span style="color:#ae81ff">1438100</span>, <span style="color:#ae81ff">818.97998</span>,  <span style="color:#ae81ff">824.159973</span>],
               [<span style="color:#ae81ff">816</span>,        <span style="color:#ae81ff">820.958984</span>, <span style="color:#ae81ff">1008100</span>, <span style="color:#ae81ff">815.48999</span>,  <span style="color:#ae81ff">819.23999</span>],
               [<span style="color:#ae81ff">819.359985</span>, <span style="color:#ae81ff">823</span>,        <span style="color:#ae81ff">1188100</span>, <span style="color:#ae81ff">818.469971</span>, <span style="color:#ae81ff">818.97998</span>],
               [<span style="color:#ae81ff">819</span>,        <span style="color:#ae81ff">823</span>,        <span style="color:#ae81ff">1198100</span>, <span style="color:#ae81ff">816</span>,        <span style="color:#ae81ff">820.450012</span>],
               [<span style="color:#ae81ff">811.700012</span>, <span style="color:#ae81ff">815.25</span>,     <span style="color:#ae81ff">1098100</span>, <span style="color:#ae81ff">809.780029</span>, <span style="color:#ae81ff">813.669983</span>],
               [<span style="color:#ae81ff">809.51001</span>,  <span style="color:#ae81ff">816.659973</span>, <span style="color:#ae81ff">1398100</span>, <span style="color:#ae81ff">804.539978</span>, <span style="color:#ae81ff">809.559998</span>]])

<span style="color:#75715e"># Normalization 정의 </span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">normalization</span>(data):
    <span style="color:#75715e"># data 를 인자로 받음</span>
    <span style="color:#75715e"># numerator = 개별 data - 열의 min값 </span>
    numerator <span style="color:#f92672">=</span> data <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>min(data, <span style="color:#ae81ff">0</span>)
    <span style="color:#75715e"># denominator = 열의 Max값 - 열의 Min값</span>
    denominator <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>max(data, <span style="color:#ae81ff">0</span>) <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>min(data, <span style="color:#ae81ff">0</span>)
    <span style="color:#75715e"># print(&#34;data : &#34;, data, &#34;np_min : &#34;, np.min(data,0), &#34;np_max : &#34;, np.max(data,0),&#34;num : &#34;, numerator, &#34;den : &#34;, denominator)</span>
    <span style="color:#75715e"># 열의 값을 Min 0 ~ Max 1의 비율로 Normalization</span>
    <span style="color:#66d9ef">return</span> numerator <span style="color:#f92672">/</span> denominator

<span style="color:#66d9ef">print</span>(xy)

xy <span style="color:#f92672">=</span> normalization(xy)

<span style="color:#66d9ef">print</span>(xy)

x_train <span style="color:#f92672">=</span> xy[:, <span style="color:#ae81ff">0</span>:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
y_train <span style="color:#f92672">=</span> xy[:, [<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]]

plt<span style="color:#f92672">.</span>plot(x_train, <span style="color:#e6db74">&#39;ro&#39;</span>)
plt<span style="color:#f92672">.</span>plot(y_train)

plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># dataset, Weight, Bias 정의</span>
dataset <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices((x_train, y_train))<span style="color:#f92672">.</span>batch(len(x_train))
W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">1</span>)), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32)
b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">1</span>,)), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32)

<span style="color:#75715e"># L2 Norm Function 정의</span>
<span style="color:#75715e"># https://junklee.tistory.com/29</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">l2_loss</span>(loss, beta <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>):
    <span style="color:#75715e"># Weight Regularization * Beta (Weight 이상치의 영향을 최소화하는 정규화)</span>
    W_reg <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>l2_loss(W) <span style="color:#75715e"># output = sum(t ** 2) / 2</span>
    loss <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(loss <span style="color:#f92672">+</span> W_reg <span style="color:#f92672">*</span> beta)
    <span style="color:#66d9ef">return</span> loss

<span style="color:#75715e"># Flag가 True일때만 L2 Norm 으로 Cost 정의</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss_fn2</span>(hypothesis, features, labels, flag <span style="color:#f92672">=</span> False):
    cost <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>square(hypothesis <span style="color:#f92672">-</span> labels))
    <span style="color:#66d9ef">if</span>(flag):
        cost <span style="color:#f92672">=</span> l2_loss(cost)
    <span style="color:#66d9ef">return</span> cost    

<span style="color:#75715e"># Learning Rate Decay True</span>
is_decay <span style="color:#f92672">=</span> True

<span style="color:#75715e"># 0.1</span>
starter_learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>

<span style="color:#75715e"># 50 Steap, 0.96, 이산 학습속도 감속 적용</span>
<span style="color:#75715e"># decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)</span>
<span style="color:#66d9ef">if</span>(is_decay):    
    learning_rate <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>schedules<span style="color:#f92672">.</span>ExponentialDecay(initial_learning_rate<span style="color:#f92672">=</span>starter_learning_rate,
                                                                  decay_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>,
                                                                  decay_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.96</span>,
                                                                  staircase<span style="color:#f92672">=</span>True)
    optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate)
<span style="color:#66d9ef">else</span>:
    optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate<span style="color:#f92672">=</span>starter_learning_rate)

<span style="color:#75715e"># Gradient Tape 정의</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad</span>(hypothesis, features, labels, l2_flag):
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
        loss_value <span style="color:#f92672">=</span> loss_fn2(linearReg_fn(features),features,labels, l2_flag)
    <span style="color:#66d9ef">return</span> tape<span style="color:#f92672">.</span>gradient(loss_value, [W,b]), loss_value

<span style="color:#75715e"># 100회</span>
EPOCHS <span style="color:#f92672">=</span> <span style="color:#ae81ff">101</span>

<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(EPOCHS):
    <span style="color:#66d9ef">for</span> features, labels  <span style="color:#f92672">in</span> dataset:
        <span style="color:#75715e"># X, Y Type Case</span>
        features <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(features, tf<span style="color:#f92672">.</span>float32)
        labels <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(labels, tf<span style="color:#f92672">.</span>float32)
        <span style="color:#75715e"># Gradient Tape</span>
        grads, loss_value <span style="color:#f92672">=</span> grad(linearReg_fn(features), features, labels, False)
        optimizer<span style="color:#f92672">.</span>apply_gradients(grads_and_vars<span style="color:#f92672">=</span>zip(grads,[W,b]))        
    <span style="color:#75715e"># Verbose 10</span>
    <span style="color:#66d9ef">if</span> step <span style="color:#f92672">%</span> <span style="color:#ae81ff">10</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Iter: {}, Loss: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(step, loss_value))

<span style="color:#75715e"># 100회</span>
EPOCHS <span style="color:#f92672">=</span> <span style="color:#ae81ff">101</span>

<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(EPOCHS):
    <span style="color:#66d9ef">for</span> features, labels  <span style="color:#f92672">in</span> dataset:
        <span style="color:#75715e"># X, Y Type Case</span>
        features <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(features, tf<span style="color:#f92672">.</span>float32)
        labels <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(labels, tf<span style="color:#f92672">.</span>float32)
        <span style="color:#75715e"># Gradient Tape</span>
        grads, loss_value <span style="color:#f92672">=</span> grad(linearReg_fn(features), features, labels, True)
        optimizer<span style="color:#f92672">.</span>apply_gradients(grads_and_vars<span style="color:#f92672">=</span>zip(grads,[W,b]))          
    <span style="color:#75715e"># Verbose 10</span>
    <span style="color:#66d9ef">if</span> step <span style="color:#f92672">%</span> <span style="color:#ae81ff">10</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Iter: {}, Loss: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(step, loss_value))
</code></pre></div></div>
    </div>
  </label>
</div>


<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-07-4</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf

tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>set_seed(<span style="color:#ae81ff">777</span>)  <span style="color:#75715e"># for reproducibility</span>
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)

<span style="color:#75715e"># MNIST Dataset 정의</span>
mnist <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>datasets<span style="color:#f92672">.</span>mnist

<span style="color:#75715e"># Train Data와 Test Data Load</span>
(x_train, y_train),(x_test, y_test) <span style="color:#f92672">=</span> mnist<span style="color:#f92672">.</span>load_data()
x_train, x_test <span style="color:#f92672">=</span> x_train <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.0</span>, x_test <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.0</span>

<span style="color:#75715e"># Kera Model Data</span>
model <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>Sequential([
  tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Flatten(),
  tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">512</span>, activation<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>relu),
  tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dropout(<span style="color:#ae81ff">0.2</span>),
  tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">10</span>, activation<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>softmax)
])

<span style="color:#75715e"># Model : Adam Optimizer / Cros Entropy Loss 선언</span>
model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>,
              loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;sparse_categorical_crossentropy&#39;</span>,
              metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>])

<span style="color:#75715e"># 5 Epoch 학습 수행</span>
model<span style="color:#f92672">.</span>fit(x_train, y_train, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)

<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">%</span>matplotlib inline

sample <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
image <span style="color:#f92672">=</span> x_test[sample]

fig <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>figure
plt<span style="color:#f92672">.</span>imshow(image, cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray_r&#39;</span>)
plt<span style="color:#f92672">.</span>show

num <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
images <span style="color:#f92672">=</span> x_train[:num]
labels <span style="color:#f92672">=</span> y_train[:num]

num_row <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>
num_col <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
fig, axes <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(num_row,num_col, figsize <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1.5</span><span style="color:#f92672">*</span>num_col,<span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>num_row))

<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num) :
  ax <span style="color:#f92672">=</span> axes[i<span style="color:#f92672">//</span>num_col, i<span style="color:#f92672">%</span>num_col]
  ax<span style="color:#f92672">.</span>imshow(images[i], cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray_r&#39;</span>)
  ax<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#39;Label: {}&#39;</span><span style="color:#f92672">.</span>format(labels[i]))

plt<span style="color:#f92672">.</span>tight_layout()
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># 모델 평가</span>
model<span style="color:#f92672">.</span>evaluate(x_test, y_test)

img_index <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
plt<span style="color:#f92672">.</span>imshow(x_test[img_index]<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">28</span>,<span style="color:#ae81ff">28</span>), cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray_r&#39;</span>)
pred <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(x_test[img_index]<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">28</span>,<span style="color:#ae81ff">28</span>,<span style="color:#ae81ff">1</span>))
<span style="color:#66d9ef">print</span>(pred<span style="color:#f92672">.</span>argmax())

num <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>
images <span style="color:#f92672">=</span> x_test[:num]
labels <span style="color:#f92672">=</span> y_test[:num]

num_row <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
num_col <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
fig, axes <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(num_row,num_col, figsize <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1.5</span><span style="color:#f92672">*</span>num_col,<span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>num_row))

<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num) :
  ax <span style="color:#f92672">=</span> axes[i<span style="color:#f92672">//</span>num_col, i<span style="color:#f92672">%</span>num_col]
  ax<span style="color:#f92672">.</span>imshow(images[i], cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray_r&#39;</span>)
  pred <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(x_test[i]<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">28</span>,<span style="color:#ae81ff">28</span>,<span style="color:#ae81ff">1</span>))  
  ax<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#39;Label: {}&#39;</span><span style="color:#f92672">.</span>format(pred<span style="color:#f92672">.</span>argmax()))

plt<span style="color:#f92672">.</span>tight_layout()
plt<span style="color:#f92672">.</span>show()

<span style="color:#66d9ef">print</span>(x_test<span style="color:#f92672">.</span>shape)

</code></pre></div></div>
    </div>
  </label>
</div>


<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-07-5</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">from</span> tensorflow <span style="color:#f92672">import</span> keras

tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>set_seed(<span style="color:#ae81ff">777</span>)  <span style="color:#75715e"># for reproducibility</span>
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)

<span style="color:#75715e"># fashion_mnist 선언</span>
fashion_mnist <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>datasets<span style="color:#f92672">.</span>fashion_mnist

<span style="color:#75715e"># train_data, train_label, test_data, test_label 선언</span>
(train_images, train_labels), (test_images, test_labels) <span style="color:#f92672">=</span> fashion_mnist<span style="color:#f92672">.</span>load_data()

<span style="color:#75715e"># 0 - T-shirt/Top</span>
<span style="color:#75715e"># 1 - Trouser</span>
<span style="color:#75715e"># 2 - Pullover</span>
<span style="color:#75715e"># 3 - Dress</span>
<span style="color:#75715e"># 4 - Coat</span>
<span style="color:#75715e"># 5 - Sandal</span>
<span style="color:#75715e"># 6 - Shirt</span>
<span style="color:#75715e"># 7 - Sneaker</span>
<span style="color:#75715e"># 8 - Bag</span>
<span style="color:#75715e"># 9 - Ankle Boot</span>
class_names <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;T-shirt/top&#39;</span>, <span style="color:#e6db74">&#39;Trouser&#39;</span>, <span style="color:#e6db74">&#39;Pullover&#39;</span>, <span style="color:#e6db74">&#39;Dress&#39;</span>, <span style="color:#e6db74">&#39;Coat&#39;</span>, <span style="color:#e6db74">&#39;Sandal&#39;</span>, <span style="color:#e6db74">&#39;Shirt&#39;</span>, <span style="color:#e6db74">&#39;Sneaker&#39;</span>, <span style="color:#e6db74">&#39;Bag&#39;</span>, <span style="color:#e6db74">&#39;Ankle boot&#39;</span>]

plt<span style="color:#f92672">.</span>figure()
plt<span style="color:#f92672">.</span>imshow(train_images[<span style="color:#ae81ff">0</span>])
plt<span style="color:#f92672">.</span>colorbar()
plt<span style="color:#f92672">.</span>grid(False)

num <span style="color:#f92672">=</span> <span style="color:#ae81ff">20</span>
images <span style="color:#f92672">=</span> train_images[:num]<span style="color:#f92672">/</span><span style="color:#ae81ff">255.0</span>
labels <span style="color:#f92672">=</span> train_labels[:num]<span style="color:#f92672">/</span><span style="color:#ae81ff">255.0</span>

num_col <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
num_row <span style="color:#f92672">=</span> int(num <span style="color:#f92672">/</span> num_col)

fig, axes <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(num_row, num_col, figsize <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1.5</span><span style="color:#f92672">*</span>num_col, <span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>num_row))

<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num) :
  ax <span style="color:#f92672">=</span> axes[i<span style="color:#f92672">//</span>num_col, i<span style="color:#f92672">%</span>num_col]
  ax<span style="color:#f92672">.</span>imshow(train_images[i], cmap<span style="color:#f92672">=</span>plt<span style="color:#f92672">.</span>cm<span style="color:#f92672">.</span>binary)
  ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;bottom&#39;</span>]<span style="color:#f92672">.</span>set_color(<span style="color:#e6db74">&#39;white&#39;</span>)
  ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;top&#39;</span>]<span style="color:#f92672">.</span>set_color(<span style="color:#e6db74">&#39;white&#39;</span>)
  ax<span style="color:#f92672">.</span>yaxis<span style="color:#f92672">.</span>label<span style="color:#f92672">.</span>set_color(<span style="color:#e6db74">&#39;white&#39;</span>)
  ax<span style="color:#f92672">.</span>xaxis<span style="color:#f92672">.</span>label<span style="color:#f92672">.</span>set_color(<span style="color:#e6db74">&#39;white&#39;</span>)
  ax<span style="color:#f92672">.</span>title<span style="color:#f92672">.</span>set_color(<span style="color:#e6db74">&#39;white&#39;</span>)
  ax<span style="color:#f92672">.</span>tick_params(axis<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;y&#39;</span>, colors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;white&#34;</span>)  
  ax<span style="color:#f92672">.</span>tick_params(axis<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;x&#39;</span>, colors<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;white&#39;</span>)
  ax<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#39;Label: {}&#39;</span><span style="color:#f92672">.</span>format(train_labels[i]))

plt<span style="color:#f92672">.</span>tight_layout()
plt<span style="color:#f92672">.</span>figure()
plt<span style="color:#f92672">.</span>show()

train_images <span style="color:#f92672">=</span> train_images <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.0</span>
test_images <span style="color:#f92672">=</span> test_images <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.0</span>

plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>,<span style="color:#ae81ff">10</span>))
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">25</span>):
    plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">5</span>,<span style="color:#ae81ff">5</span>,i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)
    plt<span style="color:#f92672">.</span>xticks([])
    plt<span style="color:#f92672">.</span>yticks([])
    plt<span style="color:#f92672">.</span>grid(True)
    plt<span style="color:#f92672">.</span>imshow(train_images[i], cmap<span style="color:#f92672">=</span>plt<span style="color:#f92672">.</span>cm<span style="color:#f92672">.</span>binary)
    plt<span style="color:#f92672">.</span>xlabel(class_names[train_labels[i]])

model <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>Sequential([
    keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Flatten(input_shape<span style="color:#f92672">=</span>(<span style="color:#ae81ff">28</span>, <span style="color:#ae81ff">28</span>)),
    keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">128</span>, activation<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>relu),
    keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">10</span>, activation<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>softmax)
])

model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>,
              loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;sparse_categorical_crossentropy&#39;</span>,
              metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>])
model<span style="color:#f92672">.</span>fit(train_images, train_labels, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)

test_loss, test_acc <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>evaluate(test_images, test_labels)

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Test accuracy:&#39;</span>, test_acc)
</code></pre></div></div>
    </div>
  </label>
</div>


<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-07-6</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf
<span style="color:#f92672">from</span> tensorflow <span style="color:#f92672">import</span> keras

tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>set_seed(<span style="color:#ae81ff">777</span>)  <span style="color:#75715e"># for reproducibility</span>
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)

<span style="color:#75715e"># 50,000 Movie Reviews IMDB (10000개의 빈도수가 높은 단어를 학습시 Vector에 사용)</span>
imdb <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>datasets<span style="color:#f92672">.</span>imdb

<span style="color:#75715e"># train_data, train_labels, test_data, test_labels</span>
(train_data, train_labels), (test_data, test_labels) <span style="color:#f92672">=</span> imdb<span style="color:#f92672">.</span>load_data(num_words<span style="color:#f92672">=</span><span style="color:#ae81ff">10000</span>)

<span style="color:#75715e"># train_data, train_labels</span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Training entries: {}, labels: {}&#34;</span><span style="color:#f92672">.</span>format(len(train_data), len(train_labels)))
<span style="color:#66d9ef">print</span>(train_data[<span style="color:#ae81ff">0</span>])

<span style="color:#75715e"># IMDB Data를 Vector을 실재 값으로 변환하여 출력</span>
<span style="color:#75715e"># A dictionary mapping words to an integer index</span>
<span style="color:#75715e"># 딕셔너리 단어를 인티져 인덱스로 맵핑</span>
word_index <span style="color:#f92672">=</span> imdb<span style="color:#f92672">.</span>get_word_index()

<span style="color:#75715e"># The first indices are reserved</span>
<span style="color:#75715e"># Data Check : Start 1 and &#39;the&#39;</span>
<span style="color:#66d9ef">for</span> k,v <span style="color:#f92672">in</span> word_index<span style="color:#f92672">.</span>items() :
  <span style="color:#75715e">#print(len(k))</span>
  <span style="color:#66d9ef">if</span> k <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;bad&#39;</span> : 
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;K : &#34;</span>,k, <span style="color:#e6db74">&#34; V : &#34;</span>, v<span style="color:#f92672">+</span><span style="color:#ae81ff">3</span>)
  <span style="color:#66d9ef">if</span> v <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span> :
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;K : &#34;</span>,k, <span style="color:#e6db74">&#34; V : &#34;</span>, v)
  <span style="color:#66d9ef">if</span> v <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span> :
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;K : &#34;</span>,k, <span style="color:#e6db74">&#34; V : &#34;</span>, v)

<span style="color:#75715e"># 최초 익덱스들은 Reserved : 0~3 예약어로 저장</span>
word_index <span style="color:#f92672">=</span> {k:(v<span style="color:#f92672">+</span><span style="color:#ae81ff">3</span>) <span style="color:#66d9ef">for</span> k,v <span style="color:#f92672">in</span> word_index<span style="color:#f92672">.</span>items()}
word_index[<span style="color:#e6db74">&#34;&lt;PAD&gt;&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
word_index[<span style="color:#e6db74">&#34;&lt;START&gt;&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
word_index[<span style="color:#e6db74">&#34;&lt;UNK&gt;&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>  <span style="color:#75715e"># unknown</span>
word_index[<span style="color:#e6db74">&#34;&lt;UNUSED&gt;&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>

reverse_word_index <span style="color:#f92672">=</span> dict([(value, key) <span style="color:#66d9ef">for</span> (key, value) <span style="color:#f92672">in</span> word_index<span style="color:#f92672">.</span>items()])

<span style="color:#75715e"># Word List : 0~3, 4~88587</span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;check : &#39;</span>, reverse_word_index<span style="color:#f92672">.</span>get(<span style="color:#ae81ff">0</span>,<span style="color:#e6db74">&#39;?&#39;</span>))
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;check : &#39;</span>, reverse_word_index<span style="color:#f92672">.</span>get(<span style="color:#ae81ff">88587</span>,<span style="color:#e6db74">&#39;?&#39;</span>))
<span style="color:#66d9ef">print</span>(len(reverse_word_index))

<span style="color:#75715e"># Review를 Decode</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">decode_review</span>(text):
    <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#39; &#39;</span><span style="color:#f92672">.</span>join([reverse_word_index<span style="color:#f92672">.</span>get(i, <span style="color:#e6db74">&#39;?&#39;</span>) <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> text])

check_data_id <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>

<span style="color:#66d9ef">print</span>(len(train_data[check_data_id]))

<span style="color:#75715e"># Train Data Value Print</span>
<span style="color:#66d9ef">print</span>(train_data[check_data_id])

<span style="color:#75715e"># Print label </span>
<span style="color:#66d9ef">print</span>(train_labels[check_data_id])

<span style="color:#75715e"># Train Data Decode Print</span>
decode_review(train_data[check_data_id])

<span style="color:#75715e"># 학습과 평가를 위해 동일길이인 256길이의 단어로 PAD값을 주어 맞춰줌 (뒤의 길이는 0값으로 맞춰줌)</span>
train_data <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>preprocessing<span style="color:#f92672">.</span>sequence<span style="color:#f92672">.</span>pad_sequences(train_data,
                                                        value<span style="color:#f92672">=</span>word_index[<span style="color:#e6db74">&#34;&lt;PAD&gt;&#34;</span>],
                                                        padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;post&#39;</span>,
                                                        maxlen<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>)

test_data <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>preprocessing<span style="color:#f92672">.</span>sequence<span style="color:#f92672">.</span>pad_sequences(test_data,
                                                       value<span style="color:#f92672">=</span>word_index[<span style="color:#e6db74">&#34;&lt;PAD&gt;&#34;</span>],
                                                       padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;post&#39;</span>,
                                                       maxlen<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>)

<span style="color:#66d9ef">print</span>(len(train_data[check_data_id]), len(test_data[check_data_id]))
<span style="color:#66d9ef">print</span>(train_data[check_data_id])

<span style="color:#75715e"># Tensorflow keras API를 통해 모델에 대한 정의</span>
<span style="color:#75715e"># 입력 Size와 학습시킬 Layer의 크기와 Activation Function 정의</span>
<span style="color:#75715e"># input shape is the vocabulary count used for the movie reviews (10,000 words)</span>
vocab_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">10000</span>

model <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>Sequential()
model<span style="color:#f92672">.</span>add(keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Embedding(vocab_size, <span style="color:#ae81ff">16</span>))
model<span style="color:#f92672">.</span>add(keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>GlobalAveragePooling1D())
model<span style="color:#f92672">.</span>add(keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">16</span>, activation<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>relu))
model<span style="color:#f92672">.</span>add(keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">1</span>, activation<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>sigmoid))

<span style="color:#75715e"># Model Summary 출력</span>
model<span style="color:#f92672">.</span>summary()

<span style="color:#75715e"># Adam Optimizer과 Cross Entropy Loss 선언</span>
model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>,
              loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;binary_crossentropy&#39;</span>,
              metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>])

<span style="color:#75715e"># 모델을 평가할 Test 데이타에 대한 정의(10000을 기준으로 학습과 평가 수행)</span>
<span style="color:#75715e"># 총 : 25000 평가 ~10000 / 학습 10001~</span>
x_val <span style="color:#f92672">=</span> train_data[:<span style="color:#ae81ff">10000</span>]
partial_x_train <span style="color:#f92672">=</span> train_data[<span style="color:#ae81ff">10000</span>:]

y_val <span style="color:#f92672">=</span> train_labels[:<span style="color:#ae81ff">10000</span>]
partial_y_train <span style="color:#f92672">=</span> train_labels[<span style="color:#ae81ff">10000</span>:]

history <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>fit(partial_x_train,
                    partial_y_train,
                    epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">40</span>,
                    batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>,
                    validation_data<span style="color:#f92672">=</span>(x_val, y_val),
                    verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)

results <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>evaluate(test_data, test_labels)
<span style="color:#66d9ef">print</span>(results)
</code></pre></div></div>
    </div>
  </label>
</div>


<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-09-1</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">%</span>matplotlib inline
<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf

tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>set_seed(<span style="color:#ae81ff">777</span>)  <span style="color:#75715e"># for reproducibility</span>

<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)

<span style="color:#75715e"># XOR : 00 -&gt; 0, 01 -&gt; 1, 10 -&gt; 1, 11 -&gt; 0</span>
x_data <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>],[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>],[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>],[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>]]
y_data <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">0</span>]   ,   [<span style="color:#ae81ff">1</span>],   [<span style="color:#ae81ff">1</span>],   [<span style="color:#ae81ff">0</span>]]

plt<span style="color:#f92672">.</span>scatter(x_data[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>],x_data[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;red&#39;</span> , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)
plt<span style="color:#f92672">.</span>scatter(x_data[<span style="color:#ae81ff">3</span>][<span style="color:#ae81ff">0</span>],x_data[<span style="color:#ae81ff">3</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;orange&#39;</span> , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)
plt<span style="color:#f92672">.</span>scatter(x_data[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">0</span>],x_data[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;blue&#39;</span> , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)
plt<span style="color:#f92672">.</span>scatter(x_data[<span style="color:#ae81ff">2</span>][<span style="color:#ae81ff">0</span>],x_data[<span style="color:#ae81ff">2</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;green&#39;</span> , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)

plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;x1&#34;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;x2&#34;</span>)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># Tensorflow data API를 통해 학습시킬 값들을 담는다 (Batch Size는 한번에 학습시킬 Size로 정한다)</span>
<span style="color:#75715e"># preprocess function으로 features,labels는 실재 학습에 쓰일 Data 연산을 위해 Type를 맞춰준다</span>
dataset <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices((x_data, y_data))<span style="color:#f92672">.</span>batch(len(x_data))

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">preprocess_data</span>(features, labels):
    features <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(features, tf<span style="color:#f92672">.</span>float32)
    labels <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(labels, tf<span style="color:#f92672">.</span>float32)
    <span style="color:#66d9ef">return</span> features, labels

<span style="color:#75715e"># Weight Bias 초기화</span>
W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>zeros((<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">1</span>)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight&#39;</span>)
b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>zeros((<span style="color:#ae81ff">1</span>,)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias&#39;</span>)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;W = {}, B = {}&#34;</span><span style="color:#f92672">.</span>format(W<span style="color:#f92672">.</span>numpy(), b<span style="color:#f92672">.</span>numpy()))

<span style="color:#75715e"># Sigmoid를 가설로 선언</span>
<span style="color:#75715e"># 0과 1의 값만을 리턴 : tf.sigmoid(tf.matmul(X, W) + b)</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">logistic_regression</span>(features):
    hypothesis  <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>divide(<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">1.</span> <span style="color:#f92672">+</span> tf<span style="color:#f92672">.</span>exp(tf<span style="color:#f92672">.</span>matmul(features, W) <span style="color:#f92672">+</span> b))
    <span style="color:#66d9ef">return</span> hypothesis

<span style="color:#75715e"># Cost 함수 정의</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss_fn</span>(hypothesis, features, labels):
    cost <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>tf<span style="color:#f92672">.</span>reduce_mean(labels <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(logistic_regression(features)) <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> labels) <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> hypothesis))
    <span style="color:#66d9ef">return</span> cost

optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>)

<span style="color:#75715e"># Sigmoid 함수를 통해 예측값이 0.5보다 크면 1을 반환하고 0.5보다 작으면 0으로 반환</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">accuracy_fn</span>(hypothesis, labels):
    predicted <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(hypothesis <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0.5</span>, dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32)
    accuracy <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>cast(tf<span style="color:#f92672">.</span>equal(predicted, labels), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32))
    <span style="color:#66d9ef">return</span> accuracy

<span style="color:#75715e"># GradientTape을 통해 경사값을 계산</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad</span>(hypothesis, features, labels):
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
        loss_value <span style="color:#f92672">=</span> loss_fn(logistic_regression(features),features,labels)
    <span style="color:#66d9ef">return</span> tape<span style="color:#f92672">.</span>gradient(loss_value, [W,b])

<span style="color:#75715e"># epoch 1000회</span>
EPOCHS <span style="color:#f92672">=</span> <span style="color:#ae81ff">1001</span>

<span style="color:#75715e"># epoch에서 1000회 step</span>
<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(EPOCHS):
    <span style="color:#75715e"># x, y</span>
    <span style="color:#66d9ef">for</span> features, labels  <span style="color:#f92672">in</span> dataset:
        <span style="color:#75715e"># features, labels를 받아 float을 맞춤</span>
        features, labels <span style="color:#f92672">=</span> preprocess_data(features, labels)
        
        <span style="color:#75715e"># gradient tape</span>
        grads <span style="color:#f92672">=</span> grad(logistic_regression(features), features, labels)
        optimizer<span style="color:#f92672">.</span>apply_gradients(grads_and_vars<span style="color:#f92672">=</span>zip(grads,[W,b]))

        <span style="color:#75715e"># Verbose 100</span>
        <span style="color:#66d9ef">if</span> step <span style="color:#f92672">%</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Iter: {}, Loss: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(step, loss_fn(logistic_regression(features),features,labels)))

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;W = {}, B = {}&#34;</span><span style="color:#f92672">.</span>format(W<span style="color:#f92672">.</span>numpy(), b<span style="color:#f92672">.</span>numpy()))
x_data, y_data <span style="color:#f92672">=</span> preprocess_data(x_data, y_data)
test_acc <span style="color:#f92672">=</span> accuracy_fn(logistic_regression(x_data),y_data)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Testset Accuracy: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(test_acc))

plt<span style="color:#f92672">.</span>scatter(x_data[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>],x_data[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;red&#39;</span> , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)
plt<span style="color:#f92672">.</span>scatter(x_data[<span style="color:#ae81ff">3</span>][<span style="color:#ae81ff">0</span>],x_data[<span style="color:#ae81ff">3</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;orange&#39;</span> , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)
plt<span style="color:#f92672">.</span>scatter(x_data[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">0</span>],x_data[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;blue&#39;</span> , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)
plt<span style="color:#f92672">.</span>scatter(x_data[<span style="color:#ae81ff">2</span>][<span style="color:#ae81ff">0</span>],x_data[<span style="color:#ae81ff">2</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;green&#39;</span> , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)
plt<span style="color:#f92672">.</span>plot(x_data,logistic_regression(x_data), c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray&#39;</span>)

plt<span style="color:#f92672">.</span>tick_params(axis<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;y&#39;</span>, colors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;white&#34;</span>)  
plt<span style="color:#f92672">.</span>tick_params(axis<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;x&#39;</span>, colors<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;white&#39;</span>)

plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;x1&#34;</span>,color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;white&#34;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;x2&#34;</span>,color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;white&#34;</span>)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># W1,b1,W2,b2,W3,b3 정의</span>
W1 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight1&#39;</span>)
b1 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">1</span>,)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias1&#39;</span>)
W2 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight2&#39;</span>)
b2 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">1</span>,)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias2&#39;</span>)
W3 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight3&#39;</span>)
b3 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">1</span>,)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias3&#39;</span>)

<span style="color:#75715e"># layer1 : sigmoid ─┬─&gt; layer3</span>
<span style="color:#75715e"># layer2 : sigmoid --┘</span>
<span style="color:#75715e"># hypothesis 가설</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">neural_net</span>(features):
    layer1 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>sigmoid(tf<span style="color:#f92672">.</span>matmul(features, W1) <span style="color:#f92672">+</span> b1)
    layer2 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>sigmoid(tf<span style="color:#f92672">.</span>matmul(features, W2) <span style="color:#f92672">+</span> b2)
    layer3 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>concat([layer1, layer2],<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
    layer3 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reshape(layer3, shape <span style="color:#f92672">=</span> [<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>])
    hypothesis <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>sigmoid(tf<span style="color:#f92672">.</span>matmul(layer3, W3) <span style="color:#f92672">+</span> b3)
    <span style="color:#66d9ef">return</span> hypothesis

<span style="color:#75715e"># 손실함수</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss_fn</span>(hypothesis, labels):
    cost <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>tf<span style="color:#f92672">.</span>reduce_mean(labels <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(hypothesis) <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> labels) <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> hypothesis))
    <span style="color:#66d9ef">return</span> cost

<span style="color:#75715e"># Optimizer SGD</span>
optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>)

<span style="color:#75715e"># Accuracy 측정</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">accuracy_fn</span>(hypothesis, labels):
    predicted <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(hypothesis <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0.5</span>, dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32)
    accuracy <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>cast(tf<span style="color:#f92672">.</span>equal(predicted, labels), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32))
    <span style="color:#66d9ef">return</span> accuracy

<span style="color:#75715e"># Gradient Tape</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad</span>(hypothesis, features, labels):
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
        loss_value <span style="color:#f92672">=</span> loss_fn(neural_net(features),labels)
    <span style="color:#66d9ef">return</span> tape<span style="color:#f92672">.</span>gradient(loss_value, [W1, W2, W3, b1, b2, b3])

<span style="color:#75715e"># epoch 50000</span>
EPOCHS <span style="color:#f92672">=</span> <span style="color:#ae81ff">50000</span>

<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(EPOCHS):
    <span style="color:#66d9ef">for</span> features, labels  <span style="color:#f92672">in</span> dataset:
        <span style="color:#75715e"># preprocess 처리</span>
        features, labels <span style="color:#f92672">=</span> preprocess_data(features, labels)
        
        <span style="color:#75715e"># neural_net : layer1 + layer2 -&gt; layer3</span>
        grads <span style="color:#f92672">=</span> grad(neural_net(features), features, labels)

        <span style="color:#75715e"># optimizer</span>
        optimizer<span style="color:#f92672">.</span>apply_gradients(grads_and_vars<span style="color:#f92672">=</span>zip(grads,[W1, W2, W3, b1, b2, b3]))

        <span style="color:#75715e"># verbose 5000</span>
        <span style="color:#66d9ef">if</span> step <span style="color:#f92672">%</span> <span style="color:#ae81ff">5000</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Iter: {}, Loss: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(step, loss_fn(neural_net(features),labels)))

x_data, y_data <span style="color:#f92672">=</span> preprocess_data(x_data, y_data)

<span style="color:#75715e"># test data</span>
test_acc <span style="color:#f92672">=</span> accuracy_fn(neural_net(x_data),y_data)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Testset Accuracy: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(test_acc))

plt<span style="color:#f92672">.</span>scatter(x_data[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>],x_data[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;red&#39;</span> , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)
plt<span style="color:#f92672">.</span>scatter(x_data[<span style="color:#ae81ff">3</span>][<span style="color:#ae81ff">0</span>],x_data[<span style="color:#ae81ff">3</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;orange&#39;</span> , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)
plt<span style="color:#f92672">.</span>scatter(x_data[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">0</span>],x_data[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;blue&#39;</span> , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)
plt<span style="color:#f92672">.</span>scatter(x_data[<span style="color:#ae81ff">2</span>][<span style="color:#ae81ff">0</span>],x_data[<span style="color:#ae81ff">2</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;green&#39;</span> , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)

plt<span style="color:#f92672">.</span>plot(x_data,neural_net(x_data), c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray&#39;</span>)

plt<span style="color:#f92672">.</span>tick_params(axis<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;y&#39;</span>, colors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;white&#34;</span>)  
plt<span style="color:#f92672">.</span>tick_params(axis<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;x&#39;</span>, colors<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;white&#39;</span>)

plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;x1&#34;</span>,color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;white&#34;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;x2&#34;</span>,color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;white&#34;</span>)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># XOR 문제를 Deep Neural Network 활용 풀이</span>
<span style="color:#75715e"># dataset </span>
dataset <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices((x_data, y_data))<span style="color:#f92672">.</span>batch(len(x_data))
nb_classes <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>

<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">wide_deep_nn</span>():
  <span style="color:#75715e"># 초기화</span>
  <span style="color:#66d9ef">def</span> __init__(self, nb_classes):
      <span style="color:#75715e"># 초기화</span>
      super(wide_deep_nn, self)<span style="color:#f92672">.</span>__init__()
      <span style="color:#75715e"># W1,b1,W2,b2,W3,b3,W4,b4 정의</span>
      self<span style="color:#f92672">.</span>W1 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">2</span>, nb_classes)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight1&#39;</span>)
      self<span style="color:#f92672">.</span>b1 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((nb_classes,)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias1&#39;</span>)
      self<span style="color:#f92672">.</span>W2 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((nb_classes, nb_classes)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight2&#39;</span>)
      self<span style="color:#f92672">.</span>b2 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((nb_classes,)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias2&#39;</span>)
      self<span style="color:#f92672">.</span>W3 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((nb_classes, nb_classes)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight3&#39;</span>)
      self<span style="color:#f92672">.</span>b3 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((nb_classes,)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias3&#39;</span>)
      self<span style="color:#f92672">.</span>W4 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((nb_classes,<span style="color:#ae81ff">1</span>)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight4&#39;</span>)
      self<span style="color:#f92672">.</span>b4 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">1</span>,)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias4&#39;</span>)
      self<span style="color:#f92672">.</span>variables <span style="color:#f92672">=</span> [self<span style="color:#f92672">.</span>W1, self<span style="color:#f92672">.</span>b1,self<span style="color:#f92672">.</span>W2, self<span style="color:#f92672">.</span>b2, self<span style="color:#f92672">.</span>W3, self<span style="color:#f92672">.</span>b3, self<span style="color:#f92672">.</span>W4, self<span style="color:#f92672">.</span>b4]

  <span style="color:#75715e"># data preprocessing</span>
  <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">preprocess_data</span>(self, features, labels):
    features <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(features, tf<span style="color:#f92672">.</span>float32)
    labels <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(labels, tf<span style="color:#f92672">.</span>float32)
    <span style="color:#66d9ef">return</span> features, labels

  <span style="color:#75715e"># 4Layer의 Neural Network를 통해 학습시킨 후 모델을 생성</span>
  <span style="color:#75715e"># layer1-&gt;layer2-&gt;layer3-&gt;hypothesis</span>
  <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">deep_nn</span>(self, features):
    layer1 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>sigmoid(tf<span style="color:#f92672">.</span>matmul(features, self<span style="color:#f92672">.</span>W1)<span style="color:#f92672">+</span>self<span style="color:#f92672">.</span>b1)
    layer2 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>sigmoid(tf<span style="color:#f92672">.</span>matmul(layer1, self<span style="color:#f92672">.</span>W2)<span style="color:#f92672">+</span>self<span style="color:#f92672">.</span>b2)
    layer3 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>sigmoid(tf<span style="color:#f92672">.</span>matmul(layer2, self<span style="color:#f92672">.</span>W3)<span style="color:#f92672">+</span>self<span style="color:#f92672">.</span>b3)
    hypothesis <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>sigmoid(tf<span style="color:#f92672">.</span>matmul(layer3, self<span style="color:#f92672">.</span>W4)<span style="color:#f92672">+</span>self<span style="color:#f92672">.</span>b4)
    <span style="color:#66d9ef">return</span> hypothesis

  <span style="color:#75715e"># loss function </span>
  <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss_fn</span>(self, hypothesis, features, labels):
    cost <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>tf<span style="color:#f92672">.</span>reduce_mean(labels<span style="color:#f92672">*</span>tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(hypothesis)<span style="color:#f92672">+</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>labels)<span style="color:#f92672">*</span>tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>hypothesis))
    <span style="color:#66d9ef">return</span> cost

  <span style="color:#75715e"># accuracy function</span>
  <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">accuracy_fn</span>(self, hypothesis,labels):
    predicted <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(hypothesis <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0.5</span>, dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32)
    accuracy <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>cast(tf<span style="color:#f92672">.</span>equal(predicted, labels), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32))
    <span style="color:#66d9ef">return</span> accuracy

  <span style="color:#75715e"># Gradient Tape </span>
  <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad</span>(self, hypothesis, features, labels):
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
      loss_value<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>loss_fn(self<span style="color:#f92672">.</span>deep_nn(features),features,labels)
    <span style="color:#66d9ef">return</span> tape<span style="color:#f92672">.</span>gradient(loss_value,self<span style="color:#f92672">.</span>variables)

  <span style="color:#75715e"># epochs 2000, verbose 500</span>
  <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fit</span>(self, dataset, EPOCHS<span style="color:#f92672">=</span><span style="color:#ae81ff">20000</span>, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">500</span>):
    optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>)
    <span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(EPOCHS):
      <span style="color:#66d9ef">for</span> features, labels <span style="color:#f92672">in</span> dataset:
        features, labels <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>preprocess_data(features, labels)
        grads <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>grad(self<span style="color:#f92672">.</span>deep_nn(features),features,labels)
        optimizer<span style="color:#f92672">.</span>apply_gradients(grads_and_vars<span style="color:#f92672">=</span>zip(grads, self<span style="color:#f92672">.</span>variables))
        <span style="color:#66d9ef">if</span> step <span style="color:#f92672">%</span> verbose <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
          <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Iter: {}, Loss: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(step, self<span style="color:#f92672">.</span>loss_fn(self<span style="color:#f92672">.</span>deep_nn(features),features,labels)))
  <span style="color:#75715e"># Model Test</span>
  <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">test_model</span>(self,x_data,y_data):
    <span style="color:#75715e"># Test Data preprocessing</span>
    x_data,y_data <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>preprocess_data(x_data,y_data)
    <span style="color:#75715e"># Accuracy check</span>
    test_acc <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>accuracy_fn(self<span style="color:#f92672">.</span>deep_nn(x_data),y_data)
    <span style="color:#75715e"># Accuracy Printing</span>
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Testset Accuracy: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(test_acc))

<span style="color:#75715e"># model 정의</span>
model <span style="color:#f92672">=</span> wide_deep_nn(nb_classes)

<span style="color:#75715e"># fitting</span>
model<span style="color:#f92672">.</span>fit(dataset)

<span style="color:#75715e"># test data accuracy</span>
model<span style="color:#f92672">.</span>test_model(x_data, y_data)

<span style="color:#75715e"># dataset 정의</span>
dataset <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices((x_data,y_data))<span style="color:#f92672">.</span>batch(len(x_data))

<span style="color:#75715e"># data preprocessing 정의</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">preprocess_data</span>(features, labels):
  features <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(features, tf<span style="color:#f92672">.</span>float32)
  labels <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(labels, tf<span style="color:#f92672">.</span>float32)
  <span style="color:#66d9ef">return</span> features, labels

<span style="color:#75715e"># summary 값을 logs폴더에 저장하고 아래 명령어로 실행해서 확인</span>
<span style="color:#75715e"># tensorboard --logdir=./logs/xor</span>
log_path <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;./logs/xor&#34;</span>
writer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>summary<span style="color:#f92672">.</span>create_file_writer(log_path)

<span style="color:#75715e"># 위의 Data를 4Layer의 Neural Network를 통해 학습시킨 후 모델을 생성</span>
<span style="color:#75715e"># 각각의 값을 histogram으로 tensorboard에 저장 (Model)</span>
<span style="color:#75715e"># 각각의 값을 scalar값으로 tensorboard에 저장 (cost, accuracy)</span>
<span style="color:#75715e"># W1,b1,W2,b2,W3,b3,W4,b4 정의 및 선언</span>
W1 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">10</span>)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight1&#39;</span>)
b1 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">10</span>,)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias1&#39;</span>)
W2 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">10</span>,<span style="color:#ae81ff">10</span>)),name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight2&#39;</span>)
b2 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">10</span>,)),name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias2&#39;</span>)
W3 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">10</span>,<span style="color:#ae81ff">10</span>)),name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight3&#39;</span>)
b3 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">10</span>,)),name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias3&#39;</span>)
W4 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">10</span>,<span style="color:#ae81ff">1</span>)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight4&#39;</span>)
b4 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">1</span>,)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias4&#39;</span>)

<span style="color:#75715e"># layer1 -&gt; layer2 -&gt; layer3 -&gt; hypothesis</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">neural_net</span>(features, step):
  layer1 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>sigmoid(tf<span style="color:#f92672">.</span>matmul(features, W1) <span style="color:#f92672">+</span> b1)
  layer2 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>sigmoid(tf<span style="color:#f92672">.</span>matmul(layer1, W2) <span style="color:#f92672">+</span> b2)
  layer3 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>sigmoid(tf<span style="color:#f92672">.</span>matmul(layer2, W3) <span style="color:#f92672">+</span> b3)
  hypothesis <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>sigmoid(tf<span style="color:#f92672">.</span>matmul(layer3,W4) <span style="color:#f92672">+</span> b4)

  <span style="color:#75715e"># tensorboard에 저장</span>
  <span style="color:#66d9ef">with</span> writer<span style="color:#f92672">.</span>as_default():
    tf<span style="color:#f92672">.</span>summary<span style="color:#f92672">.</span>histogram(<span style="color:#e6db74">&#34;weights1&#34;</span>,W1,step<span style="color:#f92672">=</span>step)
    tf<span style="color:#f92672">.</span>summary<span style="color:#f92672">.</span>histogram(<span style="color:#e6db74">&#34;biases1&#34;</span>,b1,step<span style="color:#f92672">=</span>step)
    tf<span style="color:#f92672">.</span>summary<span style="color:#f92672">.</span>histogram(<span style="color:#e6db74">&#34;layer1&#34;</span>,layer1,step<span style="color:#f92672">=</span>step)
    tf<span style="color:#f92672">.</span>summary<span style="color:#f92672">.</span>histogram(<span style="color:#e6db74">&#34;weights2&#34;</span>,W2,step<span style="color:#f92672">=</span>step)
    tf<span style="color:#f92672">.</span>summary<span style="color:#f92672">.</span>histogram(<span style="color:#e6db74">&#34;biases2&#34;</span>,b2,step<span style="color:#f92672">=</span>step)
    tf<span style="color:#f92672">.</span>summary<span style="color:#f92672">.</span>histogram(<span style="color:#e6db74">&#34;layer2&#34;</span>,layer2,step<span style="color:#f92672">=</span>step)
    tf<span style="color:#f92672">.</span>summary<span style="color:#f92672">.</span>histogram(<span style="color:#e6db74">&#34;weight3&#34;</span>,W3,step<span style="color:#f92672">=</span>step)
    tf<span style="color:#f92672">.</span>summary<span style="color:#f92672">.</span>histogram(<span style="color:#e6db74">&#34;biases3&#34;</span>,b3,step<span style="color:#f92672">=</span>step)
    tf<span style="color:#f92672">.</span>summary<span style="color:#f92672">.</span>histogram(<span style="color:#e6db74">&#34;layer3&#34;</span>,layer3,step<span style="color:#f92672">=</span>step)
    tf<span style="color:#f92672">.</span>summary<span style="color:#f92672">.</span>histogram(<span style="color:#e6db74">&#34;weights4&#34;</span>,W4,step<span style="color:#f92672">=</span>step)
    tf<span style="color:#f92672">.</span>summary<span style="color:#f92672">.</span>histogram(<span style="color:#e6db74">&#34;biases4&#34;</span>,b4, step<span style="color:#f92672">=</span>step)
    tf<span style="color:#f92672">.</span>summary<span style="color:#f92672">.</span>histogram(<span style="color:#e6db74">&#34;hypothesis&#34;</span>,hypothesis,step<span style="color:#f92672">=</span>step)
  <span style="color:#66d9ef">return</span> hypothesis

<span style="color:#75715e"># loss function</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss_fn</span>(hypothesis,labels):
  cost <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>tf<span style="color:#f92672">.</span>reduce_mean(labels <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(hypothesis) <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>labels) <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>hypothesis))
  <span style="color:#66d9ef">with</span> writer<span style="color:#f92672">.</span>as_default():
    tf<span style="color:#f92672">.</span>summary<span style="color:#f92672">.</span>scalar(<span style="color:#e6db74">&#39;loss&#39;</span>,cost,step<span style="color:#f92672">=</span>step)
  <span style="color:#66d9ef">return</span> cost

<span style="color:#75715e"># Optimizers SGD 정의 (Learning Rate 0.,1)</span>
optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>)

<span style="color:#75715e"># Accuracy Function</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">accuracy_fn</span>(hypothesis, labels):
  predicted <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(hypothesis <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0.5</span>, dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32)
  accuracy <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>cast(tf<span style="color:#f92672">.</span>equal(predicted, labels), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32))
  <span style="color:#66d9ef">return</span> accuracy

<span style="color:#75715e"># GradientTape</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad</span>(hypothesis, features, labels, step):
  <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
    loss_value <span style="color:#f92672">=</span> loss_fn(neural_net(features, step),labels)
  <span style="color:#66d9ef">return</span> tape<span style="color:#f92672">.</span>gradient(loss_value, [W1,W2,W3,W4,b1,b2,b3,b4])

<span style="color:#75715e"># Epochs 3000회</span>
EPOCHS <span style="color:#f92672">=</span> <span style="color:#ae81ff">3000</span>

<span style="color:#75715e"># Step - 3000</span>
<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(EPOCHS):
  <span style="color:#75715e"># dataset x, y</span>
  <span style="color:#66d9ef">for</span> features, labels <span style="color:#f92672">in</span> dataset:
    <span style="color:#75715e"># data preprocessing</span>
    features, labels <span style="color:#f92672">=</span> preprocess_data(features, labels)
    <span style="color:#75715e"># gradient</span>
    grads <span style="color:#f92672">=</span> grad(neural_net(features,step), features, labels, step)
    <span style="color:#75715e"># optimizer</span>
    optimizer<span style="color:#f92672">.</span>apply_gradients(grads_and_vars<span style="color:#f92672">=</span>zip(grads,[W1,W2,W3,W4,b1,b2,b3,b4]))

    <span style="color:#75715e"># verbose 50</span>
    <span style="color:#66d9ef">if</span> step <span style="color:#f92672">%</span> <span style="color:#ae81ff">50</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
      loss_value <span style="color:#f92672">=</span> loss_fn(neural_net(features, step), labels)
      <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Iter: {}, Loss: {:4f}&#34;</span><span style="color:#f92672">.</span>format(step, loss_value))

<span style="color:#75715e"># preprocessing</span>
x_data, y_data <span style="color:#f92672">=</span> preprocess_data(x_data,y_data)

<span style="color:#75715e"># accuracy</span>
test_acc <span style="color:#f92672">=</span> accuracy_fn(neural_net(x_data,step),y_data)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Testset Accuracy: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(test_acc))

<span style="color:#75715e"># Jupyter Notebook에서 Tensorboard 실행</span>
<span style="color:#75715e"># Load the TensorBoard notebook extension</span>
<span style="color:#f92672">%</span>load_ext tensorboard

<span style="color:#e6db74">&#39;&#39;&#39;Start TensorBoard through the command line or within a notebook experience.
</span><span style="color:#e6db74">The two interfaces are generally the same. In notebooks, use the %tensorboard line magic.
</span><span style="color:#e6db74">On the command line, run the same command without &#39;%&#34;. &#39;&#39;&#39;</span>

<span style="color:#f92672">%</span>tensorboard <span style="color:#f92672">--</span>logdir logs<span style="color:#f92672">/</span>xor
</code></pre></div></div>
    </div>
  </label>
</div>


<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-10-1</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># tensorflow import</span>
<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">from</span> tensorflow.keras.utils <span style="color:#f92672">import</span> to_categorical
<span style="color:#f92672">from</span> tensorflow.keras.datasets <span style="color:#f92672">import</span> mnist
<span style="color:#f92672">from</span> time <span style="color:#f92672">import</span> time
<span style="color:#f92672">import</span> os
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)

<span style="color:#75715e"># 전체 좋은 설명 : https://engineer-mole.tistory.com/m/18?category=911427</span>
<span style="color:#75715e"># Load : Check point function </span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">load</span>(model, checkpoint_dir):
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34; [*] Reading checkpoints...&#34;</span>)

    <span style="color:#75715e"># check point state를 저장</span>
    ckpt <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>get_checkpoint_state(checkpoint_dir)

    <span style="color:#75715e"># ckpt가 존재하면 </span>
    <span style="color:#66d9ef">if</span> ckpt :
        <span style="color:#75715e"># ckpt_name 가져옴</span>
        ckpt_name <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>basename(ckpt<span style="color:#f92672">.</span>model_checkpoint_path)

        <span style="color:#75715e"># checkpoint model</span>
        checkpoint <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>Checkpoint(dnn<span style="color:#f92672">=</span>model)

        <span style="color:#75715e"># save path 지정 restore</span>
        checkpoint<span style="color:#f92672">.</span>restore(save_path<span style="color:#f92672">=</span>os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(checkpoint_dir, ckpt_name))

        <span style="color:#75715e"># check point split &#39;-&#39; countrer</span>
        counter <span style="color:#f92672">=</span> int(ckpt_name<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;-&#39;</span>)[<span style="color:#ae81ff">1</span>])

        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34; [*] Success to read {}&#34;</span><span style="color:#f92672">.</span>format(ckpt_name))
        <span style="color:#66d9ef">return</span> True, counter
    <span style="color:#66d9ef">else</span>:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34; [*] Failed to find a checkpoint&#34;</span>)
        <span style="color:#66d9ef">return</span> False, <span style="color:#ae81ff">0</span>

<span style="color:#75715e"># folder check</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">check_folder</span>(dir):
    <span style="color:#75715e"># 경로가 존재하지 않을 경우 생성</span>
    <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>exists(dir):
        os<span style="color:#f92672">.</span>makedirs(dir)
    <span style="color:#66d9ef">return</span> dir

<span style="color:#75715e"># Data :Load</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">load_mnist</span>() :
    <span style="color:#75715e"># train_x, train_y, test_x, test_y : mnist data load</span>
    (train_data, train_labels), (test_data, test_labels) <span style="color:#f92672">=</span> mnist<span style="color:#f92672">.</span>load_data()
    <span style="color:#75715e"># train_data dimension 변경</span>
    train_data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>expand_dims(train_data, axis<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># [N, 28, 28] -&gt; [N, 28, 28, 1]</span>
    <span style="color:#75715e"># train_data dimension 변경</span>
    test_data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>expand_dims(test_data, axis<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># [N, 28, 28] -&gt; [N, 28, 28, 1]</span>

    <span style="color:#75715e"># train_data, test_data normalize</span>
    <span style="color:#75715e"># https://goodtogreate.tistory.com/entry/Neural-Network-%EC%A0%81%EC%9A%A9-%EC%A0%84%EC%97%90-Input-data%EB%A5%BC-Normalize-%ED%95%B4%EC%95%BC-%ED%95%98%EB%8A%94-%EC%9D%B4%EC%9C%A0</span>
    train_data, test_data <span style="color:#f92672">=</span> normalize(train_data, test_data)

    <span style="color:#75715e"># train_y, test_y 10 categorization</span>
    train_labels <span style="color:#f92672">=</span> to_categorical(train_labels, <span style="color:#ae81ff">10</span>) <span style="color:#75715e"># [N,] -&gt; [N, 10]</span>
    test_labels <span style="color:#f92672">=</span> to_categorical(test_labels, <span style="color:#ae81ff">10</span>) <span style="color:#75715e"># [N,] -&gt; [N, 10]</span>

    <span style="color:#66d9ef">return</span> train_data, train_labels, test_data, test_labels

<span style="color:#75715e"># Data pre-processing</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">normalize</span>(train_data, test_data):

    <span style="color:#75715e"># 0~255를 0~1로 정규화</span>
    train_data <span style="color:#f92672">=</span> train_data<span style="color:#f92672">.</span>astype(np<span style="color:#f92672">.</span>float32) <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.0</span>
    test_data <span style="color:#f92672">=</span> test_data<span style="color:#f92672">.</span>astype(np<span style="color:#f92672">.</span>float32) <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.0</span>

    <span style="color:#66d9ef">return</span> train_data, test_data

<span style="color:#75715e"># loss function</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss_fn</span>(model, images, labels):
    <span style="color:#75715e"># Logits 정의, Training = True 일경우 Dropout을 사용 (False는 반대)</span>
    logits <span style="color:#f92672">=</span> model(images, training<span style="color:#f92672">=</span>True)    

    <span style="color:#75715e"># loss = average ( cross_entropy( logits, labels ))</span>
    <span style="color:#75715e"># Category 분류의 문제 : categorical crossentropy</span>
    <span style="color:#75715e"># Output이 logit 상태가 아님으로 from_logits= True로 설정 </span>
    <span style="color:#75715e"># https://hwiyong.tistory.com/335 (softmax를 거치느냐 안거치느냐로 설명중)</span>
    loss <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>losses<span style="color:#f92672">.</span>categorical_crossentropy(y_pred<span style="color:#f92672">=</span>logits, y_true<span style="color:#f92672">=</span>labels, from_logits<span style="color:#f92672">=</span>True))
    <span style="color:#66d9ef">return</span> loss

<span style="color:#75715e"># Accuracy Function</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">accuracy_fn</span>(model, images, labels):
    <span style="color:#75715e"># Logits 정의</span>
    logits <span style="color:#f92672">=</span> model(images, training<span style="color:#f92672">=</span>False)

    <span style="color:#75715e"># Logits와 Label의 Equal 여부를 Prediction으로 정의</span>
    prediction <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>equal(tf<span style="color:#f92672">.</span>argmax(logits, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>), tf<span style="color:#f92672">.</span>argmax(labels, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>))

    <span style="color:#75715e"># Prediction Boolean을 float으로 전환후 accuracy return</span>
    accuracy <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>cast(prediction, tf<span style="color:#f92672">.</span>float32))
    <span style="color:#66d9ef">return</span> accuracy

<span style="color:#75715e"># Gradient Tape</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad</span>(model, images, labels):
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
        <span style="color:#75715e"># Loss Function 정의</span>
        loss <span style="color:#f92672">=</span> loss_fn(model, images, labels)
    <span style="color:#66d9ef">return</span> tape<span style="color:#f92672">.</span>gradient(loss, model<span style="color:#f92672">.</span>variables)

<span style="color:#75715e"># Model Function</span>
<span style="color:#75715e"># Layer Flatten 처리</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">flatten</span>() :
    <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Flatten()

<span style="color:#75715e"># Layer의 Dense 설정</span>
<span style="color:#75715e"># units는 Output 채널 개수를 설정</span>
<span style="color:#75715e"># use_bias는 bias 사용 여부 설정</span>
<span style="color:#75715e"># kernel initializer는 weight 초기화</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">dense</span>(label_dim, weight_init) :
    <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(units<span style="color:#f92672">=</span>label_dim, use_bias<span style="color:#f92672">=</span>True, kernel_initializer<span style="color:#f92672">=</span>weight_init)

<span style="color:#75715e"># Layer Activation Sigmoid</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sigmoid</span>() :
    <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Activation(tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>activations<span style="color:#f92672">.</span>sigmoid)

<span style="color:#75715e"># Layer Activation Relu</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">relu</span>() :
    <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Activation(tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>activations<span style="color:#f92672">.</span>relu)

<span style="color:#75715e"># overfitting 방지를 위해 0~1사이의 rate로 dropout 사용을 설정</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">dropout</span>(rate) : 
    <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dropout(rate)

<span style="color:#75715e"># overfitting 방지를 위해 Batch Normalization을 사용하는 경우 정의</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">batch_norm</span>() : 
    <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>BatchNormalization()

<span style="color:#75715e"># model class 생성</span>
<span style="color:#75715e"># keras Model 상속 필요</span>
<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">create_model_class</span>(tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Model):
    <span style="color:#75715e"># logits의 최종 아웃풋 개수 label_dim = 10</span>
    <span style="color:#66d9ef">def</span> __init__(self, label_dim):
        <span style="color:#75715e"># class 모델  초기화</span>
        super(create_model_class, self)<span style="color:#f92672">.</span>__init__()

        <span style="color:#75715e"># Weight 초기화 : RandomNormal() 은 평균이 0 분산이 1인 가우시안 분포로 랜덤한 수를 생성하여 Weght로 설정</span>
        <span style="color:#75715e"># 10-1-1, 10-1-2</span>
        <span style="color:#75715e"># weight_init = tf.keras.initializers.RandomNormal()</span>

        <span style="color:#75715e"># 10-2-1 xavier</span>
        <span style="color:#75715e"># https://flonelin.wordpress.com/2018/01/28/weight-initalizer-%EC%A2%85%EB%A5%98/</span>
        <span style="color:#75715e"># Relu : 0이하의 신호를 제거</span>
        <span style="color:#75715e"># Glorot : input / output neuron 수에 기반 초기화의 스케일을 결정</span>
        <span style="color:#75715e"># He : Relu가 0 이하의 신호를 제거함으로 분산을 두배 주어 분산을 유지</span>
        <span style="color:#75715e"># Orthogonal : SVD(Singular Value Decomposition)으로 가중치 행렬 각행이 모두 수직으로 만듬</span>
        weight_init <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>initializers<span style="color:#f92672">.</span>glorot_uniform()

        <span style="color:#75715e"># 리스트 자료구조 타입</span>
        self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Sequential()

        <span style="color:#75715e"># Convolution을 이용할 경우 Flatten과정이 필요치 않음</span>
        self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>add(flatten())

        <span style="color:#75715e"># [N,784] -&gt; [N,256] -&gt; [N,256]</span>
        <span style="color:#75715e"># 10-1-1, 10-1-2, 10-2-1</span>
        <span style="color:#75715e"># for i in range(2):</span>
        <span style="color:#75715e"># 10-2-2 Xavier Deep</span>
        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">4</span>) :
            <span style="color:#75715e"># 채널을 256으로 하고 Sigmoid를 쓸수있게 바꿈</span>
            <span style="color:#75715e"># 10-1-1, 10-1-2, 10-2-1</span>
            <span style="color:#75715e"># self.model.add(dense(256, weight_init))            </span>
            <span style="color:#75715e"># 10-2-2</span>
            self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>add(dense(<span style="color:#ae81ff">512</span>, weight_init))

            <span style="color:#75715e"># 10-1-1 sigmoid 사용시</span>
            <span style="color:#75715e"># self.model.add(sigmoid())</span>

            <span style="color:#75715e"># 10-1-2 relu 사용시</span>
            <span style="color:#75715e"># self.model.add(relu())</span>

            <span style="color:#75715e"># 10-3 dropout 설정</span>
            <span style="color:#75715e"># self.model.add(dropout(rate=0.5))</span>

            <span style="color:#75715e"># 10-4 batch normalization 사용</span>
            <span style="color:#75715e"># drop out 사용시와 순서가 다름</span>
            self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>add(batch_norm())
            self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>add(relu())
            
        <span style="color:#75715e"># logit을 구할때 10개 output이 출력됨</span>
        self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>add(dense(label_dim, weight_init))

    <span style="color:#75715e"># call 함수를 통해 아웃풀 출력을 정의</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">call</span>(self, x, training<span style="color:#f92672">=</span>None, mask<span style="color:#f92672">=</span>None):
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>model(x)
        <span style="color:#66d9ef">return</span> x

<span style="color:#75715e"># Function 형태 Create Model</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_model_function</span>(label_dim) :
    <span style="color:#75715e"># Weight 초기화</span>
    <span style="color:#75715e"># 10-1-1, 10-1-2</span>
    <span style="color:#75715e"># weight_init = tf.keras.initializers.RandomNormal()</span>

    <span style="color:#75715e"># 10-2-1 xavier</span>
    weight_init <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>initializers<span style="color:#f92672">.</span>glorot_uniform()

    model <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Sequential()
    model<span style="color:#f92672">.</span>add(flatten())

    <span style="color:#75715e"># 10-1-1, 10-1-2, 10-2-1</span>
    <span style="color:#75715e"># for i in range(2) :</span>
    <span style="color:#75715e"># 10-2-2 Xavier Deep</span>
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">4</span>) :
        <span style="color:#75715e"># 10-1-1, 10-1-2, 10-2-1</span>
        <span style="color:#75715e"># model.add(dense(256, weight_init))</span>
        <span style="color:#75715e"># 10-2-2</span>
        model<span style="color:#f92672">.</span>add(dense(<span style="color:#ae81ff">512</span>,weight_init))

        <span style="color:#75715e"># 10-1-1 sigmoid 사용시</span>
        <span style="color:#75715e"># model.add(sigmoid())</span>
        
        <span style="color:#75715e"># 10-1-2 relu 사용시</span>
        <span style="color:#75715e"># model.add(relu())</span>
        
        <span style="color:#75715e"># 10-3</span>
        <span style="color:#75715e"># model.add(dropout(rate=0.5))</span>

        <span style="color:#75715e"># 10-4 Batch Normalization</span>
        model<span style="color:#f92672">.</span>add(batch_norm())
        model<span style="color:#f92672">.</span>add(relu())

    model<span style="color:#f92672">.</span>add(dense(label_dim, weight_init))

    <span style="color:#66d9ef">return</span> model

<span style="color:#75715e"># Define data &amp; hyper-parameter</span>
<span style="color:#75715e"># dataset load</span>
<span style="color:#e6db74">&#34;&#34;&#34; dataset &#34;&#34;&#34;</span>
train_x, train_y, test_x, test_y <span style="color:#f92672">=</span> load_mnist()

<span style="color:#75715e"># parameters 설정</span>
<span style="color:#e6db74">&#34;&#34;&#34; parameters &#34;&#34;&#34;</span>
learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.001</span>
batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>
training_epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>

<span style="color:#75715e"># 60000 / 128 = 468</span>
training_iterations <span style="color:#f92672">=</span> len(train_x) <span style="color:#f92672">//</span> batch_size

label_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>

train_flag <span style="color:#f92672">=</span> True

<span style="color:#75715e"># data를 batch_size로 학습</span>
<span style="color:#75715e"># Shuffle : buffer_size는 input data 보다 커야함</span>
<span style="color:#75715e"># prefetch는 네트워크가 설정한 batch_size만큼 학습시 메모리에 batch_size만큼 적재</span>
<span style="color:#75715e"># batch_size 설정</span>
<span style="color:#75715e"># .\ 이후 space에 에러 출력됨</span>
<span style="color:#e6db74">&#34;&#34;&#34; Graph Input using Dataset API &#34;&#34;&#34;</span>
train_dataset <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices((train_x, train_y))<span style="color:#f92672">.</span>\
    shuffle(buffer_size<span style="color:#f92672">=</span><span style="color:#ae81ff">100000</span>)<span style="color:#f92672">.</span>\
    prefetch(buffer_size<span style="color:#f92672">=</span>batch_size)<span style="color:#f92672">.</span>\
    batch(batch_size, drop_remainder<span style="color:#f92672">=</span>True)

test_dataset <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices((test_x, test_y))<span style="color:#f92672">.</span>\
    shuffle(buffer_size<span style="color:#f92672">=</span><span style="color:#ae81ff">100000</span>)<span style="color:#f92672">.</span>\
    prefetch(buffer_size<span style="color:#f92672">=</span>len(test_x))<span style="color:#f92672">.</span>\
    batch(len(test_x))

<span style="color:#75715e"># 모델 / 옵티마이저 / 롸이터 정의</span>
<span style="color:#e6db74">&#34;&#34;&#34; Model &#34;&#34;&#34;</span>
network <span style="color:#f92672">=</span> create_model_function(label_dim)

<span style="color:#75715e"># Adam Optimizer</span>
<span style="color:#e6db74">&#34;&#34;&#34; Training &#34;&#34;&#34;</span>
optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>Adam(learning_rate<span style="color:#f92672">=</span>learning_rate)

<span style="color:#75715e"># Writer 정의</span>
<span style="color:#75715e"># 10-1-1 sigmoid 사용시</span>
<span style="color:#75715e"># model_dir = &#39;nn_softmax&#39;</span>
<span style="color:#75715e"># 10-1-2 relu 사용시</span>
<span style="color:#75715e"># model_dir = &#39;nn_relu&#39;</span>
<span style="color:#75715e"># 10-2-1 xavier 사용시</span>
<span style="color:#75715e"># model_dir = &#39;nn_xavier&#39;</span>
<span style="color:#75715e"># 10-2-2 xavier deep</span>
<span style="color:#75715e"># model_dir = &#39;nn_deep&#39;</span>
<span style="color:#75715e"># 10-3 relu dropout</span>
<span style="color:#75715e"># model_dir = &#39;nn_dropout&#39;</span>
<span style="color:#75715e"># 10-4 Batch Normalization</span>

<span style="color:#e6db74">&#34;&#34;&#34; Writer &#34;&#34;&#34;</span>
checkpoint_dir <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;checkpoints&#39;</span>
logs_dir <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;logs&#39;</span>

model_dir <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;nn_batchnorm&#39;</span>

checkpoint_dir <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(checkpoint_dir, model_dir)
check_folder(checkpoint_dir)
checkpoint_prefix <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(checkpoint_dir, model_dir)
logs_dir <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(logs_dir, model_dir)

<span style="color:#75715e"># Train / Test 여부에 따라 분기</span>
<span style="color:#66d9ef">if</span> train_flag :
    <span style="color:#75715e"># 체크포인트는 재학습시 변경되었던 Weight를 복원함</span>
    checkpoint <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>Checkpoint(dnn<span style="color:#f92672">=</span>network)

    <span style="color:#75715e"># 텐저보드를 위한 롸이터 생성</span>
    <span style="color:#75715e"># create writer for tensorboard</span>
    summary_writer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>summary<span style="color:#f92672">.</span>create_file_writer(logdir<span style="color:#f92672">=</span>logs_dir)

    <span style="color:#75715e"># 시작 시간</span>
    start_time <span style="color:#f92672">=</span> time()

    <span style="color:#75715e"># 체크포인트 존재시 Resotre</span>
    <span style="color:#75715e"># restore check-point if it exits</span>
    could_load, checkpoint_counter <span style="color:#f92672">=</span> load(network, checkpoint_dir)    

    <span style="color:#75715e"># Check point 존재시 해당 값으로 설정</span>
    <span style="color:#66d9ef">if</span> could_load:
        <span style="color:#75715e"># start_epoch = (int)(checkpoint_counter / training_iterations)        </span>
        <span style="color:#75715e"># counter = checkpoint_counter    </span>
        start_epoch <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
        start_iteration <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
        counter <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34; [*] Load SUCCESS&#34;</span>)
    <span style="color:#66d9ef">else</span>:
        start_epoch <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
        start_iteration <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
        counter <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34; [!] Load failed...&#34;</span>)
    
    <span style="color:#75715e"># train phase</span>
    <span style="color:#66d9ef">with</span> summary_writer<span style="color:#f92672">.</span>as_default():  <span style="color:#75715e"># for tensorboard</span>
        <span style="color:#75715e"># epoch와 iteration에 대한 중첩 for 문 수행</span>
        <span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(start_epoch, training_epochs):
            <span style="color:#66d9ef">for</span> idx, (train_input, train_label) <span style="color:#f92672">in</span> enumerate(train_dataset):            
                grads <span style="color:#f92672">=</span> grad(network, train_input, train_label)
                <span style="color:#75715e"># optimizer.apply_gradients를 호출하여 gradient를 적용하여 네트워크를 학습</span>
                optimizer<span style="color:#f92672">.</span>apply_gradients(grads_and_vars<span style="color:#f92672">=</span>zip(grads, network<span style="color:#f92672">.</span>variables))

                <span style="color:#75715e"># Loss Function</span>
                train_loss <span style="color:#f92672">=</span> loss_fn(network, train_input, train_label)

                <span style="color:#75715e"># Accuracy Function</span>
                train_accuracy <span style="color:#f92672">=</span> accuracy_fn(network, train_input, train_label)
                
                <span style="color:#75715e"># Test Dataset</span>
                <span style="color:#66d9ef">for</span> test_input, test_label <span style="color:#f92672">in</span> test_dataset:                
                    test_accuracy <span style="color:#f92672">=</span> accuracy_fn(network, test_input, test_label)

                <span style="color:#75715e"># Summary Scalar형 텐서 사용</span>
                tf<span style="color:#f92672">.</span>summary<span style="color:#f92672">.</span>scalar(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;train_loss&#39;</span>, data<span style="color:#f92672">=</span>train_loss, step<span style="color:#f92672">=</span>counter)
                tf<span style="color:#f92672">.</span>summary<span style="color:#f92672">.</span>scalar(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;train_accuracy&#39;</span>, data<span style="color:#f92672">=</span>train_accuracy, step<span style="color:#f92672">=</span>counter)
                tf<span style="color:#f92672">.</span>summary<span style="color:#f92672">.</span>scalar(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;test_accuracy&#39;</span>, data<span style="color:#f92672">=</span>test_accuracy, step<span style="color:#f92672">=</span>counter)

                <span style="color:#66d9ef">print</span>(
                    <span style="color:#e6db74">&#34;Epoch: [</span><span style="color:#e6db74">%2d</span><span style="color:#e6db74">] [</span><span style="color:#e6db74">%5d</span><span style="color:#e6db74">/</span><span style="color:#e6db74">%5d</span><span style="color:#e6db74">] time: </span><span style="color:#e6db74">%4.2f</span><span style="color:#e6db74">, tr_loss: </span><span style="color:#e6db74">%.4f</span><span style="color:#e6db74">, tr_acc: </span><span style="color:#e6db74">%.4f</span><span style="color:#e6db74">, ts_acc: </span><span style="color:#e6db74">%.4f</span><span style="color:#e6db74">&#34;</span> \
                    <span style="color:#f92672">%</span> (epoch, idx, training_iterations, time() <span style="color:#f92672">-</span> start_time, train_loss, train_accuracy,
                       test_accuracy))
                counter <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>             
        <span style="color:#75715e"># 체크 포인트 저장</span>
        checkpoint<span style="color:#f92672">.</span>save(file_prefix<span style="color:#f92672">=</span>checkpoint_prefix <span style="color:#f92672">+</span> <span style="color:#e6db74">&#39;-{}&#39;</span><span style="color:#f92672">.</span>format(counter))
        
<span style="color:#75715e"># test phase      </span>
<span style="color:#66d9ef">else</span> :
    _, _ <span style="color:#f92672">=</span> load(network, checkpoint_dir)
    <span style="color:#66d9ef">for</span> test_input, test_label <span style="color:#f92672">in</span> test_dataset:    
        test_accuracy <span style="color:#f92672">=</span> accuracy_fn(network, test_input, test_label)

    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;test_Accuracy: </span><span style="color:#e6db74">%.4f</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> (test_accuracy))
</code></pre></div></div>
    </div>
  </label>
</div>


<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-11-0</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> __future__ <span style="color:#f92672">import</span> absolute_import
<span style="color:#f92672">from</span> __future__ <span style="color:#f92672">import</span> division
<span style="color:#f92672">from</span> __future__ <span style="color:#f92672">import</span> print_function

<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf
<span style="color:#f92672">from</span> tensorflow <span style="color:#f92672">import</span> keras
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt

<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)
<span style="color:#66d9ef">print</span>(keras<span style="color:#f92672">.</span>__version__)

<span style="color:#75715e"># 1X3X3X1 shape</span>
image <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>constant([ [ [ [<span style="color:#ae81ff">1</span>],[<span style="color:#ae81ff">2</span>],[<span style="color:#ae81ff">3</span>] ],[ [<span style="color:#ae81ff">4</span>],[<span style="color:#ae81ff">5</span>],[<span style="color:#ae81ff">6</span>] ], [ [<span style="color:#ae81ff">7</span>],[<span style="color:#ae81ff">8</span>],[<span style="color:#ae81ff">9</span>] ] ] ], dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32)
<span style="color:#66d9ef">print</span>(image<span style="color:#f92672">.</span>shape)

<span style="color:#75715e"># Grey Shape</span>
plt<span style="color:#f92672">.</span>imshow(image<span style="color:#f92672">.</span>numpy()<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">3</span>), cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Greys&#39;</span>)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># 12,16,24,28 shape -&gt; plt.show (check)</span>
check <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>constant([ [ [ [<span style="color:#ae81ff">12.</span>] ],[ [<span style="color:#ae81ff">16.</span>] ] ],[ [ [<span style="color:#ae81ff">24.</span>] ],[ [<span style="color:#ae81ff">28.</span>] ] ] ], dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32)
plt<span style="color:#f92672">.</span>imshow(check<span style="color:#f92672">.</span>numpy()<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>), cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Greys&#39;</span>)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># Image : 1,3,3,1  Filter : 2,2,1,1  Stride : 1X1  Padding : Valid</span>
<span style="color:#75715e"># 1  2  3    1  1    12  16</span>
<span style="color:#75715e"># 4  5  6  + 1  1 -&gt; 24  28</span>
<span style="color:#75715e"># 7  8  9 </span>
<span style="color:#75715e"># 1 filter (22,11) with padding : valid</span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;image.shape&#34;</span>, image<span style="color:#f92672">.</span>shape)

<span style="color:#75715e"># Weight 설정</span>
weight <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([ [ [ [<span style="color:#ae81ff">1.</span>] ],[ [<span style="color:#ae81ff">1.</span>] ] ],[ [ [<span style="color:#ae81ff">1.</span>] ],[ [<span style="color:#ae81ff">1.</span>] ] ] ])
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;weight.shape&#34;</span>, weight<span style="color:#f92672">.</span>shape)

weight_init <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>constant_initializer(weight)
conv2d <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Conv2D(filters<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;VALID&#39;</span>, kernel_initializer<span style="color:#f92672">=</span>weight_init)(image)

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;conv2d.shape&#34;</span>, conv2d<span style="color:#f92672">.</span>shape)
<span style="color:#66d9ef">print</span>(conv2d<span style="color:#f92672">.</span>numpy()<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>))

plt<span style="color:#f92672">.</span>imshow(conv2d<span style="color:#f92672">.</span>numpy()<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>), cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray&#39;</span>)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># Image : 1,3,3,1  Filter : 2,2,1,1  Stride : 1X1  Padding : SAME (3X3)</span>
<span style="color:#75715e"># 1  2  3  0      1  1      12  16   9</span>
<span style="color:#75715e"># 4  5  6  0  -&gt;  1  1  -&gt;  24  28  15</span>
<span style="color:#75715e"># 7  8  9  0                15  17   9</span>
<span style="color:#75715e"># 0  0  0  0 </span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;image.shape&#34;</span>, image<span style="color:#f92672">.</span>shape)

weight <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[[[<span style="color:#ae81ff">1.</span>]],[[<span style="color:#ae81ff">1.</span>]]],[[[<span style="color:#ae81ff">1.</span>]],[[<span style="color:#ae81ff">1.</span>]]]])
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;weight.shape&#34;</span>, weight<span style="color:#f92672">.</span>shape)

weight_init <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>constant_initializer(weight)
conv2d <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Conv2D(filters<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;SAME&#39;</span>, kernel_initializer<span style="color:#f92672">=</span>weight_init)(image)

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;conv2d.shape&#34;</span>, conv2d<span style="color:#f92672">.</span>shape)
<span style="color:#66d9ef">print</span>(conv2d<span style="color:#f92672">.</span>numpy()<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">3</span>))

plt<span style="color:#f92672">.</span>imshow(conv2d<span style="color:#f92672">.</span>numpy()<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">3</span>), cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray&#39;</span>)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># image shape 1,3,3,1</span>
<span style="color:#75715e"># print(&#34;imag:\n&#34;, image)</span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;image.shape&#34;</span>, image<span style="color:#f92672">.</span>shape)

<span style="color:#75715e"># wieght shape 2,2,1,3</span>
weight <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([ [ [ [<span style="color:#ae81ff">1.</span>,<span style="color:#ae81ff">10.</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span>] ] ,
                      [ [<span style="color:#ae81ff">1.</span>,<span style="color:#ae81ff">10.</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span>] ] ] , 
                    [ [ [<span style="color:#ae81ff">1.</span>,<span style="color:#ae81ff">10.</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span>] ],
                      [ [<span style="color:#ae81ff">1.</span>,<span style="color:#ae81ff">10.</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span>] ] ] ])
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;weight.shape&#34;</span>, weight<span style="color:#f92672">.</span>shape)
weight_init <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>constant_initializer(weight)

<span style="color:#75715e">################################################################################</span>
<span style="color:#75715e"># Image : 1,3,3,1  Filter : 2,2,1,(1)  Stride : 1X1  Padding : SAME (3X3)</span>
<span style="color:#75715e"># 1  2  3        1   1      12  16   9</span>
<span style="color:#75715e"># 4  5  6   -&gt;   1   1  -&gt;  24  28  15</span>
<span style="color:#75715e"># 7  8  9                   15  17   9</span>
<span style="color:#75715e">################################################################################</span>
<span style="color:#75715e"># Image : 1,3,3,1  Filter : 2,2,1,(2)  Stride : 1X1  Padding : SAME (3X3)</span>
<span style="color:#75715e"># 1  2  3       10  10      120 160  90</span>
<span style="color:#75715e"># 4  5  6   -&gt;  10  10  -&gt;  240 280 150</span>
<span style="color:#75715e"># 7  8  9                   150 170  90</span>
<span style="color:#75715e">################################################################################</span>
<span style="color:#75715e"># Image : 1,3,3,1  Filter : 2,2,1,(3)  Stride : 1X1  Padding : SAME (3X3)</span>
<span style="color:#75715e"># 1  2  3       -1  -1      -12 -16  -9</span>
<span style="color:#75715e"># 4  5  6   -&gt;  -1  -1  -&gt;  -24 -28 -15</span>
<span style="color:#75715e"># 7  8  9                   -15 -17  -9</span>
conv2d <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Conv2D(filters<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;SAME&#39;</span>, kernel_initializer<span style="color:#f92672">=</span>weight_init)(image)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;conv2d.shape&#34;</span>, conv2d<span style="color:#f92672">.</span>shape)

<span style="color:#75715e"># 행/열 교체 : 0 -&gt; 4차원 / 3 -&gt; 1차원 </span>
feature_maps <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>swapaxes(conv2d, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">3</span>)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;feature_maps&#34;</span>, feature_maps<span style="color:#f92672">.</span>shape)

<span style="color:#75715e"># swapaxes / reshape / tranpose </span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;weight&#34;</span>, weight<span style="color:#f92672">.</span>shape)
weight_swap <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>swapaxes(weight, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>) <span style="color:#75715e"># 2,2,1,3 -&gt; 2,2,3,1</span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;weight_swap1&#34;</span>, weight_swap<span style="color:#f92672">.</span>shape)
weight_swap <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>swapaxes(weight, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">3</span>) <span style="color:#75715e"># 2,2,3,1 -&gt; 1,2,3,2</span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;weight_swap2&#34;</span>, weight_swap<span style="color:#f92672">.</span>shape)
weight_reshape <span style="color:#f92672">=</span> weight<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">3</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># 2,2,1,3 -&gt; 1,2,3,2</span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;weight_reshape1&#34;</span>, weight_reshape<span style="color:#f92672">.</span>shape)
weight_reshape <span style="color:#f92672">=</span> weight<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># 2,2,1,3 -&gt; 1,2,3,2</span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;weight_reshape2&#34;</span>, weight_reshape<span style="color:#f92672">.</span>shape)
weight_trans <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>transpose(weight)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;weight_trans&#34;</span>, weight_trans<span style="color:#f92672">.</span>shape)

<span style="color:#66d9ef">for</span> i, feature_map <span style="color:#f92672">in</span> enumerate(feature_maps):
    <span style="color:#66d9ef">print</span>(feature_map<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">3</span>))
    plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">3</span>,i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>), plt<span style="color:#f92672">.</span>imshow(feature_map<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">3</span>), cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray&#39;</span>)
plt<span style="color:#f92672">.</span>show()

image <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>constant([ [ [ [<span style="color:#ae81ff">1</span>], [<span style="color:#ae81ff">1</span>], [<span style="color:#ae81ff">2</span>], [<span style="color:#ae81ff">4</span>] ],
                        [ [<span style="color:#ae81ff">5</span>], [<span style="color:#ae81ff">6</span>], [<span style="color:#ae81ff">7</span>], [<span style="color:#ae81ff">8</span>] ],
                        [ [<span style="color:#ae81ff">3</span>], [<span style="color:#ae81ff">2</span>], [<span style="color:#ae81ff">1</span>], [<span style="color:#ae81ff">0</span>] ],
                        [ [<span style="color:#ae81ff">1</span>], [<span style="color:#ae81ff">2</span>], [<span style="color:#ae81ff">3</span>], [<span style="color:#ae81ff">4</span>] ]  ] ], dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32)
<span style="color:#66d9ef">print</span>(image<span style="color:#f92672">.</span>shape)

<span style="color:#75715e">#  1  1  2  4      </span>
<span style="color:#75715e">#  5  6  7  8  -&gt;   6   7   8</span>
<span style="color:#75715e">#  3  2  1  0       6   7   8</span>
<span style="color:#75715e">#  1  2  3  4       3   3   4</span>
pool <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>MaxPool2D(pool_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>), strides<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;VALID&#39;</span>)(image)
<span style="color:#66d9ef">print</span>(pool<span style="color:#f92672">.</span>shape)
<span style="color:#66d9ef">print</span>(pool<span style="color:#f92672">.</span>numpy())

<span style="color:#75715e">#  1  1  2  4      </span>
<span style="color:#75715e">#  5  6  7  8  -&gt;   6   8</span>
<span style="color:#75715e">#  3  2  1  0       3   4</span>
<span style="color:#75715e">#  1  2  3  4       </span>
pool <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>MaxPool2D(pool_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>), strides<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;VALID&#39;</span>)(image)
<span style="color:#66d9ef">print</span>(pool<span style="color:#f92672">.</span>shape)
<span style="color:#66d9ef">print</span>(pool<span style="color:#f92672">.</span>numpy())

<span style="color:#75715e"># image : 1X2X2X1</span>
<span style="color:#75715e">#  4  3  -&gt; 4</span>
<span style="color:#75715e">#  2  1</span>
image <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>constant([ [ [ [<span style="color:#ae81ff">4</span>], [<span style="color:#ae81ff">3</span>] ],
                          [ [<span style="color:#ae81ff">2</span>],[<span style="color:#ae81ff">1</span>] ] ] ], dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32)
<span style="color:#66d9ef">print</span>(image<span style="color:#f92672">.</span>shape)

pool <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>MaxPool2D(pool_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>), strides<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;VALID&#39;</span>)(image)
<span style="color:#66d9ef">print</span>(pool<span style="color:#f92672">.</span>shape)
<span style="color:#66d9ef">print</span>(pool<span style="color:#f92672">.</span>numpy())

<span style="color:#75715e"># image : 1X2X2X1</span>
<span style="color:#75715e">#  4  3  0  -&gt;  4  3</span>
<span style="color:#75715e">#  2  1  0      2  1</span>
<span style="color:#75715e">#  0  0  0</span>
image <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>constant([[[[<span style="color:#ae81ff">4</span>],[<span style="color:#ae81ff">3</span>]],[[<span style="color:#ae81ff">2</span>],[<span style="color:#ae81ff">1</span>]]]], dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32)
pool <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>MaxPool2D(pool_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>), strides<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;SAME&#39;</span>)(image)
<span style="color:#66d9ef">print</span>(pool<span style="color:#f92672">.</span>shape)
<span style="color:#66d9ef">print</span>(pool<span style="color:#f92672">.</span>numpy())

<span style="color:#75715e"># header</span>
mnist <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>datasets<span style="color:#f92672">.</span>mnist
class_names <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;0&#39;</span>, <span style="color:#e6db74">&#39;1&#39;</span>, <span style="color:#e6db74">&#39;2&#39;</span>, <span style="color:#e6db74">&#39;3&#39;</span>, <span style="color:#e6db74">&#39;4&#39;</span>, <span style="color:#e6db74">&#39;5&#39;</span>, <span style="color:#e6db74">&#39;6&#39;</span>, <span style="color:#e6db74">&#39;7&#39;</span>, <span style="color:#e6db74">&#39;8&#39;</span>, <span style="color:#e6db74">&#39;9&#39;</span>]

<span style="color:#75715e">#mnist = keras.datasets.fashion_mnist</span>
<span style="color:#75715e">#class_names = [&#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;]</span>

<span style="color:#75715e"># load data</span>
(train_images, train_labels), (test_images, test_labels) <span style="color:#f92672">=</span> mnist<span style="color:#f92672">.</span>load_data()

<span style="color:#75715e"># nomalizaiton</span>
train_images <span style="color:#f92672">=</span> train_images<span style="color:#f92672">.</span>astype(np<span style="color:#f92672">.</span>float32) <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.</span>
test_images <span style="color:#f92672">=</span> test_images<span style="color:#f92672">.</span>astype(np<span style="color:#f92672">.</span>float32) <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.</span>

<span style="color:#75715e"># print image 0</span>
img <span style="color:#f92672">=</span> train_images[<span style="color:#ae81ff">0</span>]
plt<span style="color:#f92672">.</span>imshow(img, cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray&#39;</span>)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># img redefine</span>
img <span style="color:#f92672">=</span> train_images[<span style="color:#ae81ff">0</span>]

<span style="color:#75715e"># img reshape</span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Before reshape&#34;</span>, img<span style="color:#f92672">.</span>shape)
img <span style="color:#f92672">=</span> img<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">28</span>,<span style="color:#ae81ff">28</span>,<span style="color:#ae81ff">1</span>)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;After reshape&#34;</span>, img<span style="color:#f92672">.</span>shape)

<span style="color:#75715e"># image를 tensor로 정의</span>
<span style="color:#75715e"># 벡터 축에 대한 행렬을 일반화</span>
img <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>convert_to_tensor(img)

<span style="color:#75715e"># 변형) Random 대신 지정</span>
<span style="color:#75715e"># 초기 Weight Random 저장</span>
<span style="color:#75715e"># weight_init = keras.initializers.RandomNormal(stddev=0.01)</span>

<span style="color:#75715e"># wieght shape 3,3,1,5</span>
weight <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([ [ [ [ <span style="color:#ae81ff">1.</span>,<span style="color:#ae81ff">0.01</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span>,<span style="color:#ae81ff">0.</span>,<span style="color:#ae81ff">10.</span>] ], [ [ <span style="color:#ae81ff">1.</span>,<span style="color:#ae81ff">0.01</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span>,<span style="color:#ae81ff">0.</span>,<span style="color:#ae81ff">10.</span>] ], [ [ <span style="color:#ae81ff">1.</span>,<span style="color:#ae81ff">0.01</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span>,<span style="color:#ae81ff">0.</span>,<span style="color:#ae81ff">10.</span>] ] ],
                    [ [ [ <span style="color:#ae81ff">1.</span>,<span style="color:#ae81ff">0.01</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span>,<span style="color:#ae81ff">0.</span>,<span style="color:#ae81ff">10.</span>] ], [ [ <span style="color:#ae81ff">1.</span>,<span style="color:#ae81ff">0.01</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span>,<span style="color:#ae81ff">0.</span>,<span style="color:#ae81ff">10.</span>] ], [ [ <span style="color:#ae81ff">1.</span>,<span style="color:#ae81ff">0.01</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span>,<span style="color:#ae81ff">0.</span>,<span style="color:#ae81ff">10.</span>] ] ],
                    [ [ [ <span style="color:#ae81ff">1.</span>,<span style="color:#ae81ff">0.01</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span>,<span style="color:#ae81ff">0.</span>,<span style="color:#ae81ff">10.</span>] ], [ [ <span style="color:#ae81ff">1.</span>,<span style="color:#ae81ff">0.01</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span>,<span style="color:#ae81ff">0.</span>,<span style="color:#ae81ff">10.</span>] ], [ [ <span style="color:#ae81ff">1.</span>,<span style="color:#ae81ff">0.01</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span>,<span style="color:#ae81ff">0.</span>,<span style="color:#ae81ff">10.</span>] ] ] ])
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;weight.shape&#34;</span>, weight<span style="color:#f92672">.</span>shape)
weight_init <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>constant_initializer(weight)

<span style="color:#75715e"># 1X28X28X1 -&gt; 2X2 -&gt; 1X14X14X5</span>
conv2d <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Conv2D(filters<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, strides<span style="color:#f92672">=</span>(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>), padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;SAME&#39;</span>, kernel_initializer<span style="color:#f92672">=</span>weight_init)(img)
<span style="color:#66d9ef">print</span>(conv2d<span style="color:#f92672">.</span>shape)

<span style="color:#75715e"># 4차원 - 1차원 Swap</span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Before swapaxes : &#34;</span>,conv2d<span style="color:#f92672">.</span>shape)
feature_maps <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>swapaxes(conv2d, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">3</span>)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;After swapaxes : &#34;</span>,feature_maps<span style="color:#f92672">.</span>shape)

<span style="color:#75715e"># Filter 수만큼 For loop</span>
<span style="color:#66d9ef">for</span> i, feature_map <span style="color:#f92672">in</span> enumerate(feature_maps):
    plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">5</span>,i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>), plt<span style="color:#f92672">.</span>imshow(feature_map<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">14</span>,<span style="color:#ae81ff">14</span>), cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray&#39;</span>)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># maxpool 수행</span>
pool <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>MaxPool2D(pool_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>), strides<span style="color:#f92672">=</span>(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>), padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;SAME&#39;</span>)(conv2d)
<span style="color:#66d9ef">print</span>(pool<span style="color:#f92672">.</span>shape)

<span style="color:#75715e"># swap 축</span>
feature_maps <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>swapaxes(pool, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">3</span>)

<span style="color:#75715e"># filter 만큼 for loop</span>
<span style="color:#66d9ef">for</span> i, feature_map <span style="color:#f92672">in</span> enumerate(feature_maps):
    plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">5</span>,i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>), plt<span style="color:#f92672">.</span>imshow(feature_map<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">7</span>), cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray&#39;</span>)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># img redefine</span>
img <span style="color:#f92672">=</span> train_images[<span style="color:#ae81ff">0</span>]

<span style="color:#75715e"># img reshape</span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Before reshape&#34;</span>, img<span style="color:#f92672">.</span>shape)
img <span style="color:#f92672">=</span> img<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">28</span>,<span style="color:#ae81ff">28</span>,<span style="color:#ae81ff">1</span>)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;After reshape&#34;</span>, img<span style="color:#f92672">.</span>shape)

<span style="color:#75715e"># image를 tensor로 정의</span>
<span style="color:#75715e"># 벡터 축에 대한 행렬을 일반화</span>
img <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>convert_to_tensor(img)

<span style="color:#75715e"># 초기 Weight Random 저장</span>
weight_init <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>initializers<span style="color:#f92672">.</span>RandomNormal(stddev<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>)

<span style="color:#75715e"># 1X28X28X1 -&gt; 2X2 -&gt; 1X14X14X5</span>
conv2d <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Conv2D(filters<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, strides<span style="color:#f92672">=</span>(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>), padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;SAME&#39;</span>, kernel_initializer<span style="color:#f92672">=</span>weight_init)(img)
<span style="color:#66d9ef">print</span>(conv2d<span style="color:#f92672">.</span>shape)

<span style="color:#75715e"># 4차원 - 1차원 Swap</span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Before swapaxes : &#34;</span>,conv2d<span style="color:#f92672">.</span>shape)
feature_maps <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>swapaxes(conv2d, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">3</span>)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;After swapaxes : &#34;</span>,feature_maps<span style="color:#f92672">.</span>shape)

<span style="color:#75715e"># Filter 수만큼 For loop</span>
<span style="color:#66d9ef">for</span> i, feature_map <span style="color:#f92672">in</span> enumerate(feature_maps):
    plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">5</span>,i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>), plt<span style="color:#f92672">.</span>imshow(feature_map<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">14</span>,<span style="color:#ae81ff">14</span>), cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray&#39;</span>)
plt<span style="color:#f92672">.</span>show()

pool <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>MaxPool2D(pool_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>), strides<span style="color:#f92672">=</span>(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>), padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;SAME&#39;</span>)(conv2d)
<span style="color:#66d9ef">print</span>(pool<span style="color:#f92672">.</span>shape)

feature_maps <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>swapaxes(pool, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">3</span>)
<span style="color:#66d9ef">for</span> i, feature_map <span style="color:#f92672">in</span> enumerate(feature_maps):
    plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">5</span>,i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>), plt<span style="color:#f92672">.</span>imshow(feature_map<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">7</span>), cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray&#39;</span>)
plt<span style="color:#f92672">.</span>show()
</code></pre></div></div>
    </div>
  </label>
</div>


<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-11-1</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> __future__ <span style="color:#f92672">import</span> absolute_import
<span style="color:#f92672">from</span> __future__ <span style="color:#f92672">import</span> division
<span style="color:#f92672">from</span> __future__ <span style="color:#f92672">import</span> print_function

<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf
<span style="color:#f92672">from</span> tensorflow <span style="color:#f92672">import</span> keras
<span style="color:#f92672">from</span> tensorflow.keras.utils <span style="color:#f92672">import</span> to_categorical
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">import</span> os

<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)
<span style="color:#66d9ef">print</span>(keras<span style="color:#f92672">.</span>__version__)

<span style="color:#75715e"># Hyper Parameters 설정</span>
learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.001</span>
training_epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">15</span>
batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>

tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>set_seed(<span style="color:#ae81ff">777</span>)

<span style="color:#75715e"># Creating a Checkpoint Directory </span>
cur_dir <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>getcwd()
<span style="color:#66d9ef">print</span>(cur_dir)

ckpt_dir_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;checkpoints&#39;</span>
model_dir_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;minst_cnn_seq&#39;</span>

checkpoint_dir <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(cur_dir, ckpt_dir_name, model_dir_name)
<span style="color:#66d9ef">print</span>(checkpoint_dir)

os<span style="color:#f92672">.</span>makedirs(checkpoint_dir, exist_ok<span style="color:#f92672">=</span>True)

checkpoint_prefix <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(checkpoint_dir, model_dir_name)
<span style="color:#66d9ef">print</span>(checkpoint_prefix)

<span style="color:#75715e">## MNIST Dataset #########################################################</span>
mnist <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>datasets<span style="color:#f92672">.</span>mnist
class_names <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;0&#39;</span>, <span style="color:#e6db74">&#39;1&#39;</span>, <span style="color:#e6db74">&#39;2&#39;</span>, <span style="color:#e6db74">&#39;3&#39;</span>, <span style="color:#e6db74">&#39;4&#39;</span>, <span style="color:#e6db74">&#39;5&#39;</span>, <span style="color:#e6db74">&#39;6&#39;</span>, <span style="color:#e6db74">&#39;7&#39;</span>, <span style="color:#e6db74">&#39;8&#39;</span>, <span style="color:#e6db74">&#39;9&#39;</span>]
<span style="color:#75715e">##########################################################################</span>

<span style="color:#75715e">## Fashion MNIST Dataset #################################################</span>
<span style="color:#75715e">#mnist = keras.datasets.fashion_mnist</span>
<span style="color:#75715e">#class_names = [&#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;]</span>
<span style="color:#75715e">##########################################################################</span>

<span style="color:#75715e"># Datasets Load</span>
(train_images, train_labels), (test_images, test_labels) <span style="color:#f92672">=</span> mnist<span style="color:#f92672">.</span>load_data()    
    
<span style="color:#75715e"># Train Data Normalization</span>
train_images <span style="color:#f92672">=</span> train_images<span style="color:#f92672">.</span>astype(np<span style="color:#f92672">.</span>float32) <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.</span>
test_images <span style="color:#f92672">=</span> test_images<span style="color:#f92672">.</span>astype(np<span style="color:#f92672">.</span>float32) <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.</span>

<span style="color:#75715e"># Train Data 마지막 차원 추가</span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Before : &#34;</span>,train_images<span style="color:#f92672">.</span>shape)
train_images <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>expand_dims(train_images, axis<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
test_images <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>expand_dims(test_images, axis<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;After : &#34;</span>,train_images<span style="color:#f92672">.</span>shape)

<span style="color:#75715e"># Label 카테고리 지정</span>
<span style="color:#75715e"># Converts a class vector (integers) to binary class matrix.</span>
train_labels <span style="color:#f92672">=</span> to_categorical(train_labels, <span style="color:#ae81ff">10</span>)
test_labels  <span style="color:#f92672">=</span> to_categorical(test_labels, <span style="color:#ae81ff">10</span>)    

<span style="color:#75715e"># 이미지 경로 또는 RAW 데이터를 TF를 사용해 DataSet으로 전환</span>
train_dataset <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices((train_images, train_labels))<span style="color:#f92672">.</span>shuffle(buffer_size<span style="color:#f92672">=</span><span style="color:#ae81ff">100000</span>)<span style="color:#f92672">.</span>batch(batch_size)
test_dataset  <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices((test_images , test_labels ))<span style="color:#f92672">.</span>batch(batch_size)

<span style="color:#75715e"># 모델 생성 (sequential-eager)</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_model_seq</span>():
    model <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>Sequential()
    
    <span style="color:#75715e"># 28 X 28 X 32</span>
    model<span style="color:#f92672">.</span>add(keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Conv2D(filters<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, activation<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>relu, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;SAME&#39;</span>, input_shape<span style="color:#f92672">=</span>(<span style="color:#ae81ff">28</span>, <span style="color:#ae81ff">28</span>, <span style="color:#ae81ff">1</span>)))
    model<span style="color:#f92672">.</span>add(keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>MaxPool2D(padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;SAME&#39;</span>))
    
    <span style="color:#75715e"># 14 X 14 X 64</span>
    model<span style="color:#f92672">.</span>add(keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Conv2D(filters<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, activation<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>relu, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;SAME&#39;</span>))
    model<span style="color:#f92672">.</span>add(keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>MaxPool2D(padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;SAME&#39;</span>))

    <span style="color:#75715e"># 7 X 7 X 128</span>
    model<span style="color:#f92672">.</span>add(keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Conv2D(filters<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, activation<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>relu, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;SAME&#39;</span>))
    model<span style="color:#f92672">.</span>add(keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>MaxPool2D(padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;SAME&#39;</span>))
    
    <span style="color:#75715e"># 4 X X 128 -&gt; 2048</span>
    model<span style="color:#f92672">.</span>add(keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Flatten())

    <span style="color:#75715e"># 256</span>
    model<span style="color:#f92672">.</span>add(keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">256</span>, activation<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>relu))
    
    <span style="color:#75715e"># Dropout 0.4</span>
    model<span style="color:#f92672">.</span>add(keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dropout(<span style="color:#ae81ff">0.4</span>))

    <span style="color:#75715e"># 10</span>
    model<span style="color:#f92672">.</span>add(keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">10</span>))
    <span style="color:#66d9ef">return</span> model

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_model_fuc</span>() :
    inputs <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>Input(shape<span style="color:#f92672">=</span>(<span style="color:#ae81ff">28</span>,<span style="color:#ae81ff">28</span>,<span style="color:#ae81ff">1</span>))
    conv1 <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Conv2D(filters<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>, kernel_size<span style="color:#f92672">=</span>[<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">3</span>],padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;SAME&#39;</span>, activation<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>relu)(inputs)
    pool1 <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>MaxPool2D(padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;SAME&#39;</span>)(conv1)
    conv2 <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Conv2D(filters<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, kernel_size<span style="color:#f92672">=</span>[<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">3</span>],padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;SAME&#39;</span>, activation<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>relu)(pool1)
    pool2 <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>MaxPool2D(padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;SAME&#39;</span>)(conv2)
    conv3 <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Conv2D(filters<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, kernel_size<span style="color:#f92672">=</span>[<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">3</span>],padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;SAME&#39;</span>, activation<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>relu)(pool2)
    pool3 <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>MaxPool2D(padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;SAME&#39;</span>)(conv3)
    pool3_flat <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Flatten()(pool3)
    dense4 <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(units<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, activation<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>relu)(pool3_flat)
    drop4 <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dropout(rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.4</span>)(dense4)
    logits <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(units<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)(drop4)
    <span style="color:#66d9ef">return</span> keras<span style="color:#f92672">.</span>Model(inputs<span style="color:#f92672">=</span>inputs, outputs<span style="color:#f92672">=</span>logits)

<span style="color:#75715e">#model = create_model_seq()</span>
model <span style="color:#f92672">=</span> create_model_fuc()

<span style="color:#75715e"># Model Summary 출력</span>
model<span style="color:#f92672">.</span>summary()

<span style="color:#75715e"># tf.function : decorator </span>
<span style="color:#75715e"># 즉시 실행이 가능한 Tensorflow Graph로 전환</span>
<span style="color:#75715e"># Loss Function </span>
<span style="color:#a6e22e">@tf.function</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss_fn</span>(model, images, labels):
    logits <span style="color:#f92672">=</span> model(images, training<span style="color:#f92672">=</span>True)
    loss <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>losses<span style="color:#f92672">.</span>categorical_crossentropy(
        y_pred<span style="color:#f92672">=</span>logits, y_true<span style="color:#f92672">=</span>labels, from_logits<span style="color:#f92672">=</span>True))    
    <span style="color:#66d9ef">return</span> loss   

<span style="color:#75715e"># Calculating Gradient</span>
<span style="color:#a6e22e">@tf.function</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad</span>(model, images, labels):
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
        loss <span style="color:#f92672">=</span> loss_fn(model, images, labels)
    <span style="color:#66d9ef">return</span> tape<span style="color:#f92672">.</span>gradient(loss, model<span style="color:#f92672">.</span>variables)

<span style="color:#75715e"># Calculating Model&#39;s Accuracy</span>
<span style="color:#a6e22e">@tf.function</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">evaluate</span>(model, images, labels):
    logits <span style="color:#f92672">=</span> model(images, training<span style="color:#f92672">=</span>False)
    correct_prediction <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>equal(tf<span style="color:#f92672">.</span>argmax(logits, <span style="color:#ae81ff">1</span>), tf<span style="color:#f92672">.</span>argmax(labels, <span style="color:#ae81ff">1</span>))
    accuracy <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>cast(correct_prediction, tf<span style="color:#f92672">.</span>float32))
    <span style="color:#66d9ef">return</span> accuracy

<span style="color:#75715e"># ADAM Optimizer</span>
<span style="color:#75715e"># Learning_rate = 0.001</span>
optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>Adam(learning_rate<span style="color:#f92672">=</span>learning_rate)

<span style="color:#75715e"># Creating Checkpoint</span>
checkpoint <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>Checkpoint(cnn<span style="color:#f92672">=</span>model)

<span style="color:#75715e"># Train Function Define</span>
<span style="color:#a6e22e">@tf.function</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train</span>(model, images, labels):
    grads <span style="color:#f92672">=</span> grad(model, images, labels)
    optimizer<span style="color:#f92672">.</span>apply_gradients(zip(grads, model<span style="color:#f92672">.</span>trainable_variables))

<span style="color:#75715e"># Train Model</span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Learning Started...&#39;</span>)

<span style="color:#75715e"># Epoch만큼 for loop</span>
<span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(training_epochs):
    <span style="color:#75715e"># default setting</span>
    avg_loss <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.</span>
    avg_train_acc <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.</span>
    avg_test_acc <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.</span>
    train_step <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    test_step <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>    
    
    <span style="color:#75715e"># train_dataset에서 for loop</span>
    <span style="color:#66d9ef">for</span> images, labels <span style="color:#f92672">in</span> train_dataset:
        <span style="color:#75715e"># train</span>
        train(model, images, labels)
        <span style="color:#75715e">#grads = grad(model, images, labels)                </span>
        <span style="color:#75715e">#optimizer.apply_gradients(zip(grads, model.variables))</span>
        
        <span style="color:#75715e"># loss</span>
        loss <span style="color:#f92672">=</span> loss_fn(model, images, labels)
        
        <span style="color:#75715e"># evaluate</span>
        acc <span style="color:#f92672">=</span> evaluate(model, images, labels)
        
        <span style="color:#75715e"># loss cum</span>
        avg_loss <span style="color:#f92672">=</span> avg_loss <span style="color:#f92672">+</span> loss
        
        <span style="color:#75715e"># acc cum</span>
        avg_train_acc <span style="color:#f92672">=</span> avg_train_acc <span style="color:#f92672">+</span> acc
        
        <span style="color:#75715e"># for loop</span>
        train_step <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
    
    <span style="color:#75715e"># avg_loss  / avg_train_acc</span>
    avg_loss <span style="color:#f92672">=</span> avg_loss <span style="color:#f92672">/</span> train_step
    avg_train_acc <span style="color:#f92672">=</span> avg_train_acc <span style="color:#f92672">/</span> train_step
    
    <span style="color:#75715e"># test_dataset에서 for loop</span>
    <span style="color:#66d9ef">for</span> images, labels <span style="color:#f92672">in</span> test_dataset:        
        <span style="color:#75715e"># evaluate </span>
        acc <span style="color:#f92672">=</span> evaluate(model, images, labels)        
        avg_test_acc <span style="color:#f92672">=</span> avg_test_acc <span style="color:#f92672">+</span> acc
        test_step <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>    
    
    <span style="color:#75715e"># test acc average</span>
    avg_test_acc <span style="color:#f92672">=</span> avg_test_acc <span style="color:#f92672">/</span> test_step    

    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;EPCH : &#39;</span>, <span style="color:#e6db74">&#39;{}&#39;</span><span style="color:#f92672">.</span>format(epoch <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>), <span style="color:#e6db74">&#39;| LOSS : &#39;</span>, <span style="color:#e6db74">&#39;{:.8f}&#39;</span><span style="color:#f92672">.</span>format(avg_loss), <span style="color:#e6db74">&#39;| TR_ACC : &#39;</span>, <span style="color:#e6db74">&#39;{:.4f}&#39;</span><span style="color:#f92672">.</span>format(avg_train_acc), <span style="color:#e6db74">&#39;| TS_ACC : &#39;</span>, <span style="color:#e6db74">&#39;{:.4f}&#39;</span><span style="color:#f92672">.</span>format(avg_test_acc))
    
    checkpoint<span style="color:#f92672">.</span>save(file_prefix<span style="color:#f92672">=</span>checkpoint_prefix)

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Learning Finished...&#39;</span>)
</code></pre></div></div>
    </div>
  </label>
</div>

<br>
</article>

            
            

            <footer class="book-footer">
                 <div class="flex flex-wrap justify-between">





</div>
 
                
                
            </footer>

             
<div class="book-comments">
<br>
<br>
<hr> </hr>
<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "prokoptasis" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a></div>
 
            

            <label for="menu-control" class="hidden book-menu-overlay"></label>
        </div>

        
    </main>

    
</body>

</html>

     