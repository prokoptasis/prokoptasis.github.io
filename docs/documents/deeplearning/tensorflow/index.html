<!DOCTYPE html>
<html lang="ko" dir=>

<head>
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Description">
<meta name="theme-color" content="#FFFFFF"><meta property="og:title" content="Tensorflow" />
<meta property="og:description" content="Description" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://prokoptasis.github.io/docs/documents/deeplearning/tensorflow/" />

<title>Tensorflow | Prokoptasis</title>
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="/book.min.001ba32d1ef4d2469d055e28d66fd96b16bfeddee698f1e2729af3a54ed86b8b.css" integrity="sha256-ABujLR700kadBV4o1m/Zaxa/7d7mmPHicprzpU7Ya4s=">
<script defer src="/ko.search.min.4582629366c34a5ed9dc3f1e03c253d2210a18fb41a786b5b0c1e2d5e49ffd0f.js" integrity="sha256-RYJik2bDSl7Z3D8eA8JT0iEKGPtBp4a1sMHi1eSf/Q8="></script>

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-176412981-2', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
<link rel="alternate" type="application/rss+xml" href="http://prokoptasis.github.io/docs/documents/deeplearning/tensorflow/index.xml" title="Prokoptasis" />
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
 
</head>

<body dir=>
    <input type="checkbox" class="hidden toggle" id="menu-control" />
    <input type="checkbox" class="hidden toggle" id="toc-control" />
    <main class="container flex">
        <aside class="book-menu">
             <nav>
<h2 class="book-brand" style="text-align: center;">
    <a href="/"><span>Prokoptasis</span>
  </a>
</h2>
<hr>

<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>





 


  



  
  <ul>
    
      
        <li class="book-section-flat" >
          
  
  

  
    <span>Documents</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-bc65bd57d9bab5039e9a79b5ae591c72" class="toggle"  />
    <label for="section-bc65bd57d9bab5039e9a79b5ae591c72" class="flex justify-between">
      <a href="http://prokoptasis.github.io/docs/documents/projects/" class="">Projects</a>
      <span>▾</span>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/projects/hugogirl/" class="">hU-Go-Girl</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/projects/rust_sdl2/" class="">Rust_SDL2</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/projects/defenders/" class="">Defenders</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/projects/wordfighter/" class="">Wordfighter</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/projects/qmodeler/" class="">Qmodeler</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/projects/cube/" class="">Cube</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/projects/scmtemplate/" class="">SCM Template</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-53481227639f164d3f7ead2411c43315" class="toggle"  />
    <label for="section-53481227639f164d3f7ead2411c43315" class="flex justify-between">
      <a href="http://prokoptasis.github.io/docs/documents/frontend/" class="">Front End</a>
      <span>▾</span>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/frontend/markdown/" class="">Markdown</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-7cff03fc158d9929dd882d45e5e83f84" class="toggle"  />
    <label for="section-7cff03fc158d9929dd882d45e5e83f84" class="flex justify-between">
      <a href="http://prokoptasis.github.io/docs/documents/backend/" class="">Back End</a>
      <span>▾</span>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-0480d4c0a3a48973d66dda9e8e9d50da" class="toggle"  />
    <label for="section-0480d4c0a3a48973d66dda9e8e9d50da" class="flex justify-between">
      <a href="http://prokoptasis.github.io/docs/documents/backend/rust/" class="">Rust</a>
      <span>▾</span>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/backend/rust/rust02/" class="">Rust Advanced</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/backend/rust/rust01/" class="">Rust Basic</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-0d89b8a13399f7331f6db819807b653c" class="toggle"  />
    <label for="section-0d89b8a13399f7331f6db819807b653c" class="flex justify-between">
      <a href="http://prokoptasis.github.io/docs/documents/backend/go/" class="">Go</a>
      <span>▾</span>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/backend/go/go01/" class="">GO Basic</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/backend/go/go02/" class="">GO Advanced</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/backend/go/go03/" class="">GO DB 연결</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-47bd3d4bf94d414a097efcbbc9ad3f79" class="toggle"  />
    <label for="section-47bd3d4bf94d414a097efcbbc9ad3f79" class="flex justify-between">
      <a href="http://prokoptasis.github.io/docs/documents/backend/sql/" class="">SQL</a>
      <span>▾</span>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/backend/sql/sql01/" class="">SQL Basic</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/backend/sql/sql02/" class="">SQL Advanced</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-08315e14cb91d15111d683212f692de5" class="toggle" checked />
    <label for="section-08315e14cb91d15111d683212f692de5" class="flex justify-between">
      <a href="http://prokoptasis.github.io/docs/documents/deeplearning/" class="">Deep Learning</a>
      <span>▾</span>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/deeplearning/machinelearning/" class="">Machine Learning</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/deeplearning/tensorflow/" class=" active">Tensorflow</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-ae112940aeb03e8cb2e77b93d2c01134" class="toggle"  />
    <label for="section-ae112940aeb03e8cb2e77b93d2c01134" class="flex justify-between">
      <a href="http://prokoptasis.github.io/docs/documents/english/" class="">English</a>
      <span>▾</span>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/english/seulsam/" class="">Seulsam</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/english/pickedup/" class="">Picked Up English</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>










 
<ul>
    
    <li>
        <a href="/posts/" >
        Blog
      </a>
    </li>
    
    <li>
        <a href="mailto:c.exigua@gmail.com" target="_blank" rel="noopener" >
        Gmail
      </a>
    </li>
    
    <li>
        <a href="https://github.com/prokoptasis/prokoptasis" target="_blank" rel="noopener" >
        Github
      </a>
    </li>
    
    <li>
        <a href="https://twitter.com/prokoptasis" target="_blank" rel="noopener" >
        Twitter
      </a>
    </li>
    
    <li>
        <a href="https://www.facebook.com/c.exigua/" target="_blank" rel="noopener" >
        Facebook
      </a>
    </li>
    
</ul>
<br>
<hr>
<div style="display: block; text-align: center;">
    <script type="text/javascript" src="https://cdnjs.buymeacoffee.com/1.0.0/button.prod.min.js" data-name="bmc-button" data-slug="prokoptasis" data-color="#e9e9e9" data-emoji="" data-font="Arial" data-text="" data-outline-color="#000" data-font-color="#000"
        data-coffee-color="#6f4e37"></script>
</div>
  

</nav>




  <script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script>

 
            
        </aside>

        <div class="book-page">
            <header class="book-header">
                 <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>Tensorflow</strong>

  <label for="toc-control">
    
  </label>
</div>
  
                
            </header>

            
<article class="markdown"><h2 id="tensor-flow">
  Tensor Flow
  <a class="anchor" href="#tensor-flow">#</a>
</h2>
<hr>
<p>텐저 플로우는 2015년 구글에서 공개한 머신러닝 라이브러리. 케라스는 딥러닝 라이브러리를 Backend로 하는 신경망 모델 구성 라이브러리. 구글은 쥬피터 노트북이라는 오픈 소스 웹 어플리케이션을 코랩이라는 서비스를 통해 제공하고 있음.</p>
<p><a href="https://www.tensorflow.org/?hl=ko" target="_blank" rel="noopener">Tensor Flow</a><br>
<a href="https://keras.io/" target="_blank" rel="noopener">Keras</a><br>
<a href="https://www.python.org/" target="_blank" rel="noopener">Python</a><br>
<a href="https://jupyter.org/" target="_blank" rel="noopener">Jupyter Notebook</a><br>
<a href="https://colab.research.google.com/notebooks/intro.ipynb" target="_blank" rel="noopener">Colab</a></p>
<p>이때 Tensor는 흘러다니는 데이터를 의미함.</p>


<script src="/mermaid.min.js"></script>

  <script>mermaid.initialize({
  "flowchart": {
    "useMaxWidth":true
  },
  "theme": "default"
}
)</script>




<p class="mermaid text-center">
graph LR
A((X:Tensor))-->|Edge|C((+:Node))-->|Edge|D((X+Y:Tensor))
B((Y:Tensor))-->|Edge|C

style A fill:#ffffff,stroke:#000000,stroke-width:1px
style B fill:#ffffff,stroke:#000000,stroke-width:1px
style C fill:#ffffff,stroke:#000000,stroke-width:1px
style D fill:#ffffff,stroke:#000000,stroke-width:1px
</p>

<h3 id="모두를-위한-딥러닝-시즌2">
  모두를 위한 딥러닝 시즌2
  <a class="anchor" href="#%eb%aa%a8%eb%91%90%eb%a5%bc-%ec%9c%84%ed%95%9c-%eb%94%a5%eb%9f%ac%eb%8b%9d-%ec%8b%9c%ec%a6%8c2">#</a>
</h3>
<p>아래는 <a href="https://www.youtube.com/playlist?list=PLQ28Nx3M4Jrguyuwg4xe9d9t2XE639e5C" target="_blank" rel="noopener">모두를 위한 딥러닝 시즌2</a>의 Tensorflow 2 Code를 분석한 내용임.</p>

<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-02-1</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np

<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)

<span style="color:#75715e"># x_data, y_data 정의</span>
x_data <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>]
y_data <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>]

<span style="color:#75715e"># matplot import</span>
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
plt<span style="color:#f92672">.</span>plot(x_data, y_data, <span style="color:#e6db74">&#39;o&#39;</span>)
plt<span style="color:#f92672">.</span>ylim(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">8</span>)

<span style="color:#75715e"># reduce_mean의 차원감소 평균 사용</span>
v <span style="color:#f92672">=</span>[<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">2.</span>, <span style="color:#ae81ff">3.</span>, <span style="color:#ae81ff">4.</span>]
tf<span style="color:#f92672">.</span>reduce_mean(v) <span style="color:#75715e"># 2.5</span>

<span style="color:#75715e"># 3의 제곱근</span>
tf<span style="color:#f92672">.</span>square(<span style="color:#ae81ff">3</span>) <span style="color:#75715e"># 9</span>

<span style="color:#75715e"># x_data, y_data 정의</span>
x_data <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>]
y_data <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>]

<span style="color:#75715e"># Weight Bias 정의</span>
W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(<span style="color:#ae81ff">2.0</span>)
b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(<span style="color:#ae81ff">0.5</span>)

<span style="color:#75715e"># 가설 정의</span>
hypothesis <span style="color:#f92672">=</span> W <span style="color:#f92672">*</span> x_data <span style="color:#f92672">+</span> b

<span style="color:#75715e"># Weight Bias Numpy 출력</span>
W<span style="color:#f92672">.</span>numpy(), b<span style="color:#f92672">.</span>numpy()

<span style="color:#75715e"># Hypothesis Numpy 출력</span>
hypothesis<span style="color:#f92672">.</span>numpy()

<span style="color:#75715e"># x_data, hypothesis -&gt; R</span>
<span style="color:#75715e"># x_data, y_data -&gt; O</span>
plt<span style="color:#f92672">.</span>plot(x_data, hypothesis<span style="color:#f92672">.</span>numpy(), <span style="color:#e6db74">&#39;r-&#39;</span>)
plt<span style="color:#f92672">.</span>plot(x_data, y_data, <span style="color:#e6db74">&#39;o&#39;</span>)
plt<span style="color:#f92672">.</span>ylim(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">12</span>)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># Cost 함수 정의 Average((H-Y)^2)</span>
cost <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>square(hypothesis <span style="color:#f92672">-</span> y_data))

<span style="color:#75715e"># 경사 하강법 Tape 정의</span>
<span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
    hypothesis <span style="color:#f92672">=</span> W <span style="color:#f92672">*</span> x_data <span style="color:#f92672">+</span> b
    cost <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>square(hypothesis <span style="color:#f92672">-</span> y_data))

<span style="color:#75715e"># Weight, Bias</span>
W_grad, b_grad <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(cost, [W, b])
W_grad<span style="color:#f92672">.</span>numpy(), b_grad<span style="color:#f92672">.</span>numpy()

<span style="color:#75715e"># 학습율</span>
learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>

<span style="color:#75715e"># Weight - (Weight * Learning Rate)</span>
W<span style="color:#f92672">.</span>assign_sub(learning_rate <span style="color:#f92672">*</span> W_grad)

<span style="color:#75715e"># Bias - (Bias * Learning Rate)</span>
b<span style="color:#f92672">.</span>assign_sub(learning_rate <span style="color:#f92672">*</span> b_grad)

W<span style="color:#f92672">.</span>numpy(), b<span style="color:#f92672">.</span>numpy()

plt<span style="color:#f92672">.</span>plot(x_data, hypothesis<span style="color:#f92672">.</span>numpy(), <span style="color:#e6db74">&#39;r-&#39;</span>)
plt<span style="color:#f92672">.</span>plot(x_data, y_data, <span style="color:#e6db74">&#39;o&#39;</span>)
plt<span style="color:#f92672">.</span>ylim(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">12</span>)

<span style="color:#75715e"># Weight / Bias 정의</span>
W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(<span style="color:#ae81ff">2.9</span>)
b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(<span style="color:#ae81ff">0.5</span>)

<span style="color:#75715e"># for loop 수행</span>
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">100</span>):
    <span style="color:#75715e"># Gradient Tap e정의</span>
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
        hypothesis <span style="color:#f92672">=</span> W <span style="color:#f92672">*</span> x_data <span style="color:#f92672">+</span> b
        cost <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>square(hypothesis <span style="color:#f92672">-</span> y_data))

    <span style="color:#75715e"># Weight Bias 정의</span>
    W_grad, b_grad <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(cost, [W, b])

    <span style="color:#75715e"># Weight - (Learning Rate * Weight)</span>
    W<span style="color:#f92672">.</span>assign_sub(learning_rate <span style="color:#f92672">*</span> W_grad)
    <span style="color:#75715e"># Bias - (Learning Rate * Bias)</span>
    b<span style="color:#f92672">.</span>assign_sub(learning_rate <span style="color:#f92672">*</span> b_grad)

    <span style="color:#75715e"># Step 10에서 출력</span>
    <span style="color:#66d9ef">if</span> i <span style="color:#f92672">%</span> <span style="color:#ae81ff">10</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
      <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;{:5}|{:10.4f}|{:10.4f}|{:10.6f}&#34;</span><span style="color:#f92672">.</span>format(i, W<span style="color:#f92672">.</span>numpy(), b<span style="color:#f92672">.</span>numpy(), cost))
      
    <span style="color:#75715e"># x_data, y_data 원본</span>
    plt<span style="color:#f92672">.</span>plot(x_data, y_data, <span style="color:#e6db74">&#39;o&#39;</span>)
    <span style="color:#75715e"># x_data, hypothesis 가설</span>
    plt<span style="color:#f92672">.</span>plot(x_data, hypothesis<span style="color:#f92672">.</span>numpy(), <span style="color:#e6db74">&#39;r-&#39;</span>)
    plt<span style="color:#f92672">.</span>ylim(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">18</span>)

<span style="color:#66d9ef">print</span>(W <span style="color:#f92672">*</span> <span style="color:#ae81ff">5</span> <span style="color:#f92672">+</span> b)
<span style="color:#66d9ef">print</span>(W <span style="color:#f92672">*</span> <span style="color:#ae81ff">2.5</span> <span style="color:#f92672">+</span> b)
</code></pre></div></div>
    </div>
  </label>
</div>


<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-03-1</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># import library</span>
<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np

<span style="color:#75715e"># tensor flow version 확인</span>
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)

<span style="color:#75715e"># X,Y 1,2,3 np array 선언 </span>
X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>])
Y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>])

<span style="color:#75715e"># cost_func 정의 (Python형태)</span>
<span style="color:#75715e"># Weight, X, Y를 인자로 전달</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">cost_func_py</span>(W, X, Y):
    <span style="color:#75715e"># c 를 0으로 초기화</span>
    c <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    <span style="color:#75715e"># X 만큼 Loop를 돌며 (W*X - Y) ^ 2 -&gt; C로 대입</span>
    <span style="color:#75715e"># C (오차 제곱근)을 X로 평균낸 값을 return</span>
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(X)):
        c <span style="color:#f92672">+=</span> (W <span style="color:#f92672">*</span> X[i] <span style="color:#f92672">-</span> Y[i]) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>
    <span style="color:#66d9ef">return</span> c <span style="color:#f92672">/</span> len(X)

<span style="color:#75715e"># cost_func 정의 (tensorflow형태)</span>
<span style="color:#75715e"># Weight, X, Y를 인자로 전달</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">cost_func_tf</span>(W, X, Y):
  hypothesis <span style="color:#f92672">=</span> X <span style="color:#f92672">*</span> W
  <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>square(hypothesis <span style="color:#f92672">-</span> Y))

<span style="color:#75715e"># Weight를 -3부터 5까지 9개의 요소로 제공 </span>
W_values <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linspace(<span style="color:#f92672">-</span><span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">12</span>, num<span style="color:#f92672">=</span><span style="color:#ae81ff">23</span>)
cost_values <span style="color:#f92672">=</span> []

<span style="color:#75715e"># numpy.linspace (start,end,number)</span>
<span style="color:#75715e"># 제공된 Weight, X, Y로 Cost를 계산</span>
<span style="color:#66d9ef">for</span> feed_W <span style="color:#f92672">in</span> W_values:
    curr_cost_py <span style="color:#f92672">=</span> cost_func_py(feed_W, X, Y)    
    curr_cost_tf <span style="color:#f92672">=</span> cost_func_tf(feed_W, X, Y)
    cost_values<span style="color:#f92672">.</span>append(curr_cost_tf)
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;{:6.3f} | {:10.5f} | {:10.5f}&#34;</span><span style="color:#f92672">.</span>format(feed_W, curr_cost_py, curr_cost_tf))

<span style="color:#75715e"># matplot import</span>
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt

<span style="color:#75715e"># figure size 지정</span>
plt<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#34;figure.figsize&#34;</span>] <span style="color:#f92672">=</span> (<span style="color:#ae81ff">8</span>,<span style="color:#ae81ff">6</span>)
plt<span style="color:#f92672">.</span>plot(W_values, cost_values, <span style="color:#e6db74">&#34;b&#34;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Cost(W)&#39;</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;W&#39;</span>)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e">##################################################################</span>
<span style="color:#75715e"># random 사용시 Seed 설정</span>
tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>set_seed(<span style="color:#ae81ff">0</span>)  <span style="color:#75715e"># for reproducibility</span>

x_data <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">2.</span>, <span style="color:#ae81ff">3.</span>, <span style="color:#ae81ff">4.</span>]
y_data <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">3.</span>, <span style="color:#ae81ff">5.</span>, <span style="color:#ae81ff">7.</span>]

<span style="color:#75715e"># random.normal((배열),mean,stddev)</span>
W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">1</span>,), <span style="color:#f92672">-</span><span style="color:#ae81ff">100.</span>, <span style="color:#ae81ff">100.</span>))

<span style="color:#75715e"># 300까지 for </span>
<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">300</span>):
    <span style="color:#75715e"># 가설을 W*X로 정의</span>
    hypothesis <span style="color:#f92672">=</span> W <span style="color:#f92672">*</span> X
    <span style="color:#75715e"># TF의 Cost 함수 사용</span>
    cost <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>square(hypothesis <span style="color:#f92672">-</span> Y))

    <span style="color:#75715e"># Learning Rate 0.01</span>
    alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>

    <span style="color:#75715e"># 경사 하강법 : 1/m * (W*X - Y) * X</span>
    gradient <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>multiply(tf<span style="color:#f92672">.</span>multiply(W, X) <span style="color:#f92672">-</span> Y, X))

    <span style="color:#75715e"># Weight - Learning Rate * Weight</span>
    descent <span style="color:#f92672">=</span> W <span style="color:#f92672">-</span> tf<span style="color:#f92672">.</span>multiply(alpha, gradient)

    <span style="color:#75715e"># Assing New Weight</span>
    W<span style="color:#f92672">.</span>assign(descent)
    
    <span style="color:#75715e"># 10 step 마다 로그</span>
    <span style="color:#66d9ef">if</span> step <span style="color:#f92672">%</span> <span style="color:#ae81ff">10</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;{:5} | {:15.6f} | {:10.6f}&#39;</span><span style="color:#f92672">.</span>format( step, cost<span style="color:#f92672">.</span>numpy(), W<span style="color:#f92672">.</span>numpy()[<span style="color:#ae81ff">0</span>]) )

<span style="color:#66d9ef">print</span>(<span style="color:#ae81ff">5.0</span> <span style="color:#f92672">*</span> W)
<span style="color:#66d9ef">print</span>(<span style="color:#ae81ff">2.5</span> <span style="color:#f92672">*</span> W)

<span style="color:#75715e">##################################################################</span>
x_data <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">2.</span>, <span style="color:#ae81ff">3.</span>, <span style="color:#ae81ff">4.</span>]
y_data <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">3.</span>, <span style="color:#ae81ff">5.</span>, <span style="color:#ae81ff">7.</span>]

<span style="color:#75715e"># Weight 5부터 접근</span>
W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable([<span style="color:#ae81ff">5.0</span>])

<span style="color:#75715e"># 300까지 for </span>
<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">300</span>):
    <span style="color:#75715e"># 가설을 W*X로 정의</span>
    hypothesis <span style="color:#f92672">=</span> W <span style="color:#f92672">*</span> X
    <span style="color:#75715e"># TF의 Cost 함수 사용</span>
    cost <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>square(hypothesis <span style="color:#f92672">-</span> Y))

    <span style="color:#75715e"># Learning Rate 0.01</span>
    alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>

    <span style="color:#75715e"># 경사 하강법 : 1/m * (W*X - Y) * X</span>
    gradient <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>multiply(tf<span style="color:#f92672">.</span>multiply(W, X) <span style="color:#f92672">-</span> Y, X))

    <span style="color:#75715e"># Weight - Learning Rate * Weight</span>
    descent <span style="color:#f92672">=</span> W <span style="color:#f92672">-</span> tf<span style="color:#f92672">.</span>multiply(alpha, gradient)

    <span style="color:#75715e"># Assing New Weight</span>
    W<span style="color:#f92672">.</span>assign(descent)
    
    <span style="color:#75715e"># 10 step 마다 로그</span>
    <span style="color:#66d9ef">if</span> step <span style="color:#f92672">%</span> <span style="color:#ae81ff">10</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;{:5} | {:10.4f} | {:10.6f}&#39;</span><span style="color:#f92672">.</span>format(step, cost<span style="color:#f92672">.</span>numpy(), W<span style="color:#f92672">.</span>numpy()[<span style="color:#ae81ff">0</span>]))
</code></pre></div></div>
    </div>
  </label>
</div>


<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-05-1</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># LIbrary 선언</span>
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">%</span>matplotlib inline
<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf

tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>set_seed(<span style="color:#ae81ff">777</span>)  <span style="color:#75715e"># for reproducibility</span>
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)

<span style="color:#75715e"># x_train data, y_train data 정의</span>
x_train <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">2.</span>],
           [<span style="color:#ae81ff">2.</span>, <span style="color:#ae81ff">3.</span>],
           [<span style="color:#ae81ff">3.</span>, <span style="color:#ae81ff">1.</span>],
           [<span style="color:#ae81ff">4.</span>, <span style="color:#ae81ff">3.</span>],
           [<span style="color:#ae81ff">5.</span>, <span style="color:#ae81ff">3.</span>],
           [<span style="color:#ae81ff">6.</span>, <span style="color:#ae81ff">2.</span>]]
y_train <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">0.</span>],
           [<span style="color:#ae81ff">0.</span>],
           [<span style="color:#ae81ff">1.</span>],
           [<span style="color:#ae81ff">1.</span>],
           [<span style="color:#ae81ff">1.</span>],
           [<span style="color:#ae81ff">1.</span>]]

<span style="color:#75715e"># x_test data, y_test data</span>
x_test <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">2.</span>,<span style="color:#ae81ff">2.</span>]]
y_test <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">0.</span>]]


<span style="color:#75715e"># x_train data의 x의 1열이 x1</span>
<span style="color:#75715e"># x_train data의 x의 2열이 x2</span>
x1 <span style="color:#f92672">=</span> [x[<span style="color:#ae81ff">0</span>] <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> x_train]
x2 <span style="color:#f92672">=</span> [x[<span style="color:#ae81ff">1</span>] <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> x_train]

<span style="color:#75715e"># y_train data의 y의 0열의 3나누기 나머지</span>
colors <span style="color:#f92672">=</span> [int(y[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">%</span> <span style="color:#ae81ff">3</span>) <span style="color:#66d9ef">for</span> y <span style="color:#f92672">in</span> y_train]

<span style="color:#75715e"># x_train data scatter plot</span>
plt<span style="color:#f92672">.</span>scatter(x1,x2, c<span style="color:#f92672">=</span>colors , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;x&#39;</span>)

<span style="color:#75715e"># x_test data scatter plot</span>
<span style="color:#75715e"># Test 데이터는 붉은색의 위치와 같이 추론시 1의 값을 가지게 됩니다.</span>
plt<span style="color:#f92672">.</span>scatter(x_test[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>],x_test[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;red&#34;</span>)

plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;x1&#34;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;x2&#34;</span>)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># 학습 dataset 정의 </span>
dataset <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices((x_train, y_train))<span style="color:#f92672">.</span>batch(len(x_train))<span style="color:#75715e">#.repeat()</span>

<span style="color:#75715e"># Weight 2*1 행렬의 0값으로 초기화</span>
<span style="color:#75715e"># Bias 1 벡터의 0값으로 초기화</span>
W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>zeros([<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">1</span>]), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight&#39;</span>)
b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>zeros([<span style="color:#ae81ff">1</span>]), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias&#39;</span>)

<span style="color:#75715e"># hypothesis = ( 1 / (1 + exponential ((fetures * W) + b)) ) &lt;- Sigmoid</span>
<span style="color:#75715e"># https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">logistic_regression</span>(features):
    hypothesis  <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>divide(<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">1.</span> <span style="color:#f92672">+</span> tf<span style="color:#f92672">.</span>exp(tf<span style="color:#f92672">.</span>matmul(features, W) <span style="color:#f92672">+</span> b))
    <span style="color:#66d9ef">return</span> hypothesis

<span style="color:#75715e"># 손실 함수 정의   - ( y * log( hypothesis(x) ) + (1- y) log(1-hypothesis) )</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss_fn</span>(hypothesis, features, labels):
    cost <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>tf<span style="color:#f92672">.</span>reduce_mean(labels <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(logistic_regression(features)) <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> labels) <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> hypothesis))
    <span style="color:#66d9ef">return</span> cost

<span style="color:#75715e"># Activation SGD로 설정 </span>
<span style="color:#75715e"># Learning Rate 0.01로 설정</span>
optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>)

<span style="color:#75715e"># Accuracy Function은 hypothesis, labes(Y)를 인자로 받음</span>
<span style="color:#75715e"># predicted = Type Cast (Hypothesis &gt; 0.5 일 경우 Float 32)</span>
<span style="color:#75715e"># accuracy = predicted, labels를 비교하여 boolean을 반납후 int 32로 cast한 값의 평균을 구함</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">accuracy_fn</span>(hypothesis, labels):
    predicted <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(hypothesis <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0.5</span>, dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32)
    accuracy <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>cast(tf<span style="color:#f92672">.</span>equal(predicted, labels), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>int32))
    <span style="color:#66d9ef">return</span> accuracy

<span style="color:#75715e"># Gradient Tape을 통해 경사값을 계산</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad</span>(features, labels):
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
        <span style="color:#75715e"># 손실 값은 loss_fn(logistic_regression(X), X, Y) 으로 정의</span>
        <span style="color:#75715e"># loss_fn은 Sigmoid</span>
        loss_value <span style="color:#f92672">=</span> loss_fn(logistic_regression(features),features,labels)
    <span style="color:#66d9ef">return</span> tape<span style="color:#f92672">.</span>gradient(loss_value, [W,b])

<span style="color:#75715e"># 1000회 반복 수행</span>
EPOCHS <span style="color:#f92672">=</span> <span style="color:#ae81ff">1001</span>

<span style="color:#75715e"># for loop 1000회</span>
<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(EPOCHS):
    <span style="color:#66d9ef">for</span> features, labels  <span style="color:#f92672">in</span> iter(dataset):
        <span style="color:#75715e"># Grad (X, Y)</span>
        grads <span style="color:#f92672">=</span> grad(features, labels)
        <span style="color:#75715e"># SGD Optimizer Vriable 설정</span>
        optimizer<span style="color:#f92672">.</span>apply_gradients(grads_and_vars<span style="color:#f92672">=</span>zip(grads,[W,b]))
        <span style="color:#75715e"># 100 Step 마다 loss log</span>
        <span style="color:#66d9ef">if</span> step <span style="color:#f92672">%</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Iter: {}, Loss: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(step, loss_fn(logistic_regression(features),features,labels)))

<span style="color:#75715e"># Accuracy Function에 Y&#39;, Y를 대입하고 Test Accuracy 산출후 출력</span>
test_acc <span style="color:#f92672">=</span> accuracy_fn(logistic_regression(x_test),y_test)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Testset Accuracy: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(test_acc))  

x_test <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">5.</span>,<span style="color:#ae81ff">2.</span>]]
y_test <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">1.</span>]]
plt<span style="color:#f92672">.</span>scatter(x_test[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>],x_test[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;red&#34;</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;x1&#34;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;x2&#34;</span>)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># Accuracy Function에 Y&#39;, Y를 대입하고 Test Accuracy 산출후 출력</span>
test_acc <span style="color:#f92672">=</span> accuracy_fn(logistic_regression(x_test),y_test)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Testset Accuracy: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(test_acc))
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Y: &#34;</span>,format(y_test),format(logistic_regression(x_test)) )
</code></pre></div></div>
    </div>
  </label>
</div>


<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-05-2</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">%</span>matplotlib inline
<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf

tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>set_seed(<span style="color:#ae81ff">777</span>)  <span style="color:#75715e"># for reproducibility</span>
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)

<span style="color:#75715e"># Google Driver 사용을 위한 Mount</span>
<span style="color:#f92672">from</span> google.colab <span style="color:#f92672">import</span> drive
drive<span style="color:#f92672">.</span>mount(<span style="color:#e6db74">&#39;/content/drive&#39;</span>)

<span style="color:#75715e"># csv data load</span>
xy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>loadtxt(<span style="color:#e6db74">&#39;/content/drive/My Drive/Colab Notebooks/dl4all_2/data-03-diabetes.csv&#39;</span>, delimiter<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;,&#39;</span>, dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32)
<span style="color:#75715e"># x_train 전체행 1열부터 마지막 전열</span>
x_train <span style="color:#f92672">=</span> xy[:, <span style="color:#ae81ff">0</span>:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
<span style="color:#75715e"># y_train 전체행 마지막 열</span>
y_train <span style="color:#f92672">=</span> xy[:, [<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]]

<span style="color:#75715e"># shape 출력</span>
<span style="color:#66d9ef">print</span>(x_train<span style="color:#f92672">.</span>shape, y_train<span style="color:#f92672">.</span>shape)

<span style="color:#75715e"># xy 출력</span>
<span style="color:#66d9ef">print</span>(xy)

dataset <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices((x_train, y_train))<span style="color:#f92672">.</span>batch(len(x_train))

<span style="color:#75715e"># Weight와 Bias를 Random으로 설정</span>
W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">1</span>)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight&#39;</span>)
b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">1</span>,)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias&#39;</span>)

<span style="color:#75715e"># Activation : Hypothesis = 1 / ( 1 + exponential ( X matmul W ) + b )</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">logistic_regression</span>(features):
    hypothesis  <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>divide(<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">1.</span> <span style="color:#f92672">+</span> tf<span style="color:#f92672">.</span>exp(tf<span style="color:#f92672">.</span>matmul(features, W) <span style="color:#f92672">+</span> b))
    <span style="color:#66d9ef">return</span> hypothesis

<span style="color:#75715e"># 손실 함수 정의   - ( y * log( hypothesis(x) ) + (1- y) log(1-hypothesis) )</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss_fn</span>(hypothesis, features, labels):
    cost <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>tf<span style="color:#f92672">.</span>reduce_mean(labels <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(logistic_regression(features)) <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> labels) <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> hypothesis))
    <span style="color:#66d9ef">return</span> cost

<span style="color:#75715e"># Activation SGD </span>
<span style="color:#75715e"># Learning Rate 0.01</span>
optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>)

<span style="color:#75715e"># Sigmoid 함수를 통해 예측값이 0.5보다 크면 1을 반환하고 0.5보다 작으면 0으로 반환</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">accuracy_fn</span>(hypothesis, labels):
    predicted <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(hypothesis <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0.5</span>, dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32)
    accuracy <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>cast(tf<span style="color:#f92672">.</span>equal(predicted, labels), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>int32))
    <span style="color:#66d9ef">return</span> accuracy

<span style="color:#75715e"># GradientTape을 통해 경사값을 계산</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad</span>(hypothesis, features, labels):
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
        loss_value <span style="color:#f92672">=</span> loss_fn(logistic_regression(features),features,labels)
    <span style="color:#66d9ef">return</span> tape<span style="color:#f92672">.</span>gradient(loss_value, [W,b])

<span style="color:#75715e"># 반복횟수 1000회</span>
EPOCHS <span style="color:#f92672">=</span> <span style="color:#ae81ff">1001</span>

<span style="color:#75715e"># Epochs 만큼 for loop</span>
<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(EPOCHS):
    <span style="color:#75715e"># dataset에 fetures와 labels 만큼 for loop</span>
    <span style="color:#66d9ef">for</span> features, labels  <span style="color:#f92672">in</span> iter(dataset):
        <span style="color:#75715e"># gradients 정의</span>
        grads <span style="color:#f92672">=</span> grad(logistic_regression(features), features, labels)

        <span style="color:#75715e"># SGD Optimizer Variable Setting</span>
        optimizer<span style="color:#f92672">.</span>apply_gradients(grads_and_vars<span style="color:#f92672">=</span>zip(grads,[W,b]))

        <span style="color:#75715e"># 100 Step 마다 로그</span>
        <span style="color:#66d9ef">if</span> step <span style="color:#f92672">%</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Iter: {}, Loss: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(step, loss_fn(logistic_regression(features),features,labels)))
</code></pre></div></div>
    </div>
  </label>
</div>


<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-06-1</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)
tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>set_seed(<span style="color:#ae81ff">777</span>)  <span style="color:#75715e"># for reproducibility</span>

<span style="color:#75715e"># x_data, y_data 정의</span>
x_data <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>],
          [<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">2</span>],
          [<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>],
          [<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">6</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">6</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">7</span>]]
y_data <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>],
          [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>],
          [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>],
          [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>],
          [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>],
          [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>]]

<span style="color:#75715e">#convert into numpy and float format</span>
<span style="color:#75715e"># numpy array float32 data로 converting</span>
x_data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>asarray(x_data, dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32)
y_data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>asarray(y_data, dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32)

<span style="color:#75715e"># dataset을 선언합니다.</span>
dataset <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices((x_data, y_data))
dataset <span style="color:#f92672">=</span> dataset<span style="color:#f92672">.</span>repeat()<span style="color:#f92672">.</span>batch(<span style="color:#ae81ff">2</span>)
nb_classes <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span> <span style="color:#75715e">#class의 개수입니다.</span>

<span style="color:#66d9ef">print</span>(x_data<span style="color:#f92672">.</span>shape)
<span style="color:#66d9ef">print</span>(y_data<span style="color:#f92672">.</span>shape)

<span style="color:#75715e">#Weight and bias setting</span>
<span style="color:#75715e"># Weight random.normal (4,3)</span>
<span style="color:#75715e"># Bias random.normal (3)</span>
W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">4</span>, nb_classes)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight&#39;</span>)
b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((nb_classes,)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias&#39;</span>)
variables <span style="color:#f92672">=</span> [W, b]

<span style="color:#66d9ef">print</span>(W,b)

<span style="color:#75715e"># tf.nn.softmax computes softmax activations</span>
<span style="color:#75715e"># softmax = exp(logits) / reduce_sum(exp(logits), dim)</span>
<span style="color:#75715e"># softmax softmax ( X * W + B )</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">hypothesis</span>(X):
    <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>softmax(tf<span style="color:#f92672">.</span>matmul(X, W) <span style="color:#f92672">+</span> b)

<span style="color:#75715e"># h(x) print</span>
<span style="color:#66d9ef">print</span>(hypothesis(x_data))

<span style="color:#75715e"># Softmax onehot test</span>
<span style="color:#75715e"># Softmax 테스트</span>
sample_db <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">8</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">4</span>]]
sample_db <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>asarray(sample_db, dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32)

<span style="color:#66d9ef">print</span>(hypothesis(sample_db))

<span style="color:#75715e"># Cost Function</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">cost_fn</span>(X, Y):
    <span style="color:#75715e"># H(X) = Logit</span>
    logits <span style="color:#f92672">=</span> hypothesis(X)

    <span style="color:#75715e"># Cost =  SUM ( Y * log(H(X)) ) </span>
    cost <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>tf<span style="color:#f92672">.</span>reduce_sum(Y <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(logits), axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)

    <span style="color:#75715e"># cost_mean =  mean( cost )</span>
    cost_mean <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(cost)
    
    <span style="color:#66d9ef">return</span> cost_mean

<span style="color:#66d9ef">print</span>(cost_fn(x_data, y_data))

<span style="color:#75715e"># x = 3</span>
x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>constant(<span style="color:#ae81ff">3.0</span>)

<span style="color:#75715e"># GradientTape 정의</span>
<span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> g:
    <span style="color:#75715e"># g.watch : Ensures that tensor is being traced by this tape.</span>
    g<span style="color:#f92672">.</span>watch(x)
    y <span style="color:#f92672">=</span> x <span style="color:#f92672">*</span> x <span style="color:#75715e"># x^2</span>

<span style="color:#75715e"># delta y , delta x</span>
dy_dx <span style="color:#f92672">=</span> g<span style="color:#f92672">.</span>gradient(y, x) <span style="color:#75715e"># Will compute to 6.0</span>
<span style="color:#66d9ef">print</span>(dy_dx)

<span style="color:#75715e"># Gradient Funtion</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad_fn</span>(X, Y):
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
        <span style="color:#75715e"># loss = cost_mean</span>
        loss <span style="color:#f92672">=</span> cost_fn(X, Y)

        <span style="color:#75715e"># gradient ( loss, )</span>
        grads <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(loss, variables)

        <span style="color:#66d9ef">return</span> grads

<span style="color:#66d9ef">print</span>(grad_fn(x_data, y_data))

<span style="color:#75715e"># fitting : X,Y, 2000 epochs, verbose 100</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fit</span>(X, Y, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">2000</span>, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>):
    <span style="color:#75715e"># learning rate setting</span>
    optimizer <span style="color:#f92672">=</span>  tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>)

    <span style="color:#75715e"># for loop </span>
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(epochs):
        <span style="color:#75715e"># gradient function X, Y</span>
        grads <span style="color:#f92672">=</span> grad_fn(X, Y)

        <span style="color:#75715e"># Optimizer Variable Seting</span>
        optimizer<span style="color:#f92672">.</span>apply_gradients(zip(grads, variables))

        <span style="color:#75715e"># verbose 100 step log</span>
        <span style="color:#66d9ef">if</span> (i<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>) <span style="color:#f92672">|</span> ((i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">%</span>verbose<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>):
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Loss at epoch </span><span style="color:#e6db74">%d</span><span style="color:#e6db74">: </span><span style="color:#e6db74">%f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span>(i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>, cost_fn(X, Y)<span style="color:#f92672">.</span>numpy()))

<span style="color:#75715e"># start fitting loop</span>
fit(x_data, y_data)

<span style="color:#75715e"># Sample 2, 1, 3, 2 -&gt; 0,0,1</span>
sample_data <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">2</span>]] <span style="color:#75715e"># answer_label [[0,0,1]]</span>
sample_data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>asarray(sample_data, dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32)

a <span style="color:#f92672">=</span> hypothesis(sample_data)

<span style="color:#66d9ef">print</span>(a)
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>argmax(a, <span style="color:#ae81ff">1</span>)) <span style="color:#75715e">#index: 2</span>

sample_data <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">6</span>,<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">4</span>]] 
sample_data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>asarray(sample_data, dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32)

a <span style="color:#f92672">=</span> hypothesis(sample_data)

<span style="color:#66d9ef">print</span>(a)
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>argmax(a, <span style="color:#ae81ff">1</span>)) <span style="color:#75715e">#index: 2</span>

b <span style="color:#f92672">=</span> hypothesis(x_data)
<span style="color:#66d9ef">print</span>(b)
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>argmax(b, <span style="color:#ae81ff">1</span>))
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>argmax(y_data, <span style="color:#ae81ff">1</span>)) <span style="color:#75715e"># matches with y_data</span>

<span style="color:#75715e"># softmax classifer</span>
<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">softmax_classifer</span>(tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Model):
    <span style="color:#75715e"># class initilization</span>
    <span style="color:#66d9ef">def</span> __init__(self, nb_classes):
        super(softmax_classifer, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">4</span>, nb_classes)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight&#39;</span>)
        self<span style="color:#f92672">.</span>b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((nb_classes,)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias&#39;</span>)
        
    <span style="color:#75715e"># softmax regression</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">softmax_regression</span>(self, X):
        <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>softmax(tf<span style="color:#f92672">.</span>matmul(X, self<span style="color:#f92672">.</span>W) <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>b)
    
    <span style="color:#75715e"># cost_function</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">cost_fn</span>(self, X, Y):
        logits <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>softmax_regression(X)
        cost <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(<span style="color:#f92672">-</span>tf<span style="color:#f92672">.</span>reduce_sum(Y <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(logits), axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>))        
        <span style="color:#66d9ef">return</span> cost
    
    <span style="color:#75715e"># gradient function</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad_fn</span>(self, X, Y):
        <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
            cost <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>cost_fn(x_data, y_data)
            grads <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(cost, self<span style="color:#f92672">.</span>variables)            
            <span style="color:#66d9ef">return</span> grads
    
    <span style="color:#75715e"># fitting</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fit</span>(self, X, Y, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">2000</span>, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">500</span>):
        optimizer <span style="color:#f92672">=</span>  tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>)

        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(epochs):
            grads <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>grad_fn(X, Y)
            optimizer<span style="color:#f92672">.</span>apply_gradients(zip(grads, self<span style="color:#f92672">.</span>variables))
            <span style="color:#66d9ef">if</span> (i<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>) <span style="color:#f92672">|</span> ((i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">%</span>verbose<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>):
                <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Loss at epoch </span><span style="color:#e6db74">%d</span><span style="color:#e6db74">: </span><span style="color:#e6db74">%f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span>(i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>cost_fn(X, Y)<span style="color:#f92672">.</span>numpy()))
            
model <span style="color:#f92672">=</span> softmax_classifer(nb_classes)
model<span style="color:#f92672">.</span>fit(x_data, y_data)
</code></pre></div></div>
    </div>
  </label>
</div>


<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-06-2</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)
tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>set_seed(<span style="color:#ae81ff">777</span>)  <span style="color:#75715e"># for reproducibility</span>

<span style="color:#75715e"># Google Driver 사용을 위한 Mount</span>
<span style="color:#f92672">from</span> google.colab <span style="color:#f92672">import</span> drive
drive<span style="color:#f92672">.</span>mount(<span style="color:#e6db74">&#39;/content/drive&#39;</span>)

<span style="color:#75715e"># csv data load</span>
xy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>loadtxt(<span style="color:#e6db74">&#39;/content/drive/My Drive/Colab Notebooks/dl4all_2/data-04-zoo.csv&#39;</span>, delimiter<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;,&#39;</span>, dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32)
<span style="color:#75715e"># x_data 전체행 1열부터 마지막 전열</span>
x_data <span style="color:#f92672">=</span> xy[:, <span style="color:#ae81ff">0</span>:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
<span style="color:#75715e"># y_data 전체행 마지막열</span>
y_data <span style="color:#f92672">=</span> xy[:, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]

<span style="color:#75715e"># nb_classes 7 </span>
nb_classes <span style="color:#f92672">=</span> <span style="color:#ae81ff">7</span>  <span style="color:#75715e"># 0 ~ 6</span>

<span style="color:#75715e"># Make Y data as onehot shape</span>
<span style="color:#75715e"># Y Data One-Hot Encoding</span>
Y_one_hot <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>one_hot(y_data<span style="color:#f92672">.</span>astype(np<span style="color:#f92672">.</span>int32), nb_classes)

<span style="color:#66d9ef">print</span>(x_data<span style="color:#f92672">.</span>shape, Y_one_hot<span style="color:#f92672">.</span>shape, y_data<span style="color:#f92672">.</span>shape, Y_one_hot, y_data)

<span style="color:#75715e"># Weight and bias setting</span>
<span style="color:#75715e"># Weight = 16, 7</span>
<span style="color:#75715e"># Bias = 16</span>
W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">16</span>, nb_classes)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight&#39;</span>)
b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((nb_classes,)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias&#39;</span>)
variables <span style="color:#f92672">=</span> [W, b]

<span style="color:#75715e"># tf.nn.softmax computes softmax activations</span>
<span style="color:#75715e"># softmax = exp(logits) / reduce_sum(exp(logits), dim)</span>
<span style="color:#75715e"># Logit Function = X * W + B</span>
<span style="color:#75715e"># cross_entropy_with_logist에서 logit_fn을 받음</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">logit_fn</span>(X):
    <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>matmul(X, W) <span style="color:#f92672">+</span> b

<span style="color:#75715e"># Hypothesis = Softmax ( Logit Function )</span>
<span style="color:#75715e"># prediction에서 hypothesis 받음</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">hypothesis</span>(X):
    <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>softmax(logit_fn(X))

<span style="color:#75715e"># Cost Function </span>
<span style="color:#75715e"># Logits = Logit Function</span>
<span style="color:#75715e"># Cost_i = Cross Entropy ( Y , Logit )</span>
<span style="color:#75715e"># Cost Reduce Mean ( Cost_i )</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">cost_fn</span>(X, Y):
    logits <span style="color:#f92672">=</span> logit_fn(X)
    cost_i <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>losses<span style="color:#f92672">.</span>categorical_crossentropy(y_true<span style="color:#f92672">=</span>Y, y_pred<span style="color:#f92672">=</span>logits, from_logits<span style="color:#f92672">=</span>True)    
    cost <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(cost_i)    
    <span style="color:#66d9ef">return</span> cost

<span style="color:#75715e"># Gradient Tape</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad_fn</span>(X, Y):
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
        <span style="color:#75715e"># Cost Function ( X, Y )</span>
        loss <span style="color:#f92672">=</span> cost_fn(X, Y)
        grads <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(loss, variables)
        <span style="color:#66d9ef">return</span> grads

<span style="color:#75715e"># Prediction : ArgMax (Hypothesis(X), 1)</span>
<span style="color:#75715e"># Correct Prediction</span>
<span style="color:#75715e"># Accuracy = Average ( Prediction Float 32 Type Casting )</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">prediction</span>(X, Y):
    <span style="color:#75715e"># Argument 중 Max값을 Prediction Value로 선택</span>
    pred <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>argmax(hypothesis(X), <span style="color:#ae81ff">1</span>)
    <span style="color:#75715e"># Prediction과 Argument Max의 동일 여부를 선택</span>
    correct_prediction <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>equal(pred, tf<span style="color:#f92672">.</span>argmax(Y, <span style="color:#ae81ff">1</span>))
    accuracy <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>cast(correct_prediction, tf<span style="color:#f92672">.</span>float32))

    <span style="color:#66d9ef">return</span> accuracy

<span style="color:#75715e"># fitting function</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fit</span>(X, Y, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>):
    <span style="color:#75715e"># SGD Optimizer : Learning Rate 0.1</span>
    optimizer <span style="color:#f92672">=</span>  tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>)
e
    <span style="color:#75715e"># Epochs for loop</span>
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(epochs):
        grads <span style="color:#f92672">=</span> grad_fn(X, Y)
        optimizer<span style="color:#f92672">.</span>apply_gradients(zip(grads, variables))

        <span style="color:#66d9ef">if</span> (i<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>) <span style="color:#f92672">|</span> ((i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">%</span>verbose<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>):
<span style="color:#75715e">#             print(&#39;Loss at epoch %d: %f&#39; %(i+1, cost_fn(X, Y).numpy()))</span>
            acc <span style="color:#f92672">=</span> prediction(X, Y)<span style="color:#f92672">.</span>numpy()
            loss <span style="color:#f92672">=</span> cost_fn(X, Y)<span style="color:#f92672">.</span>numpy() 
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Steps: {} Loss: {}, Acc: {}&#39;</span><span style="color:#f92672">.</span>format(i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>, loss, acc))

fit(x_data, Y_one_hot)
</code></pre></div></div>
    </div>
  </label>
</div>


<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-07-1</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># import</span>
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">%</span>matplotlib inline
<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf
<span style="color:#f92672">from</span> mpl_toolkits.mplot3d <span style="color:#f92672">import</span> Axes3D

tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>set_seed(<span style="color:#ae81ff">777</span>)  <span style="color:#75715e"># for reproducibility</span>

<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)

<span style="color:#75715e"># x_train data</span>
x_train <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">2</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">5</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">5</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">6</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">7</span>]]

<span style="color:#75715e"># y_train data</span>
y_train <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>],
          [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>],
          [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>],
          [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>],
          [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>],
          [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>],
          [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>]]

<span style="color:#75715e"># Evaluation our model using this test dataset</span>
<span style="color:#75715e"># x_test data</span>
x_test <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>],
          [<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>],
          [<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>]]
<span style="color:#75715e"># y_test data</span>
y_test <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>],
          [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>],
          [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>]]

<span style="color:#75715e"># x_train data</span>
x1 <span style="color:#f92672">=</span> [x[<span style="color:#ae81ff">0</span>] <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> x_train]
x2 <span style="color:#f92672">=</span> [x[<span style="color:#ae81ff">1</span>] <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> x_train]
x3 <span style="color:#f92672">=</span> [x[<span style="color:#ae81ff">2</span>] <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> x_train]

<span style="color:#75715e"># plot figure 3d</span>
fig <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>figure()
ax <span style="color:#f92672">=</span> fig<span style="color:#f92672">.</span>add_subplot(<span style="color:#ae81ff">111</span>, projection<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;3d&#39;</span>)
ax<span style="color:#f92672">.</span>scatter(x1, x2, x3, c<span style="color:#f92672">=</span>y_train, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)

<span style="color:#75715e"># test data </span>
ax<span style="color:#f92672">.</span>scatter(x_test[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>], x_test[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">1</span>], x_test[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">2</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;red&#34;</span>, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;x&#39;</span>)
ax<span style="color:#f92672">.</span>scatter(x_test[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">0</span>], x_test[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">1</span>], x_test[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">2</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;red&#34;</span>, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;x&#39;</span>)
ax<span style="color:#f92672">.</span>scatter(x_test[<span style="color:#ae81ff">2</span>][<span style="color:#ae81ff">0</span>], x_test[<span style="color:#ae81ff">2</span>][<span style="color:#ae81ff">1</span>], x_test[<span style="color:#ae81ff">2</span>][<span style="color:#ae81ff">2</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;red&#34;</span>, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;x&#39;</span>)

ax<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">&#39;X Label&#39;</span>)
ax<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">&#39;Y Label&#39;</span>)
ax<span style="color:#f92672">.</span>set_zlabel(<span style="color:#e6db74">&#39;Z Label&#39;</span>)

plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># Tensorflow data API를 통해 학습시킬 값을 담는다 (Batch Size는 한번에 학습시킬 Size로 정함)</span>
<span style="color:#75715e"># dataset 정의</span>
dataset <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices((x_train, y_train))<span style="color:#f92672">.</span>batch(len(x_train))<span style="color:#75715e">#.repeat()</span>

<span style="color:#75715e"># Weight Bias Random 정의</span>
W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>)))
b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">3</span>,)))

<span style="color:#75715e"># Softmax를 가설로 선언 : Softmax를 통해 가장 높은 값을 구함</span>
<span style="color:#75715e"># softmax_fn 선언 (Hypothesis = Softmax( F(x)를 Return))</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">softmax_fn</span>(features):
    <span style="color:#75715e"># hypothesis = softmax ( fetures (x) * Weight + Bias )</span>
    hypothesis <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>softmax(tf<span style="color:#f92672">.</span>matmul(features, W) <span style="color:#f92672">+</span> b)
    <span style="color:#66d9ef">return</span> hypothesis

<span style="color:#75715e"># 가설을 검증할 Cost 함수를 정의</span>
<span style="color:#75715e"># Average ( - SUM ( Labels (Y) * Log (Softmax(H(X))) ) )</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss_fn</span>(hypothesis, features, labels):
    cost <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(<span style="color:#f92672">-</span>tf<span style="color:#f92672">.</span>reduce_sum(labels <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(hypothesis), axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>))
    <span style="color:#66d9ef">return</span> cost

<span style="color:#75715e"># Boolean is_decay 설정</span>
is_decay <span style="color:#f92672">=</span> True

<span style="color:#75715e"># Start Learning Rate 0.1 설정</span>
starter_learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>

<span style="color:#75715e"># Decaying the learning rate</span>
<span style="color:#75715e"># tf.train.exponential_decay : This function applies an exponential decay function to a provided initial learning rate</span>
<span style="color:#75715e"># tf.train.inverse_time_decay : It is often recommended to lower the learning rate as the training progresses</span>
<span style="color:#75715e"># tf.train.natural_exp_decay : This function applies an exponential decay function to a provided initial learning rate</span>
<span style="color:#75715e"># tf.train.piecewise_constant : Piecewise constant from boundaries and interval values.</span>
<span style="color:#75715e"># tf.train.polynomial_decay : It is commonly observed that a monotonically decreasing learning rate, whose degree of change is carefully chosen, results in a better performing model.</span>
<span style="color:#75715e"># tf.train.cosine_decay : When training a model, it is often recommended to lower the learning rate as the training progresses.</span>
<span style="color:#75715e"># tf.train.linear_cosine_decay : Note that linear cosine decay is more aggressive than cosine decay and larger initial learning rates can typically be used.</span>
<span style="color:#75715e"># tf.train.noisy_linear_cosine_decay : Note that linear cosine decay is more aggressive than cosine decay and larger initial learning rates can typically be used.</span>

<span style="color:#75715e"># is_decay true 일경우</span>
<span style="color:#66d9ef">if</span>(is_decay):    
    <span style="color:#75715e"># Learning Rate = Exponential Deacy ( 시작 LR, Step, Rate, Staricase 여부 )</span>
    <span style="color:#75715e"># exp_rate = starter_rate * exp ( decay_rate * t )</span>
    <span style="color:#75715e"># initial_learning_rate : The inital Learning Rate</span>
    <span style="color:#75715e"># global_step : 현재 학습 횟수, Global step to use for the decay computation. Must not be negative.</span>
    <span style="color:#75715e"># decay_steps : 총학습 횟수, Must be positive</span>
    <span style="color:#75715e"># decay_rate : 감소 비율, * decay_rate</span>
    <span style="color:#75715e"># staricase : 이산적 학습 속도 감속 유무 ( global_step / decay_steps ) 의 제곱 적용 여부</span>
    <span style="color:#75715e"># decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)</span>
    learning_rate <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>schedules<span style="color:#f92672">.</span>ExponentialDecay(initial_learning_rate<span style="color:#f92672">=</span>starter_learning_rate,
                                                                 decay_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>,
                                                                 decay_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.96</span>,
                                                                 staircase<span style="color:#f92672">=</span>True)
    <span style="color:#75715e"># SGD Optimizer</span>
    optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate)
<span style="color:#66d9ef">else</span>:
    <span style="color:#75715e"># is_decay가 false면 초기 Learning Rate 적용</span>
    optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate<span style="color:#f92672">=</span>starter_learning_rate)

<span style="color:#75715e"># Gradient Tape</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad</span>(hypothesis, features, labels):
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
        <span style="color:#75715e"># Loss Value = Loss Function ( softmax (X) , x , y )</span>
        loss_value <span style="color:#f92672">=</span> loss_fn(softmax_fn(features),features,labels)
    <span style="color:#75715e"># Gradient Tape ( cost , [w, b] )</span>
    <span style="color:#66d9ef">return</span> tape<span style="color:#f92672">.</span>gradient(loss_value, [W,b])

<span style="color:#75715e"># Accuracy Function</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">accuracy_fn</span>(hypothesis, labels):
    <span style="color:#75715e"># Prediction = Hypothesis중 Max</span>
    prediction <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>argmax(hypothesis, <span style="color:#ae81ff">1</span>)
    <span style="color:#75715e"># Equal 판정</span>
    is_correct <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>equal(prediction, tf<span style="color:#f92672">.</span>argmax(labels, <span style="color:#ae81ff">1</span>))
    <span style="color:#75715e"># 정확도 측정</span>
    accuracy <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>cast(is_correct, tf<span style="color:#f92672">.</span>float32))
    <span style="color:#75715e"># Accuracy Return</span>
    <span style="color:#66d9ef">return</span> accuracy

<span style="color:#75715e"># epochs 1000</span>
EPOCHS <span style="color:#f92672">=</span> <span style="color:#ae81ff">1001</span>

<span style="color:#75715e"># epoch 만큼 for loop</span>
<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(EPOCHS):
    <span style="color:#75715e"># dataset 의 x, y 만큼 for loop</span>
    <span style="color:#66d9ef">for</span> features, labels  <span style="color:#f92672">in</span> iter(dataset):
        <span style="color:#75715e"># features (x)  type cast</span>
        features <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(features, tf<span style="color:#f92672">.</span>float32)
        <span style="color:#75715e"># labes (y) type cast</span>
        labels <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(labels, tf<span style="color:#f92672">.</span>float32)
        <span style="color:#75715e"># Gradient Tape</span>
        grads <span style="color:#f92672">=</span> grad(softmax_fn(features), features, labels)
        <span style="color:#75715e"># Optimizer</span>
        optimizer<span style="color:#f92672">.</span>apply_gradients(grads_and_vars<span style="color:#f92672">=</span>zip(grads,[W,b]))
        <span style="color:#75715e"># verbose 100 </span>
        <span style="color:#66d9ef">if</span> step <span style="color:#f92672">%</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Iter: {}, Loss: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(step, loss_fn(softmax_fn(features),features,labels)))

<span style="color:#75715e"># Test Set 수행</span>
x_test <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(x_test, tf<span style="color:#f92672">.</span>float32)
y_test <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(y_test, tf<span style="color:#f92672">.</span>float32)
test_acc <span style="color:#f92672">=</span> accuracy_fn(softmax_fn(x_test),y_test)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Testset Accuracy: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(test_acc))
</code></pre></div></div>
    </div>
  </label>
</div>


<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-07-2</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
<span style="color:#75715e"># import</span>
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">%</span>matplotlib inline
<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf

tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>set_seed(<span style="color:#ae81ff">777</span>)  <span style="color:#75715e"># for reproducibility</span>

<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)

<span style="color:#75715e"># xy 정의</span>
xy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[<span style="color:#ae81ff">828.659973</span>, <span style="color:#ae81ff">833.450012</span>, <span style="color:#ae81ff">908100</span>, <span style="color:#ae81ff">828.349976</span>, <span style="color:#ae81ff">831.659973</span>],
               [<span style="color:#ae81ff">823.02002</span>, <span style="color:#ae81ff">828.070007</span>, <span style="color:#ae81ff">1828100</span>, <span style="color:#ae81ff">821.655029</span>, <span style="color:#ae81ff">828.070007</span>],
               [<span style="color:#ae81ff">819.929993</span>, <span style="color:#ae81ff">824.400024</span>, <span style="color:#ae81ff">1438100</span>, <span style="color:#ae81ff">818.97998</span>, <span style="color:#ae81ff">824.159973</span>],
               [<span style="color:#ae81ff">816</span>, <span style="color:#ae81ff">820.958984</span>, <span style="color:#ae81ff">1008100</span>, <span style="color:#ae81ff">815.48999</span>, <span style="color:#ae81ff">819.23999</span>],
               [<span style="color:#ae81ff">819.359985</span>, <span style="color:#ae81ff">823</span>, <span style="color:#ae81ff">1188100</span>, <span style="color:#ae81ff">818.469971</span>, <span style="color:#ae81ff">818.97998</span>],
               [<span style="color:#ae81ff">819</span>, <span style="color:#ae81ff">823</span>, <span style="color:#ae81ff">1198100</span>, <span style="color:#ae81ff">816</span>, <span style="color:#ae81ff">820.450012</span>],
               [<span style="color:#ae81ff">811.700012</span>, <span style="color:#ae81ff">815.25</span>, <span style="color:#ae81ff">1098100</span>, <span style="color:#ae81ff">809.780029</span>, <span style="color:#ae81ff">813.669983</span>],
               [<span style="color:#ae81ff">809.51001</span>, <span style="color:#ae81ff">816.659973</span>, <span style="color:#ae81ff">1398100</span>, <span style="color:#ae81ff">804.539978</span>, <span style="color:#ae81ff">809.559998</span>]])

<span style="color:#75715e"># x_train 전행, 1열부터 마지막 전열</span>
<span style="color:#75715e"># y_train 전행, 마지막열</span>
x_train <span style="color:#f92672">=</span> xy[:, <span style="color:#ae81ff">0</span>:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
y_train <span style="color:#f92672">=</span> xy[:, [<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]]

<span style="color:#75715e"># 이상치에 의해 값이 왜곡됨</span>
plt<span style="color:#f92672">.</span>plot(x_train, <span style="color:#e6db74">&#39;ro&#39;</span>)
plt<span style="color:#f92672">.</span>plot(y_train)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># Tensorflow data API를 통해 학습시킬 값을 담음 (Batch Size는 한번에 학습시킬 Size로 정함)</span>
<span style="color:#75715e"># X(features),Y(labels)는 실재 학습에 쓰일 Data (연산을 위해 Type를 맞춰줌)</span>
dataset <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices((x_train, y_train))<span style="color:#f92672">.</span>batch(len(x_train))

<span style="color:#75715e"># Weight, Bias Random 초기화</span>
W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">1</span>)), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32)
b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">1</span>,)), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32)

<span style="color:#75715e"># Linear Regression Hypothesis 정의 : X * Weight + Bias</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">linearReg_fn</span>(features):
    hypothesis <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>matmul(features, W) <span style="color:#f92672">+</span> b
    <span style="color:#66d9ef">return</span> hypothesis

<span style="color:#75715e"># Loss Function 정의 : Average ( ( F(x) - Y ) ^ 2 )</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss_fn</span>(hypothesis, features, labels):
    cost <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>square(hypothesis <span style="color:#f92672">-</span> labels))
    <span style="color:#66d9ef">return</span> cost

<span style="color:#75715e"># SGD Optimizer 선택 : Learning Rate 0.00005</span>
optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-5</span>)

<span style="color:#75715e"># Gradient Tape </span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad</span>(hypothesis, features, labels):
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
        <span style="color:#75715e"># Loss Value = Loss Function ( LinearRegression ( x ) , x, y )</span>
        loss_value <span style="color:#f92672">=</span> loss_fn(linearReg_fn(features),features,labels)
    <span style="color:#66d9ef">return</span> tape<span style="color:#f92672">.</span>gradient(loss_value, [W,b]), loss_value

<span style="color:#75715e"># Epochs 100회</span>
EPOCHS <span style="color:#f92672">=</span> <span style="color:#ae81ff">101</span>

<span style="color:#75715e"># For loop </span>
<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(EPOCHS):
    <span style="color:#75715e"># Dataset X, Y</span>
    <span style="color:#66d9ef">for</span> features, labels  <span style="color:#f92672">in</span> dataset:
        <span style="color:#75715e"># X 의 Type Cast</span>
        features <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(features, tf<span style="color:#f92672">.</span>float32)

        <span style="color:#75715e"># Y 의 Type Case</span>
        labels <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(labels, tf<span style="color:#f92672">.</span>float32)

        <span style="color:#75715e"># Hypothesis Value = Linear Regression Function ( X )</span>
        hypo_value <span style="color:#f92672">=</span> linearReg_fn(features)

        <span style="color:#75715e"># GradientDecenst Value</span>
        grads, loss_value <span style="color:#f92672">=</span> grad(linearReg_fn(features), features, labels)        
        optimizer<span style="color:#f92672">.</span>apply_gradients(grads_and_vars<span style="color:#f92672">=</span>zip(grads,[W,b]))    
    <span style="color:#75715e"># Log without Verbose</span>
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Iter: {}, Loss: {:.4f}, Prediction: {}&#34;</span><span style="color:#f92672">.</span>format(step, loss_value, hypo_value))

<span style="color:#75715e"># xy 정의</span>
xy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[<span style="color:#ae81ff">828.659973</span>, <span style="color:#ae81ff">833.450012</span>,  <span style="color:#ae81ff">908100</span>, <span style="color:#ae81ff">828.349976</span>, <span style="color:#ae81ff">831.659973</span>],
               [<span style="color:#ae81ff">823.02002</span>,  <span style="color:#ae81ff">828.070007</span>, <span style="color:#ae81ff">1828100</span>, <span style="color:#ae81ff">821.655029</span>, <span style="color:#ae81ff">828.070007</span>],
               [<span style="color:#ae81ff">819.929993</span>, <span style="color:#ae81ff">824.400024</span>, <span style="color:#ae81ff">1438100</span>, <span style="color:#ae81ff">818.97998</span>,  <span style="color:#ae81ff">824.159973</span>],
               [<span style="color:#ae81ff">816</span>,        <span style="color:#ae81ff">820.958984</span>, <span style="color:#ae81ff">1008100</span>, <span style="color:#ae81ff">815.48999</span>,  <span style="color:#ae81ff">819.23999</span>],
               [<span style="color:#ae81ff">819.359985</span>, <span style="color:#ae81ff">823</span>,        <span style="color:#ae81ff">1188100</span>, <span style="color:#ae81ff">818.469971</span>, <span style="color:#ae81ff">818.97998</span>],
               [<span style="color:#ae81ff">819</span>,        <span style="color:#ae81ff">823</span>,        <span style="color:#ae81ff">1198100</span>, <span style="color:#ae81ff">816</span>,        <span style="color:#ae81ff">820.450012</span>],
               [<span style="color:#ae81ff">811.700012</span>, <span style="color:#ae81ff">815.25</span>,     <span style="color:#ae81ff">1098100</span>, <span style="color:#ae81ff">809.780029</span>, <span style="color:#ae81ff">813.669983</span>],
               [<span style="color:#ae81ff">809.51001</span>,  <span style="color:#ae81ff">816.659973</span>, <span style="color:#ae81ff">1398100</span>, <span style="color:#ae81ff">804.539978</span>, <span style="color:#ae81ff">809.559998</span>]])

<span style="color:#75715e"># Normalization 정의 </span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">normalization</span>(data):
    <span style="color:#75715e"># data 를 인자로 받음</span>
    <span style="color:#75715e"># numerator = 개별 data - 열의 min값 </span>
    numerator <span style="color:#f92672">=</span> data <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>min(data, <span style="color:#ae81ff">0</span>)
    <span style="color:#75715e"># denominator = 열의 Max값 - 열의 Min값</span>
    denominator <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>max(data, <span style="color:#ae81ff">0</span>) <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>min(data, <span style="color:#ae81ff">0</span>)
    <span style="color:#75715e"># print(&#34;data : &#34;, data, &#34;np_min : &#34;, np.min(data,0), &#34;np_max : &#34;, np.max(data,0),&#34;num : &#34;, numerator, &#34;den : &#34;, denominator)</span>
    <span style="color:#75715e"># 열의 값을 Min 0 ~ Max 1의 비율로 Normalization</span>
    <span style="color:#66d9ef">return</span> numerator <span style="color:#f92672">/</span> denominator

<span style="color:#66d9ef">print</span>(xy)

xy <span style="color:#f92672">=</span> normalization(xy)

<span style="color:#66d9ef">print</span>(xy)

x_train <span style="color:#f92672">=</span> xy[:, <span style="color:#ae81ff">0</span>:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
y_train <span style="color:#f92672">=</span> xy[:, [<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]]

plt<span style="color:#f92672">.</span>plot(x_train, <span style="color:#e6db74">&#39;ro&#39;</span>)
plt<span style="color:#f92672">.</span>plot(y_train)

plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># dataset, Weight, Bias 정의</span>
dataset <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices((x_train, y_train))<span style="color:#f92672">.</span>batch(len(x_train))
W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">1</span>)), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32)
b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">1</span>,)), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32)

<span style="color:#75715e"># L2 Norm Function 정의</span>
<span style="color:#75715e"># https://junklee.tistory.com/29</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">l2_loss</span>(loss, beta <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>):
    <span style="color:#75715e"># Weight Regularization * Beta (Weight 이상치의 영향을 최소화하는 정규화)</span>
    W_reg <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>l2_loss(W) <span style="color:#75715e"># output = sum(t ** 2) / 2</span>
    loss <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(loss <span style="color:#f92672">+</span> W_reg <span style="color:#f92672">*</span> beta)
    <span style="color:#66d9ef">return</span> loss

<span style="color:#75715e"># Flag가 True일때만 L2 Norm 으로 Cost 정의</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss_fn2</span>(hypothesis, features, labels, flag <span style="color:#f92672">=</span> False):
    cost <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>square(hypothesis <span style="color:#f92672">-</span> labels))
    <span style="color:#66d9ef">if</span>(flag):
        cost <span style="color:#f92672">=</span> l2_loss(cost)
    <span style="color:#66d9ef">return</span> cost    

<span style="color:#75715e"># Learning Rate Decay True</span>
is_decay <span style="color:#f92672">=</span> True

<span style="color:#75715e"># 0.1</span>
starter_learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>

<span style="color:#75715e"># 50 Steap, 0.96, 이산 학습속도 감속 적용</span>
<span style="color:#75715e"># decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)</span>
<span style="color:#66d9ef">if</span>(is_decay):    
    learning_rate <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>schedules<span style="color:#f92672">.</span>ExponentialDecay(initial_learning_rate<span style="color:#f92672">=</span>starter_learning_rate,
                                                                  decay_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>,
                                                                  decay_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.96</span>,
                                                                  staircase<span style="color:#f92672">=</span>True)
    optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate)
<span style="color:#66d9ef">else</span>:
    optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate<span style="color:#f92672">=</span>starter_learning_rate)

<span style="color:#75715e"># Gradient Tape 정의</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad</span>(hypothesis, features, labels, l2_flag):
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
        loss_value <span style="color:#f92672">=</span> loss_fn2(linearReg_fn(features),features,labels, l2_flag)
    <span style="color:#66d9ef">return</span> tape<span style="color:#f92672">.</span>gradient(loss_value, [W,b]), loss_value

<span style="color:#75715e"># 100회</span>
EPOCHS <span style="color:#f92672">=</span> <span style="color:#ae81ff">101</span>

<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(EPOCHS):
    <span style="color:#66d9ef">for</span> features, labels  <span style="color:#f92672">in</span> dataset:
        <span style="color:#75715e"># X, Y Type Case</span>
        features <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(features, tf<span style="color:#f92672">.</span>float32)
        labels <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(labels, tf<span style="color:#f92672">.</span>float32)
        <span style="color:#75715e"># Gradient Tape</span>
        grads, loss_value <span style="color:#f92672">=</span> grad(linearReg_fn(features), features, labels, False)
        optimizer<span style="color:#f92672">.</span>apply_gradients(grads_and_vars<span style="color:#f92672">=</span>zip(grads,[W,b]))        
    <span style="color:#75715e"># Verbose 10</span>
    <span style="color:#66d9ef">if</span> step <span style="color:#f92672">%</span> <span style="color:#ae81ff">10</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Iter: {}, Loss: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(step, loss_value))

<span style="color:#75715e"># 100회</span>
EPOCHS <span style="color:#f92672">=</span> <span style="color:#ae81ff">101</span>

<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(EPOCHS):
    <span style="color:#66d9ef">for</span> features, labels  <span style="color:#f92672">in</span> dataset:
        <span style="color:#75715e"># X, Y Type Case</span>
        features <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(features, tf<span style="color:#f92672">.</span>float32)
        labels <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(labels, tf<span style="color:#f92672">.</span>float32)
        <span style="color:#75715e"># Gradient Tape</span>
        grads, loss_value <span style="color:#f92672">=</span> grad(linearReg_fn(features), features, labels, True)
        optimizer<span style="color:#f92672">.</span>apply_gradients(grads_and_vars<span style="color:#f92672">=</span>zip(grads,[W,b]))          
    <span style="color:#75715e"># Verbose 10</span>
    <span style="color:#66d9ef">if</span> step <span style="color:#f92672">%</span> <span style="color:#ae81ff">10</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Iter: {}, Loss: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(step, loss_value))
</code></pre></div></div>
    </div>
  </label>
</div>


<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-07-4</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf

tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>set_seed(<span style="color:#ae81ff">777</span>)  <span style="color:#75715e"># for reproducibility</span>
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)

<span style="color:#75715e"># MNIST Dataset 정의</span>
mnist <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>datasets<span style="color:#f92672">.</span>mnist

<span style="color:#75715e"># Train Data와 Test Data Load</span>
(x_train, y_train),(x_test, y_test) <span style="color:#f92672">=</span> mnist<span style="color:#f92672">.</span>load_data()
x_train, x_test <span style="color:#f92672">=</span> x_train <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.0</span>, x_test <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.0</span>

<span style="color:#75715e"># Kera Model Data</span>
model <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>Sequential([
  tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Flatten(),
  tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">512</span>, activation<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>relu),
  tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dropout(<span style="color:#ae81ff">0.2</span>),
  tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">10</span>, activation<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>softmax)
])

<span style="color:#75715e"># Model : Adam Optimizer / Cros Entropy Loss 선언</span>
model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>,
              loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;sparse_categorical_crossentropy&#39;</span>,
              metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>])

<span style="color:#75715e"># 5 Epoch 학습 수행</span>
model<span style="color:#f92672">.</span>fit(x_train, y_train, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)

<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">%</span>matplotlib inline

sample <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
image <span style="color:#f92672">=</span> x_test[sample]

fig <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>figure
plt<span style="color:#f92672">.</span>imshow(image, cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray_r&#39;</span>)
plt<span style="color:#f92672">.</span>show

num <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
images <span style="color:#f92672">=</span> x_train[:num]
labels <span style="color:#f92672">=</span> y_train[:num]

num_row <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>
num_col <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
fig, axes <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(num_row,num_col, figsize <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1.5</span><span style="color:#f92672">*</span>num_col,<span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>num_row))

<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num) :
  ax <span style="color:#f92672">=</span> axes[i<span style="color:#f92672">//</span>num_col, i<span style="color:#f92672">%</span>num_col]
  ax<span style="color:#f92672">.</span>imshow(images[i], cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray_r&#39;</span>)
  ax<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#39;Label: {}&#39;</span><span style="color:#f92672">.</span>format(labels[i]))

plt<span style="color:#f92672">.</span>tight_layout()
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># 모델 평가</span>
model<span style="color:#f92672">.</span>evaluate(x_test, y_test)

img_index <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
plt<span style="color:#f92672">.</span>imshow(x_test[img_index]<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">28</span>,<span style="color:#ae81ff">28</span>), cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray_r&#39;</span>)
pred <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(x_test[img_index]<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">28</span>,<span style="color:#ae81ff">28</span>,<span style="color:#ae81ff">1</span>))
<span style="color:#66d9ef">print</span>(pred<span style="color:#f92672">.</span>argmax())

num <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>
images <span style="color:#f92672">=</span> x_test[:num]
labels <span style="color:#f92672">=</span> y_test[:num]

num_row <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
num_col <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
fig, axes <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(num_row,num_col, figsize <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1.5</span><span style="color:#f92672">*</span>num_col,<span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>num_row))

<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num) :
  ax <span style="color:#f92672">=</span> axes[i<span style="color:#f92672">//</span>num_col, i<span style="color:#f92672">%</span>num_col]
  ax<span style="color:#f92672">.</span>imshow(images[i], cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray_r&#39;</span>)
  pred <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(x_test[i]<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">28</span>,<span style="color:#ae81ff">28</span>,<span style="color:#ae81ff">1</span>))  
  ax<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#39;Label: {}&#39;</span><span style="color:#f92672">.</span>format(pred<span style="color:#f92672">.</span>argmax()))

plt<span style="color:#f92672">.</span>tight_layout()
plt<span style="color:#f92672">.</span>show()

<span style="color:#66d9ef">print</span>(x_test<span style="color:#f92672">.</span>shape)

</code></pre></div></div>
    </div>
  </label>
</div>


<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-07-5</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">from</span> tensorflow <span style="color:#f92672">import</span> keras

tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>set_seed(<span style="color:#ae81ff">777</span>)  <span style="color:#75715e"># for reproducibility</span>
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)

<span style="color:#75715e"># fashion_mnist 선언</span>
fashion_mnist <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>datasets<span style="color:#f92672">.</span>fashion_mnist

<span style="color:#75715e"># train_data, train_label, test_data, test_label 선언</span>
(train_images, train_labels), (test_images, test_labels) <span style="color:#f92672">=</span> fashion_mnist<span style="color:#f92672">.</span>load_data()

<span style="color:#75715e"># 0 - T-shirt/Top</span>
<span style="color:#75715e"># 1 - Trouser</span>
<span style="color:#75715e"># 2 - Pullover</span>
<span style="color:#75715e"># 3 - Dress</span>
<span style="color:#75715e"># 4 - Coat</span>
<span style="color:#75715e"># 5 - Sandal</span>
<span style="color:#75715e"># 6 - Shirt</span>
<span style="color:#75715e"># 7 - Sneaker</span>
<span style="color:#75715e"># 8 - Bag</span>
<span style="color:#75715e"># 9 - Ankle Boot</span>
class_names <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;T-shirt/top&#39;</span>, <span style="color:#e6db74">&#39;Trouser&#39;</span>, <span style="color:#e6db74">&#39;Pullover&#39;</span>, <span style="color:#e6db74">&#39;Dress&#39;</span>, <span style="color:#e6db74">&#39;Coat&#39;</span>, <span style="color:#e6db74">&#39;Sandal&#39;</span>, <span style="color:#e6db74">&#39;Shirt&#39;</span>, <span style="color:#e6db74">&#39;Sneaker&#39;</span>, <span style="color:#e6db74">&#39;Bag&#39;</span>, <span style="color:#e6db74">&#39;Ankle boot&#39;</span>]

plt<span style="color:#f92672">.</span>figure()
plt<span style="color:#f92672">.</span>imshow(train_images[<span style="color:#ae81ff">0</span>])
plt<span style="color:#f92672">.</span>colorbar()
plt<span style="color:#f92672">.</span>grid(False)

num <span style="color:#f92672">=</span> <span style="color:#ae81ff">20</span>
images <span style="color:#f92672">=</span> train_images[:num]<span style="color:#f92672">/</span><span style="color:#ae81ff">255.0</span>
labels <span style="color:#f92672">=</span> train_labels[:num]<span style="color:#f92672">/</span><span style="color:#ae81ff">255.0</span>

num_col <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
num_row <span style="color:#f92672">=</span> int(num <span style="color:#f92672">/</span> num_col)

fig, axes <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(num_row, num_col, figsize <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1.5</span><span style="color:#f92672">*</span>num_col, <span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>num_row))

<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num) :
  ax <span style="color:#f92672">=</span> axes[i<span style="color:#f92672">//</span>num_col, i<span style="color:#f92672">%</span>num_col]
  ax<span style="color:#f92672">.</span>imshow(train_images[i], cmap<span style="color:#f92672">=</span>plt<span style="color:#f92672">.</span>cm<span style="color:#f92672">.</span>binary)
  ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;bottom&#39;</span>]<span style="color:#f92672">.</span>set_color(<span style="color:#e6db74">&#39;white&#39;</span>)
  ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;top&#39;</span>]<span style="color:#f92672">.</span>set_color(<span style="color:#e6db74">&#39;white&#39;</span>)
  ax<span style="color:#f92672">.</span>yaxis<span style="color:#f92672">.</span>label<span style="color:#f92672">.</span>set_color(<span style="color:#e6db74">&#39;white&#39;</span>)
  ax<span style="color:#f92672">.</span>xaxis<span style="color:#f92672">.</span>label<span style="color:#f92672">.</span>set_color(<span style="color:#e6db74">&#39;white&#39;</span>)
  ax<span style="color:#f92672">.</span>title<span style="color:#f92672">.</span>set_color(<span style="color:#e6db74">&#39;white&#39;</span>)
  ax<span style="color:#f92672">.</span>tick_params(axis<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;y&#39;</span>, colors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;white&#34;</span>)  
  ax<span style="color:#f92672">.</span>tick_params(axis<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;x&#39;</span>, colors<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;white&#39;</span>)
  ax<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#39;Label: {}&#39;</span><span style="color:#f92672">.</span>format(train_labels[i]))

plt<span style="color:#f92672">.</span>tight_layout()
plt<span style="color:#f92672">.</span>figure()
plt<span style="color:#f92672">.</span>show()

train_images <span style="color:#f92672">=</span> train_images <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.0</span>
test_images <span style="color:#f92672">=</span> test_images <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.0</span>

plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>,<span style="color:#ae81ff">10</span>))
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">25</span>):
    plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">5</span>,<span style="color:#ae81ff">5</span>,i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)
    plt<span style="color:#f92672">.</span>xticks([])
    plt<span style="color:#f92672">.</span>yticks([])
    plt<span style="color:#f92672">.</span>grid(True)
    plt<span style="color:#f92672">.</span>imshow(train_images[i], cmap<span style="color:#f92672">=</span>plt<span style="color:#f92672">.</span>cm<span style="color:#f92672">.</span>binary)
    plt<span style="color:#f92672">.</span>xlabel(class_names[train_labels[i]])

model <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>Sequential([
    keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Flatten(input_shape<span style="color:#f92672">=</span>(<span style="color:#ae81ff">28</span>, <span style="color:#ae81ff">28</span>)),
    keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">128</span>, activation<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>relu),
    keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">10</span>, activation<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>softmax)
])

model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>,
              loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;sparse_categorical_crossentropy&#39;</span>,
              metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>])
model<span style="color:#f92672">.</span>fit(train_images, train_labels, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)

test_loss, test_acc <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>evaluate(test_images, test_labels)

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Test accuracy:&#39;</span>, test_acc)
</code></pre></div></div>
    </div>
  </label>
</div>


<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-07-6</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf
<span style="color:#f92672">from</span> tensorflow <span style="color:#f92672">import</span> keras

tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>set_seed(<span style="color:#ae81ff">777</span>)  <span style="color:#75715e"># for reproducibility</span>
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)

<span style="color:#75715e"># 50,000 Movie Reviews IMDB (10000개의 빈도수가 높은 단어를 학습시 Vector에 사용)</span>
imdb <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>datasets<span style="color:#f92672">.</span>imdb

<span style="color:#75715e"># train_data, train_labels, test_data, test_labels</span>
(train_data, train_labels), (test_data, test_labels) <span style="color:#f92672">=</span> imdb<span style="color:#f92672">.</span>load_data(num_words<span style="color:#f92672">=</span><span style="color:#ae81ff">10000</span>)

<span style="color:#75715e"># train_data, train_labels</span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Training entries: {}, labels: {}&#34;</span><span style="color:#f92672">.</span>format(len(train_data), len(train_labels)))
<span style="color:#66d9ef">print</span>(train_data[<span style="color:#ae81ff">0</span>])

<span style="color:#75715e"># IMDB Data를 Vector을 실재 값으로 변환하여 출력</span>
<span style="color:#75715e"># A dictionary mapping words to an integer index</span>
<span style="color:#75715e"># 딕셔너리 단어를 인티져 인덱스로 맵핑</span>
word_index <span style="color:#f92672">=</span> imdb<span style="color:#f92672">.</span>get_word_index()

<span style="color:#75715e"># The first indices are reserved</span>
<span style="color:#75715e"># Data Check : Start 1 and &#39;the&#39;</span>
<span style="color:#66d9ef">for</span> k,v <span style="color:#f92672">in</span> word_index<span style="color:#f92672">.</span>items() :
  <span style="color:#75715e">#print(len(k))</span>
  <span style="color:#66d9ef">if</span> k <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;bad&#39;</span> : 
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;K : &#34;</span>,k, <span style="color:#e6db74">&#34; V : &#34;</span>, v<span style="color:#f92672">+</span><span style="color:#ae81ff">3</span>)
  <span style="color:#66d9ef">if</span> v <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span> :
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;K : &#34;</span>,k, <span style="color:#e6db74">&#34; V : &#34;</span>, v)
  <span style="color:#66d9ef">if</span> v <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span> :
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;K : &#34;</span>,k, <span style="color:#e6db74">&#34; V : &#34;</span>, v)

<span style="color:#75715e"># 최초 익덱스들은 Reserved : 0~3 예약어로 저장</span>
word_index <span style="color:#f92672">=</span> {k:(v<span style="color:#f92672">+</span><span style="color:#ae81ff">3</span>) <span style="color:#66d9ef">for</span> k,v <span style="color:#f92672">in</span> word_index<span style="color:#f92672">.</span>items()}
word_index[<span style="color:#e6db74">&#34;&lt;PAD&gt;&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
word_index[<span style="color:#e6db74">&#34;&lt;START&gt;&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
word_index[<span style="color:#e6db74">&#34;&lt;UNK&gt;&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>  <span style="color:#75715e"># unknown</span>
word_index[<span style="color:#e6db74">&#34;&lt;UNUSED&gt;&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>

reverse_word_index <span style="color:#f92672">=</span> dict([(value, key) <span style="color:#66d9ef">for</span> (key, value) <span style="color:#f92672">in</span> word_index<span style="color:#f92672">.</span>items()])

<span style="color:#75715e"># Word List : 0~3, 4~88587</span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;check : &#39;</span>, reverse_word_index<span style="color:#f92672">.</span>get(<span style="color:#ae81ff">0</span>,<span style="color:#e6db74">&#39;?&#39;</span>))
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;check : &#39;</span>, reverse_word_index<span style="color:#f92672">.</span>get(<span style="color:#ae81ff">88587</span>,<span style="color:#e6db74">&#39;?&#39;</span>))
<span style="color:#66d9ef">print</span>(len(reverse_word_index))

<span style="color:#75715e"># Review를 Decode</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">decode_review</span>(text):
    <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#39; &#39;</span><span style="color:#f92672">.</span>join([reverse_word_index<span style="color:#f92672">.</span>get(i, <span style="color:#e6db74">&#39;?&#39;</span>) <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> text])

check_data_id <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>

<span style="color:#66d9ef">print</span>(len(train_data[check_data_id]))

<span style="color:#75715e"># Train Data Value Print</span>
<span style="color:#66d9ef">print</span>(train_data[check_data_id])

<span style="color:#75715e"># Print label </span>
<span style="color:#66d9ef">print</span>(train_labels[check_data_id])

<span style="color:#75715e"># Train Data Decode Print</span>
decode_review(train_data[check_data_id])

<span style="color:#75715e"># 학습과 평가를 위해 동일길이인 256길이의 단어로 PAD값을 주어 맞춰줌 (뒤의 길이는 0값으로 맞춰줌)</span>
train_data <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>preprocessing<span style="color:#f92672">.</span>sequence<span style="color:#f92672">.</span>pad_sequences(train_data,
                                                        value<span style="color:#f92672">=</span>word_index[<span style="color:#e6db74">&#34;&lt;PAD&gt;&#34;</span>],
                                                        padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;post&#39;</span>,
                                                        maxlen<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>)

test_data <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>preprocessing<span style="color:#f92672">.</span>sequence<span style="color:#f92672">.</span>pad_sequences(test_data,
                                                       value<span style="color:#f92672">=</span>word_index[<span style="color:#e6db74">&#34;&lt;PAD&gt;&#34;</span>],
                                                       padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;post&#39;</span>,
                                                       maxlen<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>)

<span style="color:#66d9ef">print</span>(len(train_data[check_data_id]), len(test_data[check_data_id]))
<span style="color:#66d9ef">print</span>(train_data[check_data_id])

<span style="color:#75715e"># Tensorflow keras API를 통해 모델에 대한 정의</span>
<span style="color:#75715e"># 입력 Size와 학습시킬 Layer의 크기와 Activation Function 정의</span>
<span style="color:#75715e"># input shape is the vocabulary count used for the movie reviews (10,000 words)</span>
vocab_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">10000</span>

model <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>Sequential()
model<span style="color:#f92672">.</span>add(keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Embedding(vocab_size, <span style="color:#ae81ff">16</span>))
model<span style="color:#f92672">.</span>add(keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>GlobalAveragePooling1D())
model<span style="color:#f92672">.</span>add(keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">16</span>, activation<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>relu))
model<span style="color:#f92672">.</span>add(keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">1</span>, activation<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>sigmoid))

<span style="color:#75715e"># Model Summary 출력</span>
model<span style="color:#f92672">.</span>summary()

<span style="color:#75715e"># Adam Optimizer과 Cross Entropy Loss 선언</span>
model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>,
              loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;binary_crossentropy&#39;</span>,
              metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>])

<span style="color:#75715e"># 모델을 평가할 Test 데이타에 대한 정의(10000을 기준으로 학습과 평가 수행)</span>
<span style="color:#75715e"># 총 : 25000 평가 ~10000 / 학습 10001~</span>
x_val <span style="color:#f92672">=</span> train_data[:<span style="color:#ae81ff">10000</span>]
partial_x_train <span style="color:#f92672">=</span> train_data[<span style="color:#ae81ff">10000</span>:]

y_val <span style="color:#f92672">=</span> train_labels[:<span style="color:#ae81ff">10000</span>]
partial_y_train <span style="color:#f92672">=</span> train_labels[<span style="color:#ae81ff">10000</span>:]

history <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>fit(partial_x_train,
                    partial_y_train,
                    epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">40</span>,
                    batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>,
                    validation_data<span style="color:#f92672">=</span>(x_val, y_val),
                    verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)

results <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>evaluate(test_data, test_labels)
<span style="color:#66d9ef">print</span>(results)
</code></pre></div></div>
    </div>
  </label>
</div>


<div class="book-expand">
  <label>
    <div class="book-expand-head flex justify-between">
      <span>lab-09-1</span>
      <span>...</span>
    </div>
    <input type="checkbox" class="hidden" />
    <div class="book-expand-content markdown-inner">
       <div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">%</span>matplotlib inline
<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf

tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>set_seed(<span style="color:#ae81ff">777</span>)  <span style="color:#75715e"># for reproducibility</span>

<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)

<span style="color:#75715e"># XOR : 00 -&gt; 0, 01 -&gt; 1, 10 -&gt; 1, 11 -&gt; 0</span>
x_data <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>],[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>],[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>],[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>]]
y_data <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">0</span>]   ,   [<span style="color:#ae81ff">1</span>],   [<span style="color:#ae81ff">1</span>],   [<span style="color:#ae81ff">0</span>]]

plt<span style="color:#f92672">.</span>scatter(x_data[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>],x_data[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;red&#39;</span> , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)
plt<span style="color:#f92672">.</span>scatter(x_data[<span style="color:#ae81ff">3</span>][<span style="color:#ae81ff">0</span>],x_data[<span style="color:#ae81ff">3</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;orange&#39;</span> , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)
plt<span style="color:#f92672">.</span>scatter(x_data[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">0</span>],x_data[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;blue&#39;</span> , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)
plt<span style="color:#f92672">.</span>scatter(x_data[<span style="color:#ae81ff">2</span>][<span style="color:#ae81ff">0</span>],x_data[<span style="color:#ae81ff">2</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;green&#39;</span> , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)

plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;x1&#34;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;x2&#34;</span>)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># Tensorflow data API를 통해 학습시킬 값들을 담는다 (Batch Size는 한번에 학습시킬 Size로 정한다)</span>
<span style="color:#75715e"># preprocess function으로 features,labels는 실재 학습에 쓰일 Data 연산을 위해 Type를 맞춰준다</span>
dataset <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices((x_data, y_data))<span style="color:#f92672">.</span>batch(len(x_data))

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">preprocess_data</span>(features, labels):
    features <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(features, tf<span style="color:#f92672">.</span>float32)
    labels <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(labels, tf<span style="color:#f92672">.</span>float32)
    <span style="color:#66d9ef">return</span> features, labels

<span style="color:#75715e"># Weight Bias 초기화</span>
W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>zeros((<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">1</span>)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight&#39;</span>)
b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>zeros((<span style="color:#ae81ff">1</span>,)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias&#39;</span>)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;W = {}, B = {}&#34;</span><span style="color:#f92672">.</span>format(W<span style="color:#f92672">.</span>numpy(), b<span style="color:#f92672">.</span>numpy()))

<span style="color:#75715e"># Sigmoid를 가설로 선언</span>
<span style="color:#75715e"># 0과 1의 값만을 리턴 : tf.sigmoid(tf.matmul(X, W) + b)</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">logistic_regression</span>(features):
    hypothesis  <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>divide(<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">1.</span> <span style="color:#f92672">+</span> tf<span style="color:#f92672">.</span>exp(tf<span style="color:#f92672">.</span>matmul(features, W) <span style="color:#f92672">+</span> b))
    <span style="color:#66d9ef">return</span> hypothesis

<span style="color:#75715e"># Cost 함수 정의</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss_fn</span>(hypothesis, features, labels):
    cost <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>tf<span style="color:#f92672">.</span>reduce_mean(labels <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(logistic_regression(features)) <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> labels) <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> hypothesis))
    <span style="color:#66d9ef">return</span> cost

optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>)

<span style="color:#75715e"># Sigmoid 함수를 통해 예측값이 0.5보다 크면 1을 반환하고 0.5보다 작으면 0으로 반환</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">accuracy_fn</span>(hypothesis, labels):
    predicted <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(hypothesis <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0.5</span>, dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32)
    accuracy <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>cast(tf<span style="color:#f92672">.</span>equal(predicted, labels), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32))
    <span style="color:#66d9ef">return</span> accuracy

<span style="color:#75715e"># GradientTape을 통해 경사값을 계산</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad</span>(hypothesis, features, labels):
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
        loss_value <span style="color:#f92672">=</span> loss_fn(logistic_regression(features),features,labels)
    <span style="color:#66d9ef">return</span> tape<span style="color:#f92672">.</span>gradient(loss_value, [W,b])

<span style="color:#75715e"># epoch 1000회</span>
EPOCHS <span style="color:#f92672">=</span> <span style="color:#ae81ff">1001</span>

<span style="color:#75715e"># epoch에서 1000회 step</span>
<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(EPOCHS):
    <span style="color:#75715e"># x, y</span>
    <span style="color:#66d9ef">for</span> features, labels  <span style="color:#f92672">in</span> dataset:
        <span style="color:#75715e"># features, labels를 받아 float을 맞춤</span>
        features, labels <span style="color:#f92672">=</span> preprocess_data(features, labels)
        
        <span style="color:#75715e"># gradient tape</span>
        grads <span style="color:#f92672">=</span> grad(logistic_regression(features), features, labels)
        optimizer<span style="color:#f92672">.</span>apply_gradients(grads_and_vars<span style="color:#f92672">=</span>zip(grads,[W,b]))

        <span style="color:#75715e"># Verbose 100</span>
        <span style="color:#66d9ef">if</span> step <span style="color:#f92672">%</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Iter: {}, Loss: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(step, loss_fn(logistic_regression(features),features,labels)))

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;W = {}, B = {}&#34;</span><span style="color:#f92672">.</span>format(W<span style="color:#f92672">.</span>numpy(), b<span style="color:#f92672">.</span>numpy()))
x_data, y_data <span style="color:#f92672">=</span> preprocess_data(x_data, y_data)
test_acc <span style="color:#f92672">=</span> accuracy_fn(logistic_regression(x_data),y_data)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Testset Accuracy: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(test_acc))

plt<span style="color:#f92672">.</span>scatter(x_data[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>],x_data[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;red&#39;</span> , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)
plt<span style="color:#f92672">.</span>scatter(x_data[<span style="color:#ae81ff">3</span>][<span style="color:#ae81ff">0</span>],x_data[<span style="color:#ae81ff">3</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;orange&#39;</span> , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)
plt<span style="color:#f92672">.</span>scatter(x_data[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">0</span>],x_data[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;blue&#39;</span> , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)
plt<span style="color:#f92672">.</span>scatter(x_data[<span style="color:#ae81ff">2</span>][<span style="color:#ae81ff">0</span>],x_data[<span style="color:#ae81ff">2</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;green&#39;</span> , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)
plt<span style="color:#f92672">.</span>plot(x_data,logistic_regression(x_data), c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray&#39;</span>)

plt<span style="color:#f92672">.</span>tick_params(axis<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;y&#39;</span>, colors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;white&#34;</span>)  
plt<span style="color:#f92672">.</span>tick_params(axis<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;x&#39;</span>, colors<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;white&#39;</span>)

plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;x1&#34;</span>,color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;white&#34;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;x2&#34;</span>,color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;white&#34;</span>)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># W1,b1,W2,b2,W3,b3 정의</span>
W1 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight1&#39;</span>)
b1 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">1</span>,)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias1&#39;</span>)
W2 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight2&#39;</span>)
b2 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">1</span>,)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias2&#39;</span>)
W3 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight3&#39;</span>)
b3 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal((<span style="color:#ae81ff">1</span>,)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bias3&#39;</span>)

<span style="color:#75715e"># layer1 : sigmoid ─┬─&gt; layer3</span>
<span style="color:#75715e"># layer2 : sigmoid --┘</span>
<span style="color:#75715e"># hypothesis 가설</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">neural_net</span>(features):
    layer1 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>sigmoid(tf<span style="color:#f92672">.</span>matmul(features, W1) <span style="color:#f92672">+</span> b1)
    layer2 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>sigmoid(tf<span style="color:#f92672">.</span>matmul(features, W2) <span style="color:#f92672">+</span> b2)
    layer3 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>concat([layer1, layer2],<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
    layer3 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reshape(layer3, shape <span style="color:#f92672">=</span> [<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>])
    hypothesis <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>sigmoid(tf<span style="color:#f92672">.</span>matmul(layer3, W3) <span style="color:#f92672">+</span> b3)
    <span style="color:#66d9ef">return</span> hypothesis

<span style="color:#75715e"># 손실함수</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss_fn</span>(hypothesis, labels):
    cost <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>tf<span style="color:#f92672">.</span>reduce_mean(labels <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(hypothesis) <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> labels) <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> hypothesis))
    <span style="color:#66d9ef">return</span> cost

<span style="color:#75715e"># Optimizer SGD</span>
optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>)

<span style="color:#75715e"># Accuracy 측정</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">accuracy_fn</span>(hypothesis, labels):
    predicted <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(hypothesis <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0.5</span>, dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32)
    accuracy <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>cast(tf<span style="color:#f92672">.</span>equal(predicted, labels), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32))
    <span style="color:#66d9ef">return</span> accuracy

<span style="color:#75715e"># Gradient Tape</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad</span>(hypothesis, features, labels):
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
        loss_value <span style="color:#f92672">=</span> loss_fn(neural_net(features),labels)
    <span style="color:#66d9ef">return</span> tape<span style="color:#f92672">.</span>gradient(loss_value, [W1, W2, W3, b1, b2, b3])

<span style="color:#75715e"># epoch 50000</span>
EPOCHS <span style="color:#f92672">=</span> <span style="color:#ae81ff">50000</span>

<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(EPOCHS):
    <span style="color:#66d9ef">for</span> features, labels  <span style="color:#f92672">in</span> dataset:
        <span style="color:#75715e"># preprocess 처리</span>
        features, labels <span style="color:#f92672">=</span> preprocess_data(features, labels)
        
        <span style="color:#75715e"># neural_net : layer1 + layer2 -&gt; layer3</span>
        grads <span style="color:#f92672">=</span> grad(neural_net(features), features, labels)

        <span style="color:#75715e"># optimizer</span>
        optimizer<span style="color:#f92672">.</span>apply_gradients(grads_and_vars<span style="color:#f92672">=</span>zip(grads,[W1, W2, W3, b1, b2, b3]))

        <span style="color:#75715e"># verbose 5000</span>
        <span style="color:#66d9ef">if</span> step <span style="color:#f92672">%</span> <span style="color:#ae81ff">5000</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Iter: {}, Loss: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(step, loss_fn(neural_net(features),labels)))

x_data, y_data <span style="color:#f92672">=</span> preprocess_data(x_data, y_data)

<span style="color:#75715e"># test data</span>
test_acc <span style="color:#f92672">=</span> accuracy_fn(neural_net(x_data),y_data)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Testset Accuracy: {:.4f}&#34;</span><span style="color:#f92672">.</span>format(test_acc))

plt<span style="color:#f92672">.</span>scatter(x_data[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>],x_data[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;red&#39;</span> , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)
plt<span style="color:#f92672">.</span>scatter(x_data[<span style="color:#ae81ff">3</span>][<span style="color:#ae81ff">0</span>],x_data[<span style="color:#ae81ff">3</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;orange&#39;</span> , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)
plt<span style="color:#f92672">.</span>scatter(x_data[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">0</span>],x_data[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;blue&#39;</span> , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)
plt<span style="color:#f92672">.</span>scatter(x_data[<span style="color:#ae81ff">2</span>][<span style="color:#ae81ff">0</span>],x_data[<span style="color:#ae81ff">2</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;green&#39;</span> , marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;^&#39;</span>)

plt<span style="color:#f92672">.</span>plot(x_data,neural_net(x_data), c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray&#39;</span>)

plt<span style="color:#f92672">.</span>tick_params(axis<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;y&#39;</span>, colors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;white&#34;</span>)  
plt<span style="color:#f92672">.</span>tick_params(axis<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;x&#39;</span>, colors<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;white&#39;</span>)

plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;x1&#34;</span>,color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;white&#34;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;x2&#34;</span>,color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;white&#34;</span>)
plt<span style="color:#f92672">.</span>show()

</code></pre></div></div>
    </div>
  </label>
</div>

<br>
</article>

            
            

            <footer class="book-footer">
                 <div class="flex flex-wrap justify-between">





</div>
 
                
                
            </footer>

             
<div class="book-comments">
<br>
<br>
<hr> </hr>
<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "prokoptasis" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a></div>
 
            

            <label for="menu-control" class="hidden book-menu-overlay"></label>
        </div>

        
    </main>

    
</body>

</html>

     