<!DOCTYPE html>
<html lang="ko" dir=>

<head>
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="머신러닝/딥러닝 학습 자료">
<meta name="theme-color" content="#FFFFFF"><meta property="og:title" content="Machine Learning" />
<meta property="og:description" content="머신러닝/딥러닝 학습 자료" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://prokoptasis.github.io/docs/documents/deeplearning/machinelearning/" />

<title>Machine Learning | Prokoptasis</title>
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="/book.min.001ba32d1ef4d2469d055e28d66fd96b16bfeddee698f1e2729af3a54ed86b8b.css" integrity="sha256-ABujLR700kadBV4o1m/Zaxa/7d7mmPHicprzpU7Ya4s=">
<script defer src="/ko.search.min.5212fde38b3c9004e8e0de12806c17dff341e423cbad9a358824afb0cbd7ab33.js" integrity="sha256-UhL944s8kATo4N4SgGwX3/NB5CPLrZo1iCSvsMvXqzM="></script>

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-176412981-2', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
<link rel="alternate" type="application/rss+xml" href="http://prokoptasis.github.io/docs/documents/deeplearning/machinelearning/index.xml" title="Prokoptasis" />
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
 
</head>

<body dir=>
    <input type="checkbox" class="hidden toggle" id="menu-control" />
    <input type="checkbox" class="hidden toggle" id="toc-control" />
    <main class="container flex">
        <aside class="book-menu">
             <nav>
<h2 class="book-brand" style="text-align: center;">
    <a href="/"><span>Prokoptasis</span>
  </a>
</h2>
<hr>

<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>





 


  



  
  <ul>
    
      
        <li class="book-section-flat" >
          
  
  

  
    <span>Documents</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-bc65bd57d9bab5039e9a79b5ae591c72" class="toggle"  />
    <label for="section-bc65bd57d9bab5039e9a79b5ae591c72" class="flex justify-between">
      <a href="http://prokoptasis.github.io/docs/documents/projects/" class="">Projects</a>
      <span>▾</span>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/projects/hugogirl/" class="">hU-Go-Girl</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-53481227639f164d3f7ead2411c43315" class="toggle"  />
    <label for="section-53481227639f164d3f7ead2411c43315" class="flex justify-between">
      <a href="http://prokoptasis.github.io/docs/documents/frontend/" class="">Front End</a>
      <span>▾</span>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/frontend/markdown/" class="">Markdown</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-7cff03fc158d9929dd882d45e5e83f84" class="toggle"  />
    <label for="section-7cff03fc158d9929dd882d45e5e83f84" class="flex justify-between">
      <a href="http://prokoptasis.github.io/docs/documents/backend/" class="">Back End</a>
      <span>▾</span>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-0480d4c0a3a48973d66dda9e8e9d50da" class="toggle"  />
    <label for="section-0480d4c0a3a48973d66dda9e8e9d50da" class="flex justify-between">
      <a href="http://prokoptasis.github.io/docs/documents/backend/rust/" class="">Rust</a>
      <span>▾</span>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/backend/rust/rust01/" class="">Rust Basic 01</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/backend/rust/rust02/" class="">Rust Basic 02</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-0d89b8a13399f7331f6db819807b653c" class="toggle"  />
    <label for="section-0d89b8a13399f7331f6db819807b653c" class="flex justify-between">
      <a href="http://prokoptasis.github.io/docs/documents/backend/go/" class="">Go</a>
      <span>▾</span>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/backend/go/go01/" class="">GO Basic 1</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/backend/go/go02/" class="">GO Basic 2</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-47bd3d4bf94d414a097efcbbc9ad3f79" class="toggle"  />
    <label for="section-47bd3d4bf94d414a097efcbbc9ad3f79" class="flex justify-between">
      <a href="http://prokoptasis.github.io/docs/documents/backend/sql/" class="">SQL</a>
      <span>▾</span>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/backend/sql/sql01/" class="">SQL Basic 1</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/backend/sql/sql02/" class="">SQL Basic 2</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/backend/sql/sql03/" class="">SQL Basic 3</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-08315e14cb91d15111d683212f692de5" class="toggle" checked />
    <label for="section-08315e14cb91d15111d683212f692de5" class="flex justify-between">
      <a href="http://prokoptasis.github.io/docs/documents/deeplearning/" class="">Deep Learning</a>
      <span>▾</span>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/deeplearning/machinelearning/" class=" active">Machine Learning</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/deeplearning/tensorflow/" class="">Tensorflow</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-ae112940aeb03e8cb2e77b93d2c01134" class="toggle"  />
    <label for="section-ae112940aeb03e8cb2e77b93d2c01134" class="flex justify-between">
      <a href="http://prokoptasis.github.io/docs/documents/english/" class="">English</a>
      <span>▾</span>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/english/pickedup/" class="">Picked Up English</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="http://prokoptasis.github.io/docs/documents/english/seulsam/" class="">Seulsam</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>










 
<ul>
    
    <li>
        <a href="/posts/" >
        Blog
      </a>
    </li>
    
    <li>
        <a href="mailto:c.exigua@gmail.com" target="_blank" rel="noopener" >
        Gmail
      </a>
    </li>
    
    <li>
        <a href="https://github.com/prokoptasis/prokoptasis" target="_blank" rel="noopener" >
        Github
      </a>
    </li>
    
    <li>
        <a href="https://twitter.com/prokoptasis" target="_blank" rel="noopener" >
        Twitter
      </a>
    </li>
    
    <li>
        <a href="https://www.facebook.com/c.exigua/" target="_blank" rel="noopener" >
        Facebook
      </a>
    </li>
    
</ul>
<br>
<hr>
<div style="display: block; text-align: center;">
    <script type="text/javascript" src="https://cdnjs.buymeacoffee.com/1.0.0/button.prod.min.js" data-name="bmc-button" data-slug="prokoptasis" data-color="#e9e9e9" data-emoji="" data-font="Arial" data-text="" data-outline-color="#000" data-font-color="#000"
        data-coffee-color="#6f4e37"></script>
</div>
  

</nav>




  <script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script>

 
            
        </aside>

        <div class="book-page">
            <header class="book-header">
                 <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>Machine Learning</strong>

  <label for="toc-control">
    
  </label>
</div>
  
                
            </header>

            
<article class="markdown"><h2 id="machine-learning">
  Machine Learning
  <a class="anchor" href="#machine-learning">#</a>
</h2>
<hr>
<h3 id="introduction">
  Introduction
  <a class="anchor" href="#introduction">#</a>
</h3>
<p>머신 러닝 관련 학습 자료들을 정리한 내용입니다.</p>
<h3 id="machine-learning-분류httpsopentutorialsorgcourse454828934">
  <a href="https://opentutorials.org/course/4548/28934" target="_blank" rel="noopener">Machine Learning 분류</a>
  <a class="anchor" href="#machine-learning-%eb%b6%84%eb%a5%98httpsopentutorialsorgcourse454828934">#</a>
</h3>
<p>머신 러닝은 지도학습, 비지도학습, 강화학습으로 분류하며 지도학습은 다시 회귀와 분류, 비지도 학습은 군집화,변환,연관으로 분류할 수 있습니다. 인터넷 상에 다양한 설명이 있습니다만 그림으로 직관적으로 분류 할 수 있는 egoing님의 자료를 참조하였습니다. 세부내용은 제목에 링크된 위키피디어를 참조해보시기 바랍니다.</p>
<ol>
<li><a href="https://en.wikipedia.org/wiki/Supervised_learning" target="_blank" rel="noopener">Supervised Learning</a>
<ul>
<li>Regression</li>
<li>Classification</li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/Unsupervised_learning" target="_blank" rel="noopener">Unsupervised Learning</a>
<ul>
<li>Clustering</li>
<li>Transform</li>
<li>Association</li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/Reinforcement_learning" target="_blank" rel="noopener">Reinforcement Learning</a></li>
</ol>
<h3 id="linear-regression1">
  Linear Regression<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>
  <a class="anchor" href="#linear-regression1">#</a>
</h3>
<p>Regression이란 &ldquo;Regression toward mean&quot;으로 특정한 데이터는 전체의 평균으로 회귀하려는 속성을 나타내며 일정 데이터의 분포를 가장 잘 설명 할 수 있는 직선의 방정식을 찾아내는 것을 나타냅니다. 말로는 설명이 아렵고 아래처럼 분포된 데이터의 점으로 표현되는 데이터들을 가장 잘 표현할 수 있는 직선을 찾는 것을 의미합니다.</p>
 <div><img width="600px" src="../../../../../201020_01_ml_linear_regression.png" alt="linear_regression1" ></div>
<!-- ![Linear Regression](../../../../../201020_01_ml_linear_regression.png) -->
<!-- <div><img width="600px" src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/1920px-Linear_regression.svg.png" alt="linear_regression1" ></div> -->
<p>위의 Linear Regression에서 직선의 방정식은 다음과 같이 나타낼 수 있습니다.</p>
<blockquote>
<p><U>Regression</U> <br><br>

<link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script><span>
  \( H(X) = WX&#43;B ( y = ax &#43; b ) \)
</span>
 <br><br>
<span>
  \( H : Hypothesis (가설)  \)
</span>
 <br><br>
<span>
  \( W : Weight (기울기)  \)
</span>
 <br><br>
<span>
  \( B : Bias (절편)  \)
</span>
 <br><br></p>
</blockquote>
<p>그렇다면 데이터를 가장 잘 대변할 수 있는 직선은 무엇일까요? 아래 점으로 나타나는 데이터와 직선의 차이의 합이 가장 작을 수록 이들을 가장 잘 표현할 수 있다고 할 수 있습니다. 이러한 차이를 Cost라 하며 음수 차이와 양수 차이의 합이 상쇄되는 것을 막기 위해 제곱해서 사용하고 있습니다.</p>
<div><img width="600px" src="../../../../../201020_02_ml_gradient_descent.png" alt="gradient_descent" ></div>
<!-- ![x^2](../../../../../201020_02_ml_gradient_descent.png201020_02_ml_gradient_descent.png) -->
<!-- <div>
<img width="250px" src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/53/Linear_least_squares_example2.png/800px-Linear_least_squares_example2.png" alt="linear_regression2" >
</div> -->
<blockquote>
<p><U>Cost</U> <br><br>
<span>
  \( Cost : H(x) - y \)
</span>
 <br><br>
<span>
  \( Total Cost : \frac{(H(x_1)-y_1)^2 &#43; (H(x_2)-y_2)^2 &#43; ... &#43; (H(x_n)-y_n)^2 }{n} \)
</span>
 <br><br>
<span>
  \( Total Cost : cost(W) = \frac{1}{n}\, \textstyle\sum_{i=1}^n \,(Wx_i-y)^2 \)
</span>
 <br><br>
<span>
  \( Cost Function : cost(W,b) = \frac{1}{n}\, \textstyle\sum_{i=1}^n \,(H(x_i)-y_i)^2 \)
</span>
 <br><br>
<span>
  \( Goal : minimize_w,_b cost(W,b) \)
</span>
 <br><br></p>
</blockquote>
<p>위 식에서와 같이 Goal은 가설과 실제 데이터의 차이의 제곱이 최소가 되는 기울기와 절편은 찾는 문제가 됩니다.</p>
<h3 id="gradient-descent">
  Gradient Descent
  <a class="anchor" href="#gradient-descent">#</a>
</h3>
<p>위의 Cost Function에서 임의 점에서 시작하여 학습률 만큼 기울기가 낮은 쪽으로 진해하며 최적의 값을 찾는 방식을 경사하강법이라고 합니다. 임의의 점에서 시작하여 Learning Rate 만큼 미분을 진행하며 최소점을 찾는 방식으로 Convext 상황에서는 잘 작동하게 됩니다.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></p>
<div><img width="600px" src="../../../../../201020_03_ml_minimize_goal.png" alt="minimize_goal" ></div>
<!-- ![x^2](../../../../../201020_03_ml_minimize_goal.png) -->
<!-- <div>
<img width="400px" src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Gradient_descent.svg/1024px-Gradient_descent.svg.png" alt="linear_regression3" >
</div> -->
<blockquote>
<p><U>Gradient Descent</U> <br><br>
<span>
  \(  W := W - \alpha \frac{\partial}{\partial W} \, \frac{1}{2m} \textstyle\sum_{i=1}^m (W(x_i)-y_i)^2  \)
</span>
 <br><br>
<span>
  \(  W := W - \alpha \frac{1}{2m} \, \textstyle\sum_{i=1}^m 2(W(x_i)-y_i)X_i  \)
</span>
 <br><br>
<span>
  \(  W := W - \alpha \frac{1}{m} \, \textstyle\sum_{i=1}^m (W(x_i)-y_i)X_i  \)
</span>
 <br><br>
<span>
  \(  W := W - \alpha \frac{\partial}{\partial W} \, cost(W) \)
</span>
 <br> <br></p>
</blockquote>
<p>위의 식을 보면 W에서 학습률*W에 편미분된 COST를 빼면서 경사 하강을 진행하게 됩니다. 오차의 기울기에 대한 편미분이 0이 되면 오차가 최소인 지점에 도달하며 최적해을 찾을 수 있게 됩니다. 이에 대한 수학적 유도는 다른 형태로 정리해보겠습니다.</p>
<h3 id="derivative">
  Derivative
  <a class="anchor" href="#derivative">#</a>
</h3>
<p>함수 f(x)에 대한 미분은 아래와 같습니다. 의미는 x의 변화량이 0으로 수렴할때 y의 변화량이 어떻게 되는지를 구하는 것을 미분이라 합니다. 바꿔말하면 함수 f(x)는 입력 x에 얼마나 민감하게 반응 하는지를 구한다는 이야기입니다.</p>
<blockquote>
<p><U>Derivative</U> <br><br>
<span>
  \(  f&#39;(x) = \frac{\Delta f(x)}{\Delta x} = \lim\limits_{\Delta x \rightarrow 0 } \, \frac{f(x&#43;\Delta x)-f(x)}{\Delta x}   \)
</span>
 <br><br></p>
</blockquote>
<p>고교수학과정입니다만 저처럼 기초가 없으신 분들을 위해 기본 풀이 부터 정리해 놓았습니다.</p>
<blockquote>
<p><U>Derivative Basic</U> <br><br>
<span>
  \(  f(x) = constant -&gt; f&#39;(x) = 0 \)
</span>
 <br><br>
<span>
  \(  f&#39;(x) = e ^ x =&gt; f&#39;(x) = e ^ x \)
</span>
 <br><br>
<span>
  \(  f&#39;(x) = e ^ -x =&gt; f&#39;(x) = -e ^ -x \)
</span>
 <br><br>
<span>
  \(  f&#39;(x) = ax ^ x =&gt; f&#39;(x) = nax ^ {n-1} \)
</span>
 <br><br>
<span>
  \(  f&#39;(x) = ln ^ x =&gt; f&#39;(x) = \frac{1}{x} \)
</span>
 <br><br>
<span>
  \(  \frac{1}{x} = x^{-1}  \)
</span>
 <br><br></p>
</blockquote>
<p>다시 미분의 문제로 돌아가서 아래와 같을 경우 미분의 의미를 살펴보겠습니다.</p>
<div><img width="600px" src="../../../../../201020_04_ml_power_x_2.gif" alt="power_x_2" ></div>
<!-- ![x^2](../../../../../201020_04_ml_power_x_2.gif) -->
<blockquote>
<p><span>
  \(  f(x) = x^2 일 경우 f&#39;(x) = 2 * x^ {2-1} = 2x \)
</span>
 <br><br>
<span>
  \(  f(2) = 2^2 = 4 일 경우 f&#39;(2) = 2*2 = 4  \)
</span>
 <br><br>
<span>
  \(  f(2) = 1^2 = 1 일 경우 f&#39;(2) = 2*1 = 2 \)
</span>
 <br><br>
<span>
  \(  f(2) = 0^2 = 0 일 경우 f&#39;(2) = 2*0 = 0 \)
</span>
 <br><br></p>
</blockquote>
<p>즉 x = 2 일때 미분 f(x)는 4의 변화를 가지게 됩니다.<br>
즉 x = 1 일때 미분 f(x)는 2의 변화를 가지게 됩니다.<br>
즉 x = 0 일때 미분 f(x)는 0의 변화를 가지게 됩니다.</p>
<p>즉 x의 한점에서 y의 변화량을 통해 위의 x^2이 Cost라 가정하다고 반복한다면 Cost가 최저인 점을 구할 수 있습니다. 즉 Cost가 최소가 되는 최적해를 구할 수 있게 됩니다.</p>
<blockquote>
<p>Practice 1 <br><br>
<span>
  \(  f(x) = 3x^2 &#43; e^x &#43; 7 의 f&#39;(x) ? \)
</span>
 <br><br>
<span>
  \(       = 3*2*x^{2-1} &#43; e^x &#43; 0 = 6x &#43; e^x \)
</span>
 <br><br>
Practice 2 <br><br>
<span>
  \(  f(x) = lnx &#43; \frac{1}{x} 의 f&#39;(x) ? \)
</span>
 <br><br>
<span>
  \(       = \frac{1}{x} - \frac{1}{x^2} \)
</span>
 <br><br></p>
</blockquote>
<p>다음은 입력변수가 하나 이상인 다변수 함수에서 사용하는 편미분에 대해 살펴 보겠습니다. 다변수 함수에서는 미분하고자 하는 변수 하나를 제외한 나머지 변수들은 상수로 취급하고 해당 변수를 미분하는 것을 의미합니다.</p>
<blockquote>
<p><U>Partial Derivative</U> <br><br>
Practice 1 <br><br>
<span>
  \(  f(x,y) = 2x &#43; 3xy &#43; y^3 \space 변수 \space x에 \space 대해 \space 편미분 \)
</span>
 <br><br>
<span>
  \(  = \frac{\partial f(x,y)}{\partial x} = \frac{\partial(2x&#43;3xy&#43;y^3)}{\partial x} \)
</span>
 <br><br>
<span>
  \(  = \frac{\partial 2x}{\partial x} &#43; \frac{\partial 3xy}{\partial x} &#43; \frac{\partial y^3}{\partial x} \)
</span>
 <br><br>
<span>
  \(  = 2 &#43; 3y &#43; 0 \)
</span>
 <br><br>
<span>
  \(  = 2 &#43; 3y \)
</span>
 <br><br>
<span>
  \(  f(x,y) = 2x &#43; 3xy &#43; y^3 \space 변수 \space y에 \space 대해 \space 편미분 \)
</span>
 <br><br>
<span>
  \(  = \frac{\partial f(x,y)}{\partial y} = \frac{\partial(2x&#43;3xy&#43;y^3)}{\partial y} \)
</span>
 <br><br>
<span>
  \(  = \frac{\partial 2x}{\partial y} &#43; \frac{\partial 3xy}{\partial y} &#43; \frac{\partial y^3}{\partial y} \)
</span>
 <br><br>
<span>
  \(  = 0 &#43; 3x &#43; 3y^2 \)
</span>
 <br><br>
<span>
  \(  = 3x &#43; 3y^2 \)
</span>
 <br><br></p>
</blockquote>
<p>여러 함수로 구성된 합성 함수를 미분하기 위한 방법으로 Chain Rule이 있습니다. 여러 함수를 결합된 함수 중 특정 함수를 치환하여 약분 개념을 적용하여 미분하는 방식입니다. 두 함수가 곱해져 있는 형태의 미분은 단순 미분의 곱이 아니며 Product Rule이 성립합니다.</p>
<blockquote>
<p><U>Chain rule</U> <br><br>
<span>
  \(  \frac{\partial f}{\partial x} = \frac{\partial f}{\partial t} \circ \frac{\partial t}{\partial x} \)
</span>
 <br><br>
Product rule <br><br>
<span>
  \(  \frac{\partial}{\partial x}[f(x)g(x)]=f(x)g&#39;(x)&#43;f&#39;(x)g(x)  \)
</span>
<br><br>
Practice 1 <br><br>
<span>
  \(  f(x) = e ^ {3x^2} \space \space \space e^t , \space t = 3x^2 \)
</span>
<br><br>
<span>
  \(  \frac{\partial f}{\partial x} = \frac{\partial f}{\partial t} \circ \frac{\partial t}{\partial x} = \frac{\partial (e^t)}{\partial t} \circ \frac{\partial (3x^2)}{\partial x} = (e^t)(6x) = (e^{3x^2})(6x) = 6xe^{3x2} \)
</span>
 <br><br>
<span>
  \(  f(x) = e^{-x} \space \space \space e^t , \space t = -x  \)
</span>
<br><br>
Practice 2 <br><br>
<span>
  \(  \frac{\partial f}{\partial x} = \frac{\partial f}{\partial t} \circ \frac{\partial t}{\partial x} = \frac{\partial (e^t)}{\partial t} \circ \frac{\partial (-x)}{\partial x} = (e^t)(-1) = (e^{-x})(-1) = -e^{-x} \)
</span>
 <br><br>
Practice 3 <br><br>
<span>
  \(  f(x) = 3xe^x  \)
</span>
 <br><br>
<span>
  \(  \frac{\partial}{\partial x}(3xe^x) = 3 \Big( \frac{\partial}{\partial x}(e^x x) \Big) \)
</span>
 <br><br>
<span>
  \( Product \space Rule을 \space 적용 \space \frac{\partial}{\partial x}(u v) = v \frac{\partial u}{\partial x} &#43; u \frac{\partial v}{\partial x} \space 다음으로 치환 \space u = e^x \space v = x  \)
</span>
 <br><br>
<span>
  \( 3 \Big( x \frac{\partial}{\partial x}(e^x) &#43; e^x \frac {\partial}{\partial x}(x) \Big) \)
</span>
 <br><br>
<span>
  \( Chain \space Rule을 \space 적용 \space \frac{\partial}{\partial x}(e^x) = \frac {\partial e^u}{\partial u} \frac{\partial u}{\partial x} \space 다음으로 \space 치환 \space u = x, \space \frac{\partial}{\partial u}(e^u)=e^u \)
</span>
 <br><br>
<span>
  \( = 3 \Big( x e^x \frac{\partial}{\partial x}(x)  &#43; e^x  \frac{\partial}{\partial x}(x) \Big) \)
</span>
 <br><br>
<span>
  \( = 3 \Big( x e^x 1 &#43; e^x \frac{\partial}{\partial x}(x) \Big) \)
</span>
 <br><br>
<span>
  \( = 3 \Big( x e^x &#43; e^x 1 \Big) \)
</span>
 <br><br>
<span>
  \( = 3 ( x e^x &#43; e^x ) \)
</span>
 <br><br>
<span>
  \( = 3 e^x( x &#43; 1 ) \)
</span>
 <br><br></p>
</blockquote>
<h3 id="loss-function">
  Loss Function
  <a class="anchor" href="#loss-function">#</a>
</h3>
<p>미분의 매력에 흠뻑 빠지셨다면 이제 다시 원점으로 돌아가서 Linear Regression에 있어 Loss Function에 대해 이야기해보겠습니다. Linear Regression은 실측되는 데이터와 이를 표현하는 직선의 방정식 f(x) = wx+b의 error가 최소가 되어야 합니다. 즉 Error가 최소가 되는 W,B를 연속적으로 탐색하는 과정에서 이를 측정하기 위해 사용되는 것이 손실함수입니다.</p>
<div><img width="600px" src="../../../../../201020_02_ml_gradient_descent.png" alt="gradient_descent" ></div>
<!-- ![Error](../../../../../201020_02_ml_gradient_descent.png) -->
<blockquote>
<p><U>Loss Function</U> <br><br>
<span>
  \(  Loss Funtion = \frac{(t_1-y_1)^2 &#43; (t_2-y_2)^2 &#43; ... &#43; (t_n-y_n)^2}{n} \)
</span>
 <br><br>
<span>
  \(  = \frac{[t_1-(Wx_1&#43;b)]^2 &#43; [t_2-(Wx_2&#43;b)]^2 &#43; ... &#43; [t_n-(Wx_n&#43;b)]^2}{n} \)
</span>
 <br><br>
<span>
  \(  = \frac{1}{n} \, \displaystyle\sum_{i=1}^n \, [t_i-(Wx_i&#43;b)]^2 \)
</span>
 <br><br></p>
</blockquote>
<div><img width="600px" src="../../../../../201020_02_ml_gradient_descent.png" alt="gradient_descent" ></div>
<p>Gradient Descent 알고리즘도 다시 보겠습니다. 원 기울기에서 학습률*미분만큼을 감소시키며 기울기가 0이 되는 지점 즉 오차가 최소화되는 지점으로 진행하기 위한 수식임을 알 수 있습니다.</p>
<blockquote>
<p><U>Gradient Descent</U> <br><br>
<span>
  \(  W := W - \alpha \frac{\partial}{\partial W} \, \frac{1}{2m} \textstyle\sum_{i=1}^m (W(x_i)-y_i)^2  \)
</span>
 <br><br>
<span>
  \(  W := W - \alpha \frac{1}{2m} \, \textstyle\sum_{i=1}^m 2(W(x_i)-y_i)X_i  \)
</span>
 <br><br>
<span>
  \(  W := W - \alpha \frac{1}{m} \, \textstyle\sum_{i=1}^m (W(x_i)-y_i)X_i  \)
</span>
 <br><br>
<span>
  \(  W := W - \alpha \frac{\partial}{\partial W} \, cost(W) \)
</span>
 <br> <br>
<span>
  \(  b := b - \alpha \frac{\partial}{\partial b} \, cost(b) \)
</span>
 <br> <br></p>
</blockquote>
<p>여기서 학습률은 최적해를 향한 진행 폭을 결정하게 됩니다. 작게 잡으면 성능이 저하되고 크게 잡으면 학습이 잘 진행되지 않을 수 있습니다.</p>
<div><img width="600px" src="../../../../../201020_05_ml_learning_rate.png" alt="learning_rate" ></div>
<!-- ![Learning Rate](../../../../../201020_05_ml_learning_rate.png) -->
<blockquote>
<p><U>Learning Rate</U> <br><br>
<span>
  \(  W = W - \alpha \, \frac{\partial E(W,b)}{\partial W}  \)
</span>
<br> <br></p>
</blockquote>
<div><img width="600px" src="../../../../../201020_06_ml_convex_function.png" alt="convex_function" ></div>
<p>이론적으로 Convex Function이라면 가장 잘 최적해를 찾을 수 있습니다. 하지만 문제에 따라 Convex가 아니라면 Parameter에 따라 적절치 위치에서 학습이 종료될 수도 있습니다.</p>
<div><img width="600px" src="../../../../../201020_07_ml_local_minima.png" alt="local_minima" ></div>
<h3 id="dot-product">
  Dot Product
  <a class="anchor" href="#dot-product">#</a>
</h3>
<p>Linear Regression 문제에 혜성처럼 행렬이 등장합니다. 다음과 같은 방정식에서 뭐든 요소들의 오차들은 행렬곱을 통해 한번에 계산될 수 있습니다.</p>
<blockquote>
<p><U>Dot Product</U> <br><br>
<span>
  \(  x_1 * W &#43; b_1 = y_1 \)
</span>
<br><br>
<span>
  \(  x_2 * W &#43; b_2 = y_2 \)
</span>
<br><br>
<span>
  \(  x_3 * W &#43; b_3 = y_3 \)
</span>
<br><br>
<span>
  \(  x_4 * W &#43; b_4 = y_4 \)
</span>
<br><br>
<span>
  \(  x_5 * W &#43; b_5 = y_5 \)
</span>
<br><br>
행렬의 곱으로 전환 <br><br>
<span>
  \( \begin{pmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \end{pmatrix} \circ (W) &#43; b =  \begin{pmatrix} y_1 \\ y_2 \\ y_3 \\ y_4 \\ y_5 \end{pmatrix} \)
</span>
<br><br>
<span>
  \( X \circ W &#43; b = Y \)
</span>
<br><br></p>
</blockquote>
<blockquote>
<p><U>Multi Variable Dot Product</U> <br><br>
<span>
  \( \begin{pmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \end{pmatrix} \circ \begin{pmatrix} w_1 \\ w_2 \\ w_3 \end{pmatrix} &#43; b =  \begin{pmatrix} y_1 \\ y_2 \\ y_3 \\ y_4 \\ y_5 \end{pmatrix} \)
</span>
<br><br>
<span>
  \( X \circ W &#43; b = Y \)
</span>
<br><br></p>
</blockquote>
<h3 id="logistic-regression">
  Logistic Regression
  <a class="anchor" href="#logistic-regression">#</a>
</h3>
<p>행렬의 곱까지 잘 왔다면 이제 Regression을 통해 처리된 결과를 Classification 해야될 차례입니다. 분류의 문제에 있어서 Linear Regression은 선형의 수치형 값을 가지게 됩니다. 하지만 분류의 문제에 있어서는 선택의 문제로 표현되어야 합니다. 즉 Linear한 결과를 Logistic Regression을 통해 특정 값으로 한정하고 이를 Decision Boundary를 통해 분류의 결과로 대치할 수 있는 필요가 생깁니다.</p>
<div><img width="600px" src="../../../../../201020_09_ml_logistic_regression.png" alt="logistic regression" ></div>
<p>분류의 문제를 위해 지수 함수 <span>
  \( e^x \)
</span>
를 <span>
  \( e^{-x} \)
</span>
로 전환 후 <span>
  \( \frac{1}{1 &#43; e^{-x}} \)
</span>
에 대입하여 x의 좌측으로 진행할수로 <span>
  \( \infty \)
</span>
에 가까워지며 0에 수렴하게되고 x의 우측으로 진행하게 될수록 0에 수렴하면서 1에 가까워지는 결과가 나오게 된다.</p>
<div><img width="600px" src="../../../../../201020_10_ml_e_x.png" alt="e_x" ></div>
<div><img width="600px" src="../../../../../201020_08_ml_sigmoid.png" alt="local_minima" ></div>
<blockquote>
<p><U>Sigmoid</U> <br><br>
<span>
  \(  Z = Wx &#43; b  \)
</span>
<br><br>
<span>
  \(  y = \frac {1}{1&#43;e^{-(Wx&#43;b)}}  \)
</span>
<br><br>
<span>
  \(  y = sigmoid(Z) = \sigma (z) = \frac{1}{1&#43;e^{-z}}  \)
</span>
<br><br></p>
</blockquote>
<p>Logistic Regression의 Cost Function은 y = 1 일때와 y = 0 일때로 구분할 수 있는데 y = 1 일때는 <span>
  \( \log(h(x)) \)
</span>
의 역 <span>
  \( -\log(h(x)) \)
</span>
을 취해 0으로 근접할 수록 오차가 <span>
  \( \infty \)
</span>
에 수렴하게 됩니다. 반대로 y = 0 일때는 <span>
  \( \log(1 - h(x)) \)
</span>
 와 같이 1에서 차감하여 1에 근접할 수록 오차가 <span>
  \( \infty \)
</span>
에 수렴하게 합니다. 이 두 식을 합하면 Logistic Regression의 Convex한 Cost Funtion이 되게 됩니다.</p>
<div><img width="600px" src="../../../../../201020_11_log_y_1.jpg" alt="log y=1" ></div>
<div><img width="600px" src="../../../../../201020_12_log_y_0.jpg" alt="log y=0" ></div>
<p>Cross-Entropy 유도의 또다른 설명은 다음과 같습니다.</p>
<blockquote>
<p>하나의 입력 x에 대해 출력이 1일 확률을 y로 정의. y는 0 또는 1일임으로 y = sigmodi(Wx+b)로 나타낼 수 있습니다.
<span>
  \(  p(C=1|x) = y = sigmoid(Wx&#43;b)  \)
</span>
<br><br>
입력 x에 대해 출력이 0일 확률은 1이 나타날 확률의 나머지임으로 1-y 입니다.
<span>
  \(  p(C=0|x) = 1 - p(C=1|x) = 1 - y  \)
</span>
<br><br>
확률 변수 C는 0 또는 1 이외에는 존재하지 않음으로 베르누이 시행을 전제로 하며 베르누이 분포의 확률질량함수(PMF)는 다음과 같이 정의됩니다.
<span>
  \(  p(C=p|x) = y^p ( 1 - y )^{1-P}  \)
</span>
<br><br>
<span>
  \( Likelyhood (W,b) = \displaystyle\prod_{i=1}^n p(C=p_i|x_i) = \displaystyle\prod_{i=1}^n y_i^{p_i} (1-y_i)^{1-p_i}  \)
</span>
<br><br>
이를 Log 변환을 통해 Convex의 형태와 극점의 위치를 유지하며 곱을 선형의 조합 꼴로 풀 수 있도록 변환합니다.<br><br>
<span>
  \( E(W,b) = - \log L(W,b) = -\displaystyle\sum_{i=1}^n \lbrace t_i \textstyle\log y_i &#43; (1-p_i) \log(1-y_i) \rbrace  \)
</span>
<br><br></p>
</blockquote>
<blockquote>
<p><U>Logistic Regression Cross Entropy</U> <br><br>
<span>
  \(  y = \frac{1}{1&#43;e^{-(Wx&#43;b)}} , \space p_i = 0 \space or \space 1  \)
</span>
<br><br>
<span>
  \(  E(W,b) = - \, \displaystyle\sum_{i=1}^n \, \lbrace p_i \, \log y_i &#43; (1-p_i) \log (1-y_i) \rbrace \)
</span>
<br><br></p>
</blockquote>
<h3 id="logit--sigmoid--softmax">
  Logit / Sigmoid / Softmax
  <a class="anchor" href="#logit--sigmoid--softmax">#</a>
</h3>
<p>Logit /Sigmoid /Softmax의 관계를 정리해보도록 하겠습니다.</p>
<p><a href="https://velog.io/@gwkoo/logit-sigmoid-softmax%EC%9D%98-%EA%B4%80%EA%B3%84" target="_blank" rel="noopener">참고1</a><br>
<a href="https://opentutorials.org/module/3653/22995" target="_blank" rel="noopener">참고2</a><br>
<a href="https://chacha95.github.io/2019-04-04-logit/" target="_blank" rel="noopener">참고3</a><br>
<a href="https://ratsgo.github.io/statistics/2017/07/01/bayes/" target="_blank" rel="noopener">참고4</a></p>
<p>불확실성 하의 의사 결정의 문제를 수학적으로 다룰때 사용되는 베이즈의 정리(Bayes Theorem)와 전확률 법칙(law of total probability)는 다음과 같습니다.</p>
<blockquote>
<p><U>Bayes Theorem</U> <br><br>
<span>
  \(  P(Y|X) = \frac {P(X \bigcap Y)}{P(X)} \)
</span>
<br><br>
<span>
  \(  P(X|Y) = \frac {P(Y \bigcap X)}{P(Y)} \)
</span>
<br><br>
<span>
  \(  P(Y \bigcap X) = P(X \bigcap Y) = P(X|Y)P(Y) = P(Y|X)P(X) \)
</span>
<br><br>
<span>
  \( \therefore P(Y|X) = \frac {P(X|Y)P(Y)}{P(X)} \)
</span>
<br><br>
<span>
  \( P(Y|X) : 사후확률(Posterio probability) \)
</span>
<br><br>
<span>
  \( P(X|Y) : 가능도(likelihood) \)
</span>
<br><br>
<span>
  \( P(Y) : 확률변수 Y의 사전확률(prior probability) \)
</span>
<br><br>
<span>
  \( P(X) : 확률변수 X의 사전확률(prior probability) \)
</span>
<br><br></p>
</blockquote>
<p>표본 공간 S를 n개로 나누었을때 사건 A의 확률은 다음과 같이 나타나며 총합은 1이됩니다.</p>
<blockquote>
<p><U>Law of total probability</U> <br><br>
<span>
  \(  P(A) = P(A \bigcap B_1) &#43; P(A \bigcap B_2) &#43; ... &#43; P(A \bigcap B_n) \)
</span>
<br><br></p>
</blockquote>
<p>조금더 내용을 들어가 보도록 하겠습니다. 일반적으로 <span>
  \( A_1,A_2,A_3 \)
</span>
가 서로 Mutually Exclusive이고 이들의 합집합이 표본공간과 S와 같으면 사건 <span>
  \( A_1,A_2,A_3 \)
</span>
는 표본공간 S의 분할이라고 정의합니다 우리가 관심있는 사건 B가 나타날 확률은 전확률 공식에 의거 다음과 같이 표현할 수 있습니다.</p>
<blockquote>
<p><span>
  \(  P(B)) = P(A_1)P(B|A_1)&#43; P(A_2)P(B|A_2) &#43; P(A_3)P(B|A_3) = \displaystyle\sum_{i=1}^3 P(A_i)P(B|A_i) \)
</span>
<br><br></p>
</blockquote>
<p><span>
  \(P(A_1)\)
</span>
,<span>
  \(P(A_2)\)
</span>
,<span>
  \(P(A_3)\)
</span>
은 미리 알고 있다는 의미로 사전확률(Prior Probability)로 불리고, <span>
  \(P(B|A_1)\)
</span>
,<span>
  \(P(B|A_2)\)
</span>
,<span>
  \(P(B|A_3)\)
</span>
는 우도(Likelihood Probability)라 부릅니다.</p>
<p><span>
  \( P(A_1|B) \)
</span>
는 사건 B를 관측한 후 원인이 되는 사건 A의 확률을 따졌다는 의미에서 사후확률(Posterior Probability)로 정의되며 다음과 같이 나타낼 수 있습니다.</p>
<blockquote>
<p><span>
  \(  P(A_1|B)) = \frac {P(A_1)P(B|A_1)}{P(B)} = \frac {P(A_1)P(B|A_1)}{P(A_1)P(B|A_1)&#43;P(A_2)P(B|A_2)&#43;P(A_3)P(B|A_3)} \)
</span>
<br><br></p>
</blockquote>
<blockquote>
<p>전체 인구의 1%가 어떤 병에 걸렸다고합시다. 이 병의 진단 정확도가 97% 오진률은 6%라고 가정하면 다음과 같습니다.</p>
</blockquote>
<blockquote>
<p><U>Practice 1</U> <br><br>
<span>
  \( P(D) = 0.01 \)
</span>
 : 사전확률 <br>
<span>
  \( P( \backsim D) = 0.99 \)
</span>
 : 사전확률 <br>
<span>
  \( P(&#43;|D) = 0.97 \)
</span>
 : 우도 <br>
<span>
  \( P(&#43;| \backsim D) = 0.06 \)
</span>
 : 우도 <br>
<span>
  \( P(D|&#43;) \)
</span>
 = ????? : 사후확률 (진단테스트 양성일때 실제 환자일 확률 ) <br><br>
<span>
  \( P(&#43;) = P(D \cap &#43;)&#43;P( \backsim D \cap &#43;) \)
</span>
<br>
<span>
  \( = P(D)P(&#43;|D)&#43;P( \backsim D)P(&#43;| \backsim D) \)
</span>
<br>
<span>
  \( = 0.01 \times 0.97 &#43; 0.99 \times 0.06 \)
</span>
<br>
<span>
  \( = 0.691 \)
</span>
<br><br>
<span>
  \( P(D|&#43;) = \frac {P(D)P(&#43;|D)}{P(&#43;)} \)
</span>
<br>
<span>
  \( = \frac {0.01 \times 0.97}{0.691} \)
</span>
<br>
<span>
  \( = 0.014 \)
</span>
<br><br></p>
</blockquote>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>&ldquo;Regression toward the mean&rdquo;, Sir Francis Galton (1822~1911) <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p><a href="https://towardsdatascience.com/why-gradient-descent-isnt-enough-a-comprehensive-introduction-to-optimization-algorithms-in-59670fd5c096" target="_blank" rel="noopener">굴곡진 인생의 정답을 찾아가는 험난한 여정</a> <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>
</article>

            
            

            <footer class="book-footer">
                 <div class="flex flex-wrap justify-between">





</div>
 
                
                
            </footer>

             
<div class="book-comments">
<br>
<br>
<hr> </hr>
<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "prokoptasis" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a></div>
 
            

            <label for="menu-control" class="hidden book-menu-overlay"></label>
        </div>

        
    </main>

    
</body>

</html>

     